"""
é‡‘èæƒ…æ„Ÿåˆ†ææ¨¡å‹æµ‹è¯•è„šæœ¬
å¯¹æ¯” LoRA å¾®è°ƒæ¨¡å‹ vs åŸå§‹æ¨¡å‹çš„å‡†ç¡®ç‡
"""

import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# ==================== é…ç½® ====================
BASE_MODEL_PATH = "/home/user150/models/Qwen3-14B"
LORA_PATH = "/home/user150/LLaMA-Factory/saves/qwen3-14b/qlora/financial_sentiment/checkpoint-2000"

# ==================== åŠ è½½æ¨¡å‹ ====================
print("æ­£åœ¨åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

# QLoRA: 4-bit é‡åŒ–åŠ è½½
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

print("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ (4-bit)...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_PATH,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

print("æ­£åœ¨åˆ›å»ºå¾®è°ƒæ¨¡å‹ (åŠ è½½ LoRA æƒé‡)...")
finetuned_model = PeftModel.from_pretrained(base_model, LORA_PATH)
finetuned_model.eval()

print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")


# ==================== å·¥å…·å‡½æ•° ====================
def extract_sentiment(text: str) -> str:
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­æå–æƒ…æ„Ÿæ ‡ç­¾
    å¤„ç† <think>...</think> æ ¼å¼ï¼Œæå–æœ€ç»ˆç­”æ¡ˆ
    """
    # å»é™¤ <think>...</think> éƒ¨åˆ†
    text_clean = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œå°è¯•ä»åŸæ–‡æå–
    if not text_clean:
        text_clean = text

    # è½¬å°å†™ä¾¿äºåŒ¹é…
    text_lower = text_clean.lower()

    # æŒ‰ä¼˜å…ˆçº§åŒ¹é…æƒ…æ„Ÿè¯
    if "positive" in text_lower:
        return "positive"
    elif "negative" in text_lower:
        return "negative"
    elif "neutral" in text_lower:
        return "neutral"
    else:
        # å°è¯•ä»åŸå§‹æ–‡æœ¬ï¼ˆåŒ…æ‹¬ think éƒ¨åˆ†ï¼‰æå–
        text_lower_full = text.lower()
        # æŸ¥æ‰¾æœ€åå‡ºç°çš„æƒ…æ„Ÿè¯ï¼ˆé€šå¸¸æ˜¯ç»“è®ºï¼‰
        last_pos = -1
        result = "unknown"
        for sentiment in ["positive", "negative", "neutral"]:
            pos = text_lower_full.rfind(sentiment)
            if pos > last_pos:
                last_pos = pos
                result = sentiment
        return result


# ==================== æ¨ç†å‡½æ•° ====================
def predict_sentiment(
    model, text: str, use_lora: bool = True, max_tokens: int = 500
) -> str:
    """
    é¢„æµ‹é‡‘èæ–‡æœ¬çš„æƒ…æ„Ÿ
    use_lora: True ä½¿ç”¨å¾®è°ƒæ¨¡å‹ï¼ŒFalse ä½¿ç”¨åŸå§‹æ¨¡å‹
    """
    prompt = f"Analyze the sentiment of this financial text and classify it as positive, negative, or neutral.\n\nText: {text}"

    messages = [{"role": "user", "content": prompt}]

    input_text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        if use_lora:
            model.enable_adapter_layers()
        else:
            model.disable_adapter_layers()

        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=False,
            temperature=1.0,
            pad_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
    )
    return response.strip()


# ==================== æµ‹è¯•æ•°æ®ï¼ˆå¸¦æ ‡ç­¾ï¼‰ ====================
test_cases = [
    # (æ–‡æœ¬, æ­£ç¡®æ ‡ç­¾)
    ("ä»Šå¤©ä¸ä¹è§‚", "negative"),
    (
        "Apple stock surges 5% after strong quarterly earnings beat expectations",
        "positive",
    ),
    ("The company announced massive layoffs affecting 10,000 employees", "negative"),
    ("Federal Reserve keeps interest rates unchanged as expected", "neutral"),
    ("Tesla shares plummet amid concerns over declining demand in China", "negative"),
    ("Microsoft reports record cloud revenue growth in Q3", "positive"),
    ("Oil prices remain stable despite geopolitical tensions", "neutral"),
    ("Amazon shares jump 8% on strong holiday shopping forecast", "positive"),
    ("Bank of America reports $2 billion loss in trading division", "negative"),
    ("The S&P 500 closed flat as investors await inflation data", "neutral"),
    ("Netflix subscriber growth exceeds analyst expectations", "positive"),
    ("Company faces massive lawsuit over environmental violations", "negative"),
]

print("=" * 100)
print("é‡‘èæƒ…æ„Ÿåˆ†æå‡†ç¡®ç‡å¯¹æ¯”ï¼šğŸ¯ å¾®è°ƒæ¨¡å‹ vs ğŸ“¦ åŸå§‹æ¨¡å‹")
print("=" * 100)

finetuned_correct = 0
original_correct = 0
total = len(test_cases)

results = []

for i, (text, label) in enumerate(test_cases, 1):
    print(f"\nã€æµ‹è¯• {i}/{total}ã€‘")
    print(f"ğŸ“° æ–‡æœ¬: {text}")
    print(f"âœ… æ­£ç¡®æ ‡ç­¾: {label}")
    print("-" * 100)

    # å¾®è°ƒæ¨¡å‹
    finetuned_raw = predict_sentiment(
        finetuned_model, text, use_lora=True, max_tokens=50
    )
    finetuned_pred = extract_sentiment(finetuned_raw)
    finetuned_match = "âœ“" if finetuned_pred == label else "âœ—"
    if finetuned_pred == label:
        finetuned_correct += 1

    # åŸå§‹æ¨¡å‹ï¼ˆéœ€è¦æ›´å¤š token è®©å®ƒå®Œæˆæ¨ç†ï¼‰
    original_raw = predict_sentiment(
        finetuned_model, text, use_lora=False, max_tokens=500
    )
    original_pred = extract_sentiment(original_raw)
    original_match = "âœ“" if original_pred == label else "âœ—"
    if original_pred == label:
        original_correct += 1

    print(
        f"ğŸ¯ å¾®è°ƒæ¨¡å‹: {finetuned_pred:10s} {finetuned_match}  (åŸå§‹è¾“å‡º: {finetuned_raw[:50]})"
    )
    print(
        f"ğŸ“¦ åŸå§‹æ¨¡å‹: {original_pred:10s} {original_match}  (æå–è‡ª {len(original_raw)} å­—ç¬¦)"
    )

    results.append(
        {
            "text": text[:50],
            "label": label,
            "finetuned": finetuned_pred,
            "original": original_pred,
        }
    )

# ==================== æ±‡æ€»ç»Ÿè®¡ ====================
print("\n" + "=" * 100)
print("ğŸ“Š å‡†ç¡®ç‡ç»Ÿè®¡")
print("=" * 100)
print(
    f"ğŸ¯ å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {finetuned_correct}/{total} = {finetuned_correct/total*100:.1f}%"
)
print(
    f"ğŸ“¦ åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {original_correct}/{total} = {original_correct/total*100:.1f}%"
)
print(f"ğŸ“ˆ æå‡: {(finetuned_correct - original_correct)/total*100:+.1f}%")
print("=" * 100)

# è¯¦ç»†ç»“æœè¡¨æ ¼
print("\nè¯¦ç»†ç»“æœ:")
print(
    f"{'åºå·':<4} {'æ ‡ç­¾':<10} {'å¾®è°ƒ':<10} {'åŸå§‹':<10} {'å¾®è°ƒæ­£ç¡®':<8} {'åŸå§‹æ­£ç¡®':<8}"
)
print("-" * 60)
for i, r in enumerate(results, 1):
    ft_ok = "âœ“" if r["finetuned"] == r["label"] else "âœ—"
    og_ok = "âœ“" if r["original"] == r["label"] else "âœ—"
    print(
        f"{i:<4} {r['label']:<10} {r['finetuned']:<10} {r['original']:<10} {ft_ok:<8} {og_ok:<8}"
    )
