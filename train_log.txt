nohup: ignoring input
[INFO|2025-12-18 01:33:18] llamafactory.launcher:143 >> Initializing 3 distributed tasks at: 127.0.0.1:60691
W1218 01:33:19.457000 3923058 site-packages/torch/distributed/run.py:803] 
W1218 01:33:19.457000 3923058 site-packages/torch/distributed/run.py:803] *****************************************
W1218 01:33:19.457000 3923058 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 01:33:19.457000 3923058 site-packages/torch/distributed/run.py:803] *****************************************
[2025-12-18 01:33:45,566] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-18 01:33:45,855] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-18 01:33:45,863] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-18 01:33:46,319] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-18 01:33:46,319] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W1218 01:33:46.998750091 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-18 01:33:46,623] [INFO] [comm.py:669:init_distributed] cdb=None
[W1218 01:33:46.302872558 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[2025-12-18 01:33:46,645] [INFO] [comm.py:669:init_distributed] cdb=None
[W1218 01:33:46.324498183 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|2025-12-18 01:33:46] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-18 01:33:46] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 3, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,719 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,719 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,719 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,720 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,720 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,720 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:46,720 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-12-18 01:33:47,013 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:763] 2025-12-18 01:33:47,014 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-18 01:33:47,016 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,016 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,016 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,016 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,016 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,017 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,017 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2025-12-18 01:33:47,017 >> loading file chat_template.jinja
[INFO|2025-12-18 01:33:47] llamafactory.hparams.parser:468 >> Process rank: 1, world size: 3, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-18 01:33:47] llamafactory.hparams.parser:468 >> Process rank: 2, world size: 3, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2364] 2025-12-18 01:33:47,324 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-12-18 01:33:47] llamafactory.data.loader:143 >> Loading dataset financial_sentiment_train.json...
Converting format of dataset (num_proc=16): 100%|██████████| 27540/27540 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 28258 examples [00:00, 2020.13 examples/s]          Converting format of dataset (num_proc=16): 46753 examples [00:00, 54464.45 examples/s]Converting format of dataset (num_proc=16): 55080 examples [00:00, 41571.64 examples/s]
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W1218 01:34:03.528350993 ProcessGroupNCCL.cpp:5068] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
Running tokenizer on dataset (num_proc=16): 100%|██████████| 27540/27540 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16): 28540 examples [00:01, 646.64 examples/s]           Running tokenizer on dataset (num_proc=16): 30540 examples [00:01, 2180.89 examples/s]Running tokenizer on dataset (num_proc=16): 31540 examples [00:01, 2882.81 examples/s]Running tokenizer on dataset (num_proc=16): 34262 examples [00:01, 5794.79 examples/s]Running tokenizer on dataset (num_proc=16): 36984 examples [00:02, 8184.71 examples/s]Running tokenizer on dataset (num_proc=16): 39706 examples [00:02, 10104.43 examples/s]Running tokenizer on dataset (num_proc=16): 42870 examples [00:02, 13408.41 examples/s]Running tokenizer on dataset (num_proc=16): 45591 examples [00:02, 15131.93 examples/s]Running tokenizer on dataset (num_proc=16): 48033 examples [00:02, 16040.02 examples/s]Running tokenizer on dataset (num_proc=16): 50475 examples [00:02, 17342.39 examples/s]Running tokenizer on dataset (num_proc=16): 52917 examples [00:03, 13302.33 examples/s]Running tokenizer on dataset (num_proc=16): 55080 examples [00:03, 9626.09 examples/s] Running tokenizer on dataset (num_proc=16): 55080 examples [00:03, 7595.78 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 5896, 18237, 57294, 304, 25975, 6358, 13, 4615, 3383, 374, 311, 23643, 279, 25975, 315, 5896, 21984, 323, 48129, 1105, 1119, 825, 315, 2326, 11059, 25, 6785, 11, 20628, 11, 476, 8225, 382, 16686, 10999, 510, 12, 6785, 25, 44267, 35936, 35621, 11, 6513, 4650, 11, 36749, 3081, 4682, 11, 476, 1661, 5896, 5068, 624, 12, 20628, 25, 44267, 59901, 1995, 2041, 2797, 6785, 476, 8225, 24154, 624, 12, 8225, 25, 44267, 72523, 4532, 35621, 11, 17704, 11, 91971, 3081, 4682, 11, 476, 7852, 5896, 5068, 382, 65354, 448, 1172, 825, 3409, 25, 6785, 11, 20628, 11, 476, 8225, 13, 151645, 198, 151644, 872, 198, 2082, 55856, 279, 25975, 315, 279, 2701, 5896, 1467, 323, 48129, 432, 438, 6785, 11, 20628, 11, 476, 8225, 382, 1178, 25, 88689, 279, 14195, 13014, 8242, 11, 451, 80775, 8547, 1865, 1045, 45452, 10797, 389, 1181, 13014, 21786, 11, 86699, 264, 2421, 14473, 804, 323, 18279, 22111, 13, 451, 80775, 3406, 1199, 5573, 25, 451, 80775, 8547, 11, 892, 26057, 279, 15526, 13190, 11, 51283, 1181, 2309, 304, 15819, 95109, 11, 4848, 13, 320, 87511, 25, 79341, 568, 151645, 198, 151644, 77091, 198, 42224, 151645, 198]
inputs:
<|im_start|>system
You are an expert financial analyst specializing in sentiment analysis. Your task is to analyze the sentiment of financial texts and classify them into one of three categories: positive, neutral, or negative.

Guidelines:
- positive: Indicates optimistic outlook, growth potential, favorable market conditions, or good financial performance.
- neutral: Indicates factual information without clear positive or negative implications.
- negative: Indicates pessimistic outlook, decline, unfavorable market conditions, or poor financial performance.

Respond with only one word: positive, neutral, or negative.<|im_end|>
<|im_start|>user
Analyze the sentiment of the following financial text and classify it as positive, neutral, or negative.

Text: Amid the ongoing tech rout, Norges Bank made some cosmetic moves on its tech investments, barring a few liquidations and stake builds. Norges Quits Facebook: Norges Bank, which operates the Oil Fund, exited its position in Meta Platforms, Inc. (NASDAQ: META).<|im_end|>
<|im_start|>assistant
negative<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 42224, 151645, 198]
labels:
negative<|im_end|>

[INFO|configuration_utils.py:763] 2025-12-18 01:34:20,029 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-18 01:34:20,031 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-12-18 01:34:20] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1169] 2025-12-18 01:34:20,164 >> loading weights file /home/user150/models/Qwen3-14B/model.safetensors.index.json
[INFO|modeling_utils.py:4373] 2025-12-18 01:34:20,165 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-12-18 01:34:20,165] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 3
[WARNING|logging.py:328] 2025-12-18 01:34:20,176 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|configuration_utils.py:986] 2025-12-18 01:34:20,178 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[WARNING|logging.py:328] 2025-12-18 01:34:20,180 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-18 01:34:20,297] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 3
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-18 01:34:20,349] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 3
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-18 01:34:22,338] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 443, num_elems = 14.77B
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.47s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:03,  1.64it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:03,  1.59it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:08,  1.45s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.14it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.13it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.45s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:04<00:03,  1.06s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:04<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:05<00:05,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:06<00:02,  1.18s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:06<00:02,  1.18s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:07<00:04,  1.44s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:07<00:01,  1.26s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:07<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.03it/s]
Loading checkpoint shards:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:10<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.33s/it]
[INFO|configuration_utils.py:939] 2025-12-18 01:34:33,013 >> loading configuration file /home/user150/models/Qwen3-14B/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-18 01:34:33,014 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[INFO|dynamic_module_utils.py:423] 2025-12-18 01:34:33,015 >> Could not locate the custom_generate/generate.py inside /home/user150/models/Qwen3-14B.
[INFO|2025-12-18 01:34:33] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-18 01:34:33] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-18 01:34:33] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-12-18 01:34:33] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-18 01:34:33] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,o_proj,down_proj,k_proj,up_proj,gate_proj,v_proj
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2025-12-18 01:34:35] llamafactory.model.loader:143 >> trainable params: 32,112,640 || all params: 14,800,419,840 || trainable%: 0.2170
[INFO|trainer.py:749] 2025-12-18 01:34:35,233 >> Using auto half precision backend
[WARNING|trainer.py:982] 2025-12-18 01:34:35,234 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 16. Using DeepSpeed's value.
[2025-12-18 01:34:35,747] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-18 01:34:35,747] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 3
[2025-12-18 01:34:35,792] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-18 01:34:35,796] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-18 01:34:35,796] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-18 01:34:35,835] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-12-18 01:34:35,835] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-12-18 01:34:35,835] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-12-18 01:34:35,835] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-12-18 01:34:36,100] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-12-18 01:34:36,100] [INFO] [utils.py:782:see_memory_usage] MA 9.23 GB         Max_MA 13.03 GB         CA 9.3 GB         Max_CA 13 GB 
[2025-12-18 01:34:36,101] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.1 GB, percent = 5.0%
[2025-12-18 01:34:36,110] [INFO] [stage3.py:170:__init__] Reduce bucket size 26214400
[2025-12-18 01:34:36,110] [INFO] [stage3.py:171:__init__] Prefetch bucket size 23592960
[2025-12-18 01:34:36,363] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-12-18 01:34:36,364] [INFO] [utils.py:782:see_memory_usage] MA 9.23 GB         Max_MA 9.23 GB         CA 9.3 GB         Max_CA 9 GB 
[2025-12-18 01:34:36,364] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.1 GB, percent = 5.0%
Parameter Offload: Total persistent parameters: 15825920 in 601 params
[2025-12-18 01:34:37,528] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-12-18 01:34:37,529] [INFO] [utils.py:782:see_memory_usage] MA 9.19 GB         Max_MA 9.23 GB         CA 9.3 GB         Max_CA 9 GB 
[2025-12-18 01:34:37,529] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.2 GB, percent = 5.0%
[2025-12-18 01:34:37,875] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-12-18 01:34:37,876] [INFO] [utils.py:782:see_memory_usage] MA 9.19 GB         Max_MA 9.19 GB         CA 9.3 GB         Max_CA 9 GB 
[2025-12-18 01:34:37,876] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.2 GB, percent = 5.0%
[2025-12-18 01:34:38,629] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-12-18 01:34:38,630] [INFO] [utils.py:782:see_memory_usage] MA 9.19 GB         Max_MA 9.19 GB         CA 9.2 GB         Max_CA 9 GB 
[2025-12-18 01:34:38,630] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.22 GB, percent = 5.0%
[2025-12-18 01:34:39,001] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-12-18 01:34:39,001] [INFO] [utils.py:782:see_memory_usage] MA 9.19 GB         Max_MA 9.19 GB         CA 9.2 GB         Max_CA 9 GB 
[2025-12-18 01:34:39,001] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.23 GB, percent = 5.0%
[2025-12-18 01:34:39,378] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-12-18 01:34:39,379] [INFO] [utils.py:782:see_memory_usage] MA 9.23 GB         Max_MA 9.25 GB         CA 9.26 GB         Max_CA 9 GB 
[2025-12-18 01:34:39,379] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.21 GB, percent = 5.0%
[2025-12-18 01:34:39,745] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-18 01:34:39,745] [INFO] [utils.py:782:see_memory_usage] MA 9.23 GB         Max_MA 9.23 GB         CA 9.26 GB         Max_CA 9 GB 
[2025-12-18 01:34:39,746] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.21 GB, percent = 5.0%
[2025-12-18 01:34:40,099] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-18 01:34:40,100] [INFO] [utils.py:782:see_memory_usage] MA 9.23 GB         Max_MA 9.27 GB         CA 9.3 GB         Max_CA 9 GB 
[2025-12-18 01:34:40,100] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.21 GB, percent = 5.0%
[2025-12-18 01:34:40,101] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
[2025-12-18 01:34:40,654] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-18 01:34:40,655] [INFO] [utils.py:782:see_memory_usage] MA 9.3 GB         Max_MA 9.3 GB         CA 9.32 GB         Max_CA 9 GB 
[2025-12-18 01:34:40,655] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.43 GB, percent = 5.0%
[2025-12-18 01:34:40,655] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-12-18 01:34:40,655] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-18 01:34:40,655] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-18 01:34:40,655] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-18 01:34:40,664] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-18 01:34:40,664] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-18 01:34:40,664] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-18 01:34:40,664] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-18 01:34:40,664] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f653cfd7550>
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 16
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-18 01:34:40,665] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   train_batch_size ............. 48
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   world_size ................... 3
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-18 01:34:40,666] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-12-18 01:34:40,666] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 48, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.621440e+07, 
        "stage3_prefetch_bucket_size": 2.359296e+07, 
        "stage3_param_persistence_threshold": 5.120000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-18 01:34:40,668 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-18 01:34:40,668 >>   Num examples = 24,786
[INFO|trainer.py:2521] 2025-12-18 01:34:40,668 >>   Num Epochs = 2
[INFO|trainer.py:2522] 2025-12-18 01:34:40,668 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2525] 2025-12-18 01:34:40,668 >>   Total train batch size (w. parallel, distributed & accumulation) = 48
[INFO|trainer.py:2526] 2025-12-18 01:34:40,668 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:2527] 2025-12-18 01:34:40,668 >>   Total optimization steps = 1,034
[INFO|trainer.py:2528] 2025-12-18 01:34:40,674 >>   Number of trainable parameters = 32,112,640
  0%|          | 0/1034 [00:00<?, ?it/s]  0%|          | 1/1034 [01:55<33:04:16, 115.25s/it]  0%|          | 2/1034 [03:43<31:53:02, 111.22s/it]  0%|          | 3/1034 [05:34<31:46:04, 110.93s/it]  0%|          | 4/1034 [07:24<31:37:34, 110.54s/it]  0%|          | 5/1034 [09:12<31:23:18, 109.81s/it]  1%|          | 6/1034 [11:01<31:17:46, 109.60s/it]  1%|          | 7/1034 [12:51<31:16:23, 109.62s/it]  1%|          | 8/1034 [14:40<31:10:10, 109.37s/it]  1%|          | 9/1034 [16:28<31:04:05, 109.12s/it]  1%|          | 10/1034 [18:18<31:02:03, 109.11s/it]                                                     {'loss': 17.2454, 'grad_norm': 36.3407537411524, 'learning_rate': 1.7307692307692308e-06, 'epoch': 0.02}
  1%|          | 10/1034 [18:18<31:02:03, 109.11s/it]  1%|          | 11/1034 [20:07<31:00:30, 109.12s/it]  1%|          | 12/1034 [21:58<31:07:50, 109.66s/it]  1%|▏         | 13/1034 [23:48<31:07:43, 109.76s/it]  1%|▏         | 14/1034 [25:36<31:00:15, 109.43s/it]  1%|▏         | 15/1034 [27:26<31:01:16, 109.59s/it]  2%|▏         | 16/1034 [29:17<31:05:59, 109.98s/it]  2%|▏         | 17/1034 [31:06<30:57:06, 109.56s/it]  2%|▏         | 18/1034 [32:54<30:50:12, 109.26s/it]  2%|▏         | 19/1034 [34:45<30:53:32, 109.57s/it]  2%|▏         | 20/1034 [36:35<30:57:23, 109.90s/it]                                                     {'loss': 17.1635, 'grad_norm': 38.7514759074626, 'learning_rate': 3.653846153846154e-06, 'epoch': 0.04}
  2%|▏         | 20/1034 [36:35<30:57:23, 109.90s/it]  2%|▏         | 21/1034 [38:24<30:48:59, 109.52s/it]  2%|▏         | 22/1034 [40:12<30:41:52, 109.20s/it]  2%|▏         | 23/1034 [42:02<30:42:12, 109.33s/it]  2%|▏         | 24/1034 [43:52<30:41:55, 109.42s/it]  2%|▏         | 25/1034 [45:40<30:35:16, 109.13s/it]  3%|▎         | 26/1034 [47:28<30:29:45, 108.91s/it]  3%|▎         | 27/1034 [49:19<30:36:13, 109.41s/it]  3%|▎         | 28/1034 [51:10<30:40:19, 109.76s/it]  3%|▎         | 29/1034 [52:58<30:33:33, 109.47s/it]  3%|▎         | 30/1034 [54:47<30:26:25, 109.15s/it]                                                     {'loss': 16.7788, 'grad_norm': 44.38979217746937, 'learning_rate': 5.576923076923077e-06, 'epoch': 0.06}
  3%|▎         | 30/1034 [54:47<30:26:25, 109.15s/it]  3%|▎         | 31/1034 [56:36<30:26:32, 109.26s/it]  3%|▎         | 32/1034 [58:25<30:22:46, 109.15s/it]  3%|▎         | 33/1034 [1:00:15<30:26:03, 109.45s/it]  3%|▎         | 34/1034 [1:02:06<30:28:05, 109.69s/it]  3%|▎         | 35/1034 [1:03:55<30:23:55, 109.54s/it]  3%|▎         | 36/1034 [1:05:43<30:16:59, 109.24s/it]  4%|▎         | 37/1034 [1:07:32<30:13:15, 109.12s/it]  4%|▎         | 38/1034 [1:09:23<30:17:37, 109.50s/it]  4%|▍         | 39/1034 [1:11:13<30:22:43, 109.91s/it]  4%|▍         | 40/1034 [1:13:03<30:17:19, 109.70s/it]                                                       {'loss': 15.0022, 'grad_norm': 50.91632084985503, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.08}
  4%|▍         | 40/1034 [1:13:03<30:17:19, 109.70s/it]  4%|▍         | 41/1034 [1:14:51<30:09:41, 109.35s/it]  4%|▍         | 42/1034 [1:16:40<30:03:31, 109.08s/it]  4%|▍         | 43/1034 [1:18:31<30:11:54, 109.70s/it]  4%|▍         | 44/1034 [1:20:22<30:15:56, 110.06s/it]  4%|▍         | 45/1034 [1:22:11<30:12:38, 109.97s/it]  4%|▍         | 46/1034 [1:24:00<30:03:01, 109.50s/it]  5%|▍         | 47/1034 [1:25:48<29:56:16, 109.20s/it]  5%|▍         | 48/1034 [1:27:38<29:55:49, 109.28s/it]  5%|▍         | 49/1034 [1:29:28<30:00:20, 109.67s/it]  5%|▍         | 50/1034 [1:31:18<29:59:53, 109.75s/it]                                                       {'loss': 10.5907, 'grad_norm': 44.482505559180154, 'learning_rate': 9.423076923076923e-06, 'epoch': 0.1}
  5%|▍         | 50/1034 [1:31:18<29:59:53, 109.75s/it]  5%|▍         | 51/1034 [1:33:07<29:51:44, 109.36s/it]  5%|▌         | 52/1034 [1:35:00<30:07:46, 110.45s/it]  5%|▌         | 53/1034 [1:37:13<31:58:01, 117.31s/it]  5%|▌         | 54/1034 [1:39:13<32:07:44, 118.03s/it]  5%|▌         | 55/1034 [1:41:08<31:54:42, 117.35s/it]  5%|▌         | 56/1034 [1:43:02<31:33:38, 116.17s/it]  6%|▌         | 57/1034 [1:44:54<31:11:15, 114.92s/it]  6%|▌         | 58/1034 [1:46:57<31:47:38, 117.27s/it]  6%|▌         | 59/1034 [1:48:58<32:04:19, 118.42s/it]  6%|▌         | 60/1034 [1:51:11<33:13:55, 122.83s/it]                                                       {'loss': 5.1991, 'grad_norm': 25.749157642573596, 'learning_rate': 1.1346153846153847e-05, 'epoch': 0.12}
  6%|▌         | 60/1034 [1:51:11<33:13:55, 122.83s/it]  6%|▌         | 61/1034 [1:53:18<33:33:04, 124.14s/it]  6%|▌         | 62/1034 [1:55:23<33:34:21, 124.34s/it]  6%|▌         | 63/1034 [1:57:28<33:36:11, 124.58s/it]  6%|▌         | 64/1034 [1:59:33<33:35:57, 124.70s/it]  6%|▋         | 65/1034 [2:01:36<33:24:05, 124.09s/it]  6%|▋         | 66/1034 [2:03:36<33:04:56, 123.03s/it]  6%|▋         | 67/1034 [2:05:26<31:57:03, 118.95s/it]  7%|▋         | 68/1034 [2:07:16<31:14:21, 116.42s/it]  7%|▋         | 69/1034 [2:09:07<30:45:48, 114.77s/it]  7%|▋         | 70/1034 [2:10:56<30:14:58, 112.97s/it]                                                       {'loss': 1.3657, 'grad_norm': 17.885046732010878, 'learning_rate': 1.3269230769230769e-05, 'epoch': 0.14}
  7%|▋         | 70/1034 [2:10:56<30:14:58, 112.97s/it]  7%|▋         | 71/1034 [2:12:44<29:51:32, 111.62s/it]  7%|▋         | 72/1034 [2:14:34<29:39:13, 110.97s/it]  7%|▋         | 73/1034 [2:16:24<29:33:55, 110.75s/it]  7%|▋         | 74/1034 [2:18:14<29:29:11, 110.57s/it]  7%|▋         | 75/1034 [2:20:03<29:17:32, 109.96s/it]  7%|▋         | 76/1034 [2:21:51<29:09:01, 109.54s/it]  7%|▋         | 77/1034 [2:23:41<29:06:28, 109.50s/it]  8%|▊         | 78/1034 [2:25:37<29:35:41, 111.44s/it]  8%|▊         | 79/1034 [2:27:40<30:28:40, 114.89s/it]  8%|▊         | 80/1034 [2:29:31<30:08:27, 113.74s/it]                                                       {'loss': 0.3551, 'grad_norm': 7.8018571753801, 'learning_rate': 1.5192307692307693e-05, 'epoch': 0.15}
  8%|▊         | 80/1034 [2:29:31<30:08:27, 113.74s/it]  8%|▊         | 81/1034 [2:31:25<30:08:02, 113.83s/it]  8%|▊         | 82/1034 [2:33:25<30:38:51, 115.89s/it]  8%|▊         | 83/1034 [2:35:32<31:27:19, 119.07s/it]  8%|▊         | 84/1034 [2:37:27<31:05:49, 117.84s/it]  8%|▊         | 85/1034 [2:39:20<30:41:01, 116.40s/it]  8%|▊         | 86/1034 [2:41:23<31:11:43, 118.46s/it]  8%|▊         | 87/1034 [2:43:32<31:56:30, 121.43s/it]  9%|▊         | 88/1034 [2:45:30<31:40:19, 120.53s/it]  9%|▊         | 89/1034 [2:47:23<31:05:03, 118.42s/it]  9%|▊         | 90/1034 [2:49:31<31:48:11, 121.28s/it]                                                       {'loss': 0.3022, 'grad_norm': 3.937951255720946, 'learning_rate': 1.7115384615384617e-05, 'epoch': 0.17}
  9%|▊         | 90/1034 [2:49:31<31:48:11, 121.28s/it]  9%|▉         | 91/1034 [2:51:33<31:49:32, 121.50s/it]  9%|▉         | 92/1034 [2:53:28<31:13:16, 119.32s/it]  9%|▉         | 93/1034 [2:55:23<30:53:49, 118.20s/it]  9%|▉         | 94/1034 [2:57:33<31:48:15, 121.80s/it]  9%|▉         | 95/1034 [2:59:33<31:37:04, 121.22s/it]  9%|▉         | 96/1034 [3:01:29<31:10:33, 119.65s/it]  9%|▉         | 97/1034 [3:03:29<31:06:28, 119.52s/it]  9%|▉         | 98/1034 [3:05:38<31:49:28, 122.40s/it] 10%|▉         | 99/1034 [3:07:39<31:42:01, 122.06s/it] 10%|▉         | 100/1034 [3:09:33<31:03:34, 119.72s/it]                                                        {'loss': 0.265, 'grad_norm': 8.159989920965076, 'learning_rate': 1.903846153846154e-05, 'epoch': 0.19}
 10%|▉         | 100/1034 [3:09:33<31:03:34, 119.72s/it] 10%|▉         | 101/1034 [3:11:31<30:51:41, 119.08s/it] 10%|▉         | 102/1034 [3:13:39<31:33:30, 121.90s/it] 10%|▉         | 103/1034 [3:15:36<31:07:37, 120.36s/it] 10%|█         | 104/1034 [3:17:30<30:34:54, 118.38s/it] 10%|█         | 105/1034 [3:19:39<31:21:45, 121.53s/it] 10%|█         | 106/1034 [3:21:40<31:17:22, 121.38s/it] 10%|█         | 107/1034 [3:23:33<30:37:35, 118.94s/it] 10%|█         | 108/1034 [3:25:39<31:09:55, 121.16s/it] 11%|█         | 109/1034 [3:27:44<31:26:36, 122.37s/it] 11%|█         | 110/1034 [3:29:38<30:42:42, 119.66s/it]                                                        {'loss': 0.2132, 'grad_norm': 11.75700752644978, 'learning_rate': 1.99985736255971e-05, 'epoch': 0.21}
 11%|█         | 110/1034 [3:29:38<30:42:42, 119.66s/it] 11%|█         | 111/1034 [3:31:38<30:43:38, 119.85s/it] 11%|█         | 112/1034 [3:33:46<31:20:04, 122.35s/it] 11%|█         | 113/1034 [3:35:42<30:47:02, 120.33s/it] 11%|█         | 114/1034 [3:37:38<30:27:18, 119.17s/it] 11%|█         | 115/1034 [3:39:47<31:08:28, 121.99s/it] 11%|█         | 116/1034 [3:41:51<31:16:25, 122.64s/it] 11%|█▏        | 117/1034 [3:43:58<31:32:42, 123.84s/it] 11%|█▏        | 118/1034 [3:46:09<32:05:33, 126.13s/it] 12%|█▏        | 119/1034 [3:48:29<33:06:34, 130.27s/it] 12%|█▏        | 120/1034 [3:50:34<32:40:10, 128.68s/it]                                                        {'loss': 0.1912, 'grad_norm': 5.002647294011304, 'learning_rate': 1.998716507171053e-05, 'epoch': 0.23}
 12%|█▏        | 120/1034 [3:50:34<32:40:10, 128.68s/it] 12%|█▏        | 121/1034 [3:52:37<32:11:48, 126.95s/it] 12%|█▏        | 122/1034 [3:54:46<32:20:24, 127.66s/it] 12%|█▏        | 123/1034 [3:56:54<32:17:08, 127.58s/it] 12%|█▏        | 124/1034 [3:58:51<31:29:01, 124.55s/it] 12%|█▏        | 125/1034 [4:00:49<30:57:03, 122.58s/it] 12%|█▏        | 126/1034 [4:02:53<31:02:59, 123.11s/it] 12%|█▏        | 127/1034 [4:05:10<32:03:36, 127.25s/it] 12%|█▏        | 128/1034 [4:07:07<31:12:34, 124.01s/it] 12%|█▏        | 129/1034 [4:09:04<30:41:23, 122.08s/it] 13%|█▎        | 130/1034 [4:11:08<30:47:17, 122.61s/it]                                                        {'loss': 0.2327, 'grad_norm': 5.680665062669503, 'learning_rate': 1.996436098130433e-05, 'epoch': 0.25}
 13%|█▎        | 130/1034 [4:11:08<30:47:17, 122.61s/it] 13%|█▎        | 131/1034 [4:13:29<32:06:31, 128.01s/it] 13%|█▎        | 132/1034 [4:15:39<32:12:35, 128.55s/it] 13%|█▎        | 133/1034 [4:17:44<31:56:43, 127.64s/it] 13%|█▎        | 134/1034 [4:19:43<31:13:21, 124.89s/it] 13%|█▎        | 135/1034 [4:21:37<30:21:49, 121.59s/it] 13%|█▎        | 136/1034 [4:23:30<29:45:07, 119.27s/it] 13%|█▎        | 137/1034 [4:25:27<29:29:02, 118.33s/it] 13%|█▎        | 138/1034 [4:27:22<29:14:27, 117.49s/it] 13%|█▎        | 139/1034 [4:29:17<28:59:13, 116.60s/it] 14%|█▎        | 140/1034 [4:31:14<28:59:39, 116.76s/it]                                                        {'loss': 0.1867, 'grad_norm': 4.660519122559444, 'learning_rate': 1.9930187374259338e-05, 'epoch': 0.27}
 14%|█▎        | 140/1034 [4:31:14<28:59:39, 116.76s/it] 14%|█▎        | 141/1034 [4:33:16<29:23:48, 118.51s/it] 14%|█▎        | 142/1034 [4:35:16<29:26:07, 118.80s/it] 14%|█▍        | 143/1034 [4:37:15<29:25:08, 118.87s/it] 14%|█▍        | 144/1034 [4:39:08<28:59:34, 117.27s/it] 14%|█▍        | 145/1034 [4:41:03<28:46:04, 116.50s/it] 14%|█▍        | 146/1034 [4:43:01<28:48:10, 116.77s/it] 14%|█▍        | 147/1034 [4:44:54<28:29:49, 115.66s/it] 14%|█▍        | 148/1034 [4:46:48<28:20:58, 115.19s/it] 14%|█▍        | 149/1034 [4:48:54<29:06:11, 118.39s/it] 15%|█▍        | 150/1034 [4:50:47<28:42:29, 116.91s/it]                                                        {'loss': 0.192, 'grad_norm': 5.334414377950816, 'learning_rate': 1.9884683243281117e-05, 'epoch': 0.29}
 15%|█▍        | 150/1034 [4:50:47<28:42:29, 116.91s/it] 15%|█▍        | 151/1034 [4:52:43<28:38:37, 116.78s/it] 15%|█▍        | 152/1034 [4:54:39<28:30:58, 116.39s/it] 15%|█▍        | 153/1034 [4:56:32<28:14:16, 115.39s/it] 15%|█▍        | 154/1034 [4:58:26<28:04:16, 114.84s/it] 15%|█▍        | 155/1034 [5:00:18<27:51:57, 114.13s/it] 15%|█▌        | 156/1034 [5:02:11<27:43:56, 113.71s/it] 15%|█▌        | 157/1034 [5:04:05<27:42:24, 113.73s/it] 15%|█▌        | 158/1034 [5:06:38<30:32:24, 125.51s/it] 15%|█▌        | 159/1034 [5:10:42<39:11:48, 161.27s/it] 15%|█▌        | 160/1034 [5:15:28<48:11:34, 198.51s/it]                                                        {'loss': 0.1607, 'grad_norm': 1.9443199171609928, 'learning_rate': 1.9827900509408583e-05, 'epoch': 0.31}
 15%|█▌        | 160/1034 [5:15:28<48:11:34, 198.51s/it] 16%|█▌        | 161/1034 [5:19:58<53:24:07, 220.21s/it] 16%|█▌        | 162/1034 [5:24:37<57:35:36, 237.77s/it] 16%|█▌        | 163/1034 [5:28:57<59:09:36, 244.52s/it] 16%|█▌        | 164/1034 [5:33:18<60:14:39, 249.29s/it] 16%|█▌        | 165/1034 [5:37:21<59:42:06, 247.33s/it] 16%|█▌        | 166/1034 [5:41:51<61:19:02, 254.31s/it] 16%|█▌        | 167/1034 [5:45:57<60:38:13, 251.78s/it] 16%|█▌        | 168/1034 [5:50:39<62:44:25, 260.81s/it] 16%|█▋        | 169/1034 [5:54:39<61:08:54, 254.49s/it] 16%|█▋        | 170/1034 [5:59:27<63:32:05, 264.73s/it]                                                        {'loss': 0.1731, 'grad_norm': 2.6153113265271353, 'learning_rate': 1.9759903962771155e-05, 'epoch': 0.33}
 16%|█▋        | 170/1034 [5:59:27<63:32:05, 264.73s/it] 17%|█▋        | 171/1034 [6:03:35<62:15:36, 259.72s/it] 17%|█▋        | 172/1034 [6:08:14<63:34:09, 265.49s/it] 17%|█▋        | 173/1034 [6:11:38<59:02:15, 246.85s/it] 17%|█▋        | 174/1034 [6:13:32<49:30:12, 207.22s/it] 17%|█▋        | 175/1034 [6:15:48<44:20:29, 185.83s/it] 17%|█▋        | 176/1034 [6:18:37<43:03:52, 180.69s/it] 17%|█▋        | 177/1034 [6:22:48<48:03:04, 201.85s/it] 17%|█▋        | 178/1034 [6:27:38<54:16:16, 228.24s/it] 17%|█▋        | 179/1034 [6:32:02<56:43:21, 238.83s/it] 17%|█▋        | 180/1034 [6:36:42<59:36:39, 251.29s/it]                                                        {'loss': 0.1526, 'grad_norm': 2.562007981633879, 'learning_rate': 1.9680771188662044e-05, 'epoch': 0.35}
 17%|█▋        | 180/1034 [6:36:42<59:36:39, 251.29s/it] 18%|█▊        | 181/1034 [6:40:53<59:31:25, 251.21s/it] 18%|█▊        | 182/1034 [6:45:33<61:29:40, 259.84s/it] 18%|█▊        | 183/1034 [6:49:58<61:48:59, 261.50s/it] 18%|█▊        | 184/1034 [6:54:59<64:29:52, 273.17s/it] 18%|█▊        | 185/1034 [6:59:10<62:53:48, 266.70s/it] 18%|█▊        | 186/1034 [7:03:18<61:29:28, 261.05s/it] 18%|█▊        | 187/1034 [7:07:25<60:24:29, 256.75s/it] 18%|█▊        | 188/1034 [7:11:30<59:31:54, 253.33s/it] 18%|█▊        | 189/1034 [7:15:39<59:06:19, 251.81s/it] 18%|█▊        | 190/1034 [7:19:46<58:44:04, 250.53s/it]                                                        {'loss': 0.157, 'grad_norm': 4.296875432575317, 'learning_rate': 1.9590592479012022e-05, 'epoch': 0.37}
 18%|█▊        | 190/1034 [7:19:46<58:44:04, 250.53s/it] 18%|█▊        | 191/1034 [7:28:49<79:10:35, 338.12s/it] 19%|█▊        | 192/1034 [7:43:16<116:11:35, 496.79s/it] 19%|█▊        | 193/1034 [7:45:41<91:25:55, 391.39s/it]  19%|█▉        | 194/1034 [8:04:50<144:21:27, 618.68s/it] 19%|█▉        | 195/1034 [8:07:02<110:09:51, 472.70s/it] 19%|█▉        | 196/1034 [8:09:06<85:40:44, 368.07s/it]  19%|█▉        | 197/1034 [8:11:15<68:51:24, 296.16s/it] 19%|█▉        | 198/1034 [8:16:21<69:31:16, 299.37s/it] 19%|█▉        | 199/1034 [8:18:16<56:36:42, 244.08s/it] 19%|█▉        | 200/1034 [8:20:30<48:52:43, 210.99s/it]                                                        {'loss': 0.192, 'grad_norm': 5.475943664823144, 'learning_rate': 1.9489470729364694e-05, 'epoch': 0.39}
 19%|█▉        | 200/1034 [8:20:30<48:52:43, 210.99s/it] 19%|█▉        | 201/1034 [8:22:33<42:41:16, 184.49s/it] 20%|█▉        | 202/1034 [8:24:28<37:49:13, 163.65s/it] 20%|█▉        | 203/1034 [8:26:23<34:26:05, 149.18s/it] 20%|█▉        | 204/1034 [8:28:20<32:07:41, 139.35s/it] 20%|█▉        | 205/1034 [8:30:16<30:31:16, 132.54s/it] 20%|█▉        | 206/1034 [8:32:30<30:34:53, 132.96s/it] 20%|██        | 207/1034 [8:35:47<34:54:26, 151.96s/it] 20%|██        | 208/1034 [8:37:44<32:29:36, 141.62s/it] 20%|██        | 209/1034 [8:40:00<32:05:07, 140.01s/it] 20%|██        | 210/1034 [8:42:34<32:59:21, 144.13s/it]                                                        {'loss': 0.1318, 'grad_norm': 2.7859425655026007, 'learning_rate': 1.9377521321470806e-05, 'epoch': 0.41}
 20%|██        | 210/1034 [8:42:34<32:59:21, 144.13s/it] 20%|██        | 211/1034 [8:46:22<38:43:15, 169.38s/it] 21%|██        | 212/1034 [8:50:35<44:23:30, 194.42s/it] 21%|██        | 213/1034 [8:54:40<47:47:28, 209.56s/it] 21%|██        | 214/1034 [8:58:49<50:23:48, 221.25s/it] 21%|██        | 215/1034 [9:02:51<51:48:28, 227.73s/it] 21%|██        | 216/1034 [9:06:50<52:30:03, 231.06s/it] 21%|██        | 217/1034 [9:23:03<102:54:18, 453.44s/it] 21%|██        | 218/1034 [9:25:21<81:22:11, 358.98s/it]  21%|██        | 219/1034 [9:27:53<67:12:35, 296.88s/it] 21%|██▏       | 220/1034 [9:30:17<56:43:34, 250.88s/it]                                                        {'loss': 0.1384, 'grad_norm': 6.208847266446755, 'learning_rate': 1.9254871991635598e-05, 'epoch': 0.43}
 21%|██▏       | 220/1034 [9:30:17<56:43:34, 250.88s/it] 21%|██▏       | 221/1034 [9:32:47<49:51:04, 220.74s/it] 21%|██▏       | 222/1034 [9:35:23<45:24:29, 201.32s/it] 22%|██▏       | 223/1034 [9:37:49<41:35:46, 184.64s/it] 22%|██▏       | 224/1034 [9:40:21<39:19:31, 174.78s/it] 22%|██▏       | 225/1034 [9:42:55<37:54:04, 168.66s/it] 22%|██▏       | 226/1034 [9:45:19<36:13:24, 161.39s/it] 22%|██▏       | 227/1034 [9:47:46<35:08:55, 156.80s/it] 22%|██▏       | 228/1034 [9:50:09<34:13:58, 152.90s/it] 22%|██▏       | 229/1034 [9:53:04<35:36:59, 159.28s/it] 22%|██▏       | 230/1034 [9:55:29<34:38:49, 155.14s/it]                                                        {'loss': 0.1641, 'grad_norm': 3.1138127740201087, 'learning_rate': 1.9121662684969337e-05, 'epoch': 0.45}
 22%|██▏       | 230/1034 [9:55:29<34:38:49, 155.14s/it] 22%|██▏       | 231/1034 [9:57:45<33:20:14, 149.46s/it] 22%|██▏       | 232/1034 [10:00:37<34:47:36, 156.18s/it] 23%|██▎       | 233/1034 [10:03:30<35:50:34, 161.09s/it] 23%|██▎       | 234/1034 [10:06:15<36:05:03, 162.38s/it] 23%|██▎       | 235/1034 [10:09:04<36:29:06, 164.39s/it] 23%|██▎       | 236/1034 [10:11:50<36:32:19, 164.84s/it] 23%|██▎       | 237/1034 [10:14:21<35:36:30, 160.84s/it] 23%|██▎       | 238/1034 [10:17:02<35:34:16, 160.87s/it] 23%|██▎       | 239/1034 [10:19:49<35:52:56, 162.49s/it] 23%|██▎       | 240/1034 [10:22:25<35:25:58, 160.65s/it]                                                         {'loss': 0.1596, 'grad_norm': 7.945605453556443, 'learning_rate': 1.897804539570742e-05, 'epoch': 0.46}
 23%|██▎       | 240/1034 [10:22:25<35:25:58, 160.65s/it] 23%|██▎       | 241/1034 [10:25:04<35:17:45, 160.23s/it] 23%|██▎       | 242/1034 [10:27:51<35:39:44, 162.10s/it] 24%|██▎       | 243/1034 [10:30:27<35:13:02, 160.28s/it] 24%|██▎       | 244/1034 [10:33:03<34:55:55, 159.18s/it] 24%|██▎       | 245/1034 [10:35:43<34:53:00, 159.16s/it] 24%|██▍       | 246/1034 [10:38:15<34:23:10, 157.09s/it] 24%|██▍       | 247/1034 [10:40:48<34:07:07, 156.07s/it] 24%|██▍       | 248/1034 [10:43:18<33:40:38, 154.25s/it] 24%|██▍       | 249/1034 [10:45:45<33:06:42, 151.85s/it] 24%|██▍       | 250/1034 [10:48:15<32:58:36, 151.42s/it]                                                         {'loss': 0.1683, 'grad_norm': 3.175932571427395, 'learning_rate': 1.8824183993782193e-05, 'epoch': 0.48}
 24%|██▍       | 250/1034 [10:48:15<32:58:36, 151.42s/it] 24%|██▍       | 251/1034 [10:51:06<34:10:53, 157.16s/it] 24%|██▍       | 252/1034 [10:54:10<35:56:18, 165.45s/it] 24%|██▍       | 253/1034 [10:56:57<35:58:43, 165.84s/it] 25%|██▍       | 254/1034 [10:59:49<36:19:50, 167.68s/it] 25%|██▍       | 255/1034 [11:02:27<35:37:00, 164.60s/it] 25%|██▍       | 256/1034 [11:05:11<35:34:50, 164.64s/it] 25%|██▍       | 257/1034 [11:07:52<35:15:47, 163.38s/it] 25%|██▍       | 258/1034 [11:10:28<34:46:06, 161.30s/it] 25%|██▌       | 259/1034 [11:13:14<35:00:02, 162.58s/it] 25%|██▌       | 260/1034 [11:15:58<35:02:07, 162.96s/it]                                                         {'loss': 0.1413, 'grad_norm': 1.8810289040073953, 'learning_rate': 1.866025403784439e-05, 'epoch': 0.5}
 25%|██▌       | 260/1034 [11:15:58<35:02:07, 162.96s/it] 25%|██▌       | 261/1034 [11:18:35<34:39:35, 161.42s/it] 25%|██▌       | 262/1034 [11:21:03<33:42:56, 157.22s/it] 25%|██▌       | 263/1034 [11:23:40<33:39:54, 157.19s/it] 26%|██▌       | 264/1034 [11:26:14<33:25:11, 156.25s/it] 26%|██▌       | 265/1034 [11:28:59<33:57:08, 158.94s/it] 26%|██▌       | 266/1034 [11:31:38<33:55:14, 159.00s/it] 26%|██▌       | 267/1034 [11:34:17<33:49:28, 158.76s/it] 26%|██▌       | 268/1034 [11:36:52<33:35:32, 157.88s/it] 26%|██▌       | 269/1034 [11:39:21<32:56:00, 154.98s/it] 26%|██▌       | 270/1034 [11:41:51<32:33:51, 153.44s/it]                                                         {'loss': 0.1385, 'grad_norm': 4.186847861873451, 'learning_rate': 1.848644257494751e-05, 'epoch': 0.52}
 26%|██▌       | 270/1034 [11:41:51<32:33:51, 153.44s/it] 26%|██▌       | 271/1034 [11:45:04<35:04:46, 165.51s/it] 26%|██▋       | 272/1034 [11:47:51<35:07:54, 165.98s/it] 26%|██▋       | 273/1034 [11:50:18<33:51:50, 160.20s/it] 26%|██▋       | 274/1034 [11:52:49<33:12:32, 157.31s/it] 27%|██▋       | 275/1034 [11:55:42<34:09:25, 162.01s/it] 27%|██▋       | 276/1034 [11:58:00<32:38:09, 155.00s/it] 27%|██▋       | 277/1034 [12:00:32<32:25:17, 154.18s/it] 27%|██▋       | 278/1034 [12:02:56<31:40:43, 150.85s/it] 27%|██▋       | 279/1034 [12:05:29<31:49:12, 151.73s/it] 27%|██▋       | 280/1034 [12:07:58<31:33:46, 150.70s/it]                                                         {'loss': 0.1352, 'grad_norm': 1.1874452870386836, 'learning_rate': 1.8302947927123767e-05, 'epoch': 0.54}
 27%|██▋       | 280/1034 [12:07:58<31:33:46, 150.70s/it] 27%|██▋       | 281/1034 [12:10:39<32:11:02, 153.87s/it] 27%|██▋       | 282/1034 [12:13:23<32:48:13, 157.04s/it] 27%|██▋       | 283/1034 [12:16:10<33:20:28, 159.83s/it] 27%|██▋       | 284/1034 [12:18:52<33:26:07, 160.49s/it] 28%|██▊       | 285/1034 [12:21:25<32:56:34, 158.34s/it] 28%|██▊       | 286/1034 [12:23:50<32:02:47, 154.23s/it] 28%|██▊       | 287/1034 [12:26:20<31:47:19, 153.20s/it] 28%|██▊       | 288/1034 [12:28:44<31:08:17, 150.26s/it] 28%|██▊       | 289/1034 [12:31:15<31:08:20, 150.47s/it] 28%|██▊       | 290/1034 [12:34:33<34:04:01, 164.84s/it]                                                         {'loss': 0.1465, 'grad_norm': 2.240304609271756, 'learning_rate': 1.8109979465095014e-05, 'epoch': 0.56}
 28%|██▊       | 290/1034 [12:34:33<34:04:01, 164.84s/it] 28%|██▊       | 291/1034 [12:37:20<34:08:38, 165.44s/it] 28%|██▊       | 292/1034 [12:40:04<33:59:15, 164.90s/it] 28%|██▊       | 293/1034 [12:42:37<33:14:28, 161.50s/it] 28%|██▊       | 294/1034 [12:45:24<33:29:57, 162.97s/it] 29%|██▊       | 295/1034 [12:48:07<33:28:01, 163.03s/it] 29%|██▊       | 296/1034 [12:50:45<33:05:52, 161.45s/it] 29%|██▊       | 297/1034 [12:53:22<32:49:38, 160.35s/it] 29%|██▉       | 298/1034 [12:56:16<33:36:43, 164.41s/it] 29%|██▉       | 299/1034 [12:59:06<33:53:07, 165.97s/it] 29%|██▉       | 300/1034 [13:01:40<33:06:45, 162.41s/it]                                                         {'loss': 0.142, 'grad_norm': 1.5775769953517254, 'learning_rate': 1.7907757369376984e-05, 'epoch': 0.58}
 29%|██▉       | 300/1034 [13:01:40<33:06:45, 162.41s/it][INFO|trainer.py:4309] 2025-12-18 14:36:55,276 >> Saving model checkpoint to saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300
[INFO|configuration_utils.py:763] 2025-12-18 14:36:55,315 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-18 14:36:55,317 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2421] 2025-12-18 14:36:55,422 >> chat template saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-18 14:36:55,424 >> tokenizer config file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-18 14:36:55,424 >> Special tokens file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/special_tokens_map.json
[2025-12-18 14:36:57,740] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2025-12-18 14:36:57,785] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-18 14:36:57,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-18 14:36:57,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-18 14:36:57,830] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-18 14:36:58,061] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-18 14:36:58,064] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-18 14:36:58,112] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
 29%|██▉       | 301/1034 [13:05:09<35:54:27, 176.35s/it] 29%|██▉       | 302/1034 [13:07:52<35:02:05, 172.30s/it] 29%|██▉       | 303/1034 [13:10:19<33:28:08, 164.83s/it] 29%|██▉       | 304/1034 [13:13:03<33:21:16, 164.49s/it] 29%|██▉       | 305/1034 [13:15:34<32:31:01, 160.58s/it] 30%|██▉       | 306/1034 [13:18:03<31:44:31, 156.97s/it] 30%|██▉       | 307/1034 [13:20:36<31:26:44, 155.71s/it] 30%|██▉       | 308/1034 [13:23:07<31:08:56, 154.46s/it] 30%|██▉       | 309/1034 [13:26:19<33:20:40, 165.57s/it] 30%|██▉       | 310/1034 [13:28:49<32:22:55, 161.02s/it]                                                         {'loss': 0.1357, 'grad_norm': 1.870059854387132, 'learning_rate': 1.7696512379049323e-05, 'epoch': 0.6}
 30%|██▉       | 310/1034 [13:28:49<32:22:55, 161.02s/it] 30%|███       | 311/1034 [13:31:17<31:33:03, 157.10s/it] 30%|███       | 312/1034 [13:33:45<30:56:40, 154.29s/it] 30%|███       | 313/1034 [13:36:19<30:54:09, 154.30s/it] 30%|███       | 314/1034 [13:39:05<31:34:32, 157.88s/it] 30%|███       | 315/1034 [13:41:34<31:00:29, 155.26s/it] 31%|███       | 316/1034 [13:44:18<31:28:02, 157.78s/it] 31%|███       | 317/1034 [13:46:41<30:32:03, 153.31s/it] 31%|███       | 318/1034 [13:49:04<29:53:20, 150.28s/it] 31%|███       | 319/1034 [13:51:28<29:28:14, 148.38s/it] 31%|███       | 320/1034 [13:54:03<29:50:30, 150.46s/it]                                                         {'loss': 0.1423, 'grad_norm': 1.9049143513251605, 'learning_rate': 1.7476485528478093e-05, 'epoch': 0.62}
 31%|███       | 320/1034 [13:54:03<29:50:30, 150.46s/it] 31%|███       | 321/1034 [13:56:30<29:32:48, 149.18s/it] 31%|███       | 322/1034 [13:58:50<29:00:44, 146.69s/it] 31%|███       | 323/1034 [14:01:10<28:32:38, 144.53s/it] 31%|███▏      | 324/1034 [14:03:56<29:46:32, 150.98s/it] 31%|███▏      | 325/1034 [14:06:22<29:25:11, 149.38s/it] 32%|███▏      | 326/1034 [14:08:38<28:37:14, 145.53s/it] 32%|███▏      | 327/1034 [14:10:51<27:48:53, 141.63s/it] 32%|███▏      | 328/1034 [14:13:26<28:35:39, 145.81s/it] 32%|███▏      | 329/1034 [14:16:45<31:39:35, 161.67s/it] 32%|███▏      | 330/1034 [14:19:14<30:51:03, 157.76s/it]                                                         {'loss': 0.1169, 'grad_norm': 2.0621595996705686, 'learning_rate': 1.72479278722912e-05, 'epoch': 0.64}
 32%|███▏      | 330/1034 [14:19:14<30:51:03, 157.76s/it] 32%|███▏      | 331/1034 [14:22:11<31:57:25, 163.65s/it] 32%|███▏      | 332/1034 [14:24:44<31:19:19, 160.63s/it] 32%|███▏      | 333/1034 [14:27:03<29:59:53, 154.06s/it] 32%|███▏      | 334/1034 [14:29:26<29:16:59, 150.60s/it] 32%|███▏      | 335/1034 [14:32:08<29:56:10, 154.18s/it] 32%|███▏      | 336/1034 [14:34:34<29:24:18, 151.66s/it] 33%|███▎      | 337/1034 [14:37:26<30:33:09, 157.80s/it] 33%|███▎      | 338/1034 [14:39:57<30:06:05, 155.70s/it] 33%|███▎      | 339/1034 [14:42:30<29:55:33, 155.01s/it] 33%|███▎      | 340/1034 [14:45:04<29:47:53, 154.57s/it]                                                         {'loss': 0.1424, 'grad_norm': 2.587709463983073, 'learning_rate': 1.7011100198920528e-05, 'epoch': 0.66}
 33%|███▎      | 340/1034 [14:45:04<29:47:53, 154.57s/it] 33%|███▎      | 341/1034 [14:47:50<30:24:50, 157.99s/it] 33%|███▎      | 342/1034 [14:50:20<29:53:30, 155.51s/it] 33%|███▎      | 343/1034 [14:52:57<29:57:04, 156.04s/it] 33%|███▎      | 344/1034 [14:55:34<29:58:45, 156.41s/it] 33%|███▎      | 345/1034 [14:58:00<29:18:05, 153.10s/it] 33%|███▎      | 346/1034 [15:00:30<29:07:28, 152.40s/it] 34%|███▎      | 347/1034 [15:03:00<28:54:39, 151.50s/it] 34%|███▎      | 348/1034 [15:06:03<30:42:23, 161.14s/it] 34%|███▍      | 349/1034 [15:08:45<30:40:52, 161.25s/it] 34%|███▍      | 350/1034 [15:11:13<29:51:50, 157.18s/it]                                                         {'loss': 0.1471, 'grad_norm': 2.469595861967908, 'learning_rate': 1.6766272733037575e-05, 'epoch': 0.68}
 34%|███▍      | 350/1034 [15:11:13<29:51:50, 157.18s/it] 34%|███▍      | 351/1034 [15:13:38<29:09:45, 153.71s/it] 34%|███▍      | 352/1034 [15:16:09<28:57:29, 152.86s/it] 34%|███▍      | 353/1034 [15:18:44<29:02:37, 153.54s/it] 34%|███▍      | 354/1034 [15:21:15<28:49:52, 152.64s/it] 34%|███▍      | 355/1034 [15:23:44<28:34:39, 151.52s/it] 34%|███▍      | 356/1034 [15:26:19<28:44:55, 152.65s/it] 35%|███▍      | 357/1034 [15:28:53<28:47:23, 153.09s/it] 35%|███▍      | 358/1034 [15:31:17<28:12:48, 150.25s/it] 35%|███▍      | 359/1034 [15:33:49<28:15:54, 150.75s/it] 35%|███▍      | 360/1034 [15:36:22<28:22:55, 151.60s/it]                                                         {'loss': 0.1472, 'grad_norm': 4.239368547909626, 'learning_rate': 1.6513724827222225e-05, 'epoch': 0.7}
 35%|███▍      | 360/1034 [15:36:22<28:22:55, 151.60s/it] 35%|███▍      | 361/1034 [15:38:48<28:01:17, 149.89s/it] 35%|███▌      | 362/1034 [15:41:15<27:48:46, 149.00s/it] 35%|███▌      | 363/1034 [15:43:44<27:46:11, 148.99s/it] 35%|███▌      | 364/1034 [15:46:14<27:47:55, 149.37s/it] 35%|███▌      | 365/1034 [15:48:35<27:15:49, 146.71s/it] 35%|███▌      | 366/1034 [15:51:06<27:29:17, 148.14s/it] 35%|███▌      | 367/1034 [15:53:39<27:41:54, 149.50s/it] 36%|███▌      | 368/1034 [15:56:40<29:26:12, 159.12s/it] 36%|███▌      | 369/1034 [15:59:42<30:38:24, 165.87s/it] 36%|███▌      | 370/1034 [16:02:16<29:57:02, 162.38s/it]                                                         {'loss': 0.1624, 'grad_norm': 2.696163009346301, 'learning_rate': 1.625374464321637e-05, 'epoch': 0.72}
 36%|███▌      | 370/1034 [16:02:16<29:57:02, 162.38s/it] 36%|███▌      | 371/1034 [16:04:40<28:53:33, 156.88s/it] 36%|███▌      | 372/1034 [16:07:01<27:58:40, 152.15s/it] 36%|███▌      | 373/1034 [16:09:23<27:22:15, 149.07s/it] 36%|███▌      | 374/1034 [16:11:38<26:33:55, 144.90s/it] 36%|███▋      | 375/1034 [16:14:01<26:25:12, 144.33s/it] 36%|███▋      | 376/1034 [16:16:34<26:49:32, 146.77s/it] 36%|███▋      | 377/1034 [16:18:59<26:41:44, 146.28s/it] 37%|███▋      | 378/1034 [16:21:18<26:13:53, 143.95s/it] 37%|███▋      | 379/1034 [16:23:35<25:50:13, 142.01s/it] 37%|███▋      | 380/1034 [16:25:57<25:49:00, 142.11s/it]                                                         {'loss': 0.1897, 'grad_norm': 3.6826989688242784, 'learning_rate': 1.598662882312615e-05, 'epoch': 0.74}
 37%|███▋      | 380/1034 [16:25:57<25:49:00, 142.11s/it] 37%|███▋      | 381/1034 [16:28:02<24:50:00, 136.91s/it] 37%|███▋      | 382/1034 [16:30:44<26:07:32, 144.25s/it] 37%|███▋      | 383/1034 [16:33:07<26:02:30, 144.01s/it] 37%|███▋      | 384/1034 [16:35:22<25:29:36, 141.19s/it] 37%|███▋      | 385/1034 [16:37:58<26:15:54, 145.69s/it] 37%|███▋      | 386/1034 [16:40:23<26:12:29, 145.60s/it] 37%|███▋      | 387/1034 [16:42:45<25:56:27, 144.34s/it] 38%|███▊      | 388/1034 [16:45:28<26:55:03, 150.01s/it] 38%|███▊      | 389/1034 [16:47:43<26:04:15, 145.51s/it] 38%|███▊      | 390/1034 [16:50:11<26:11:50, 146.45s/it]                                                         {'loss': 0.1704, 'grad_norm': 4.443755811052909, 'learning_rate': 1.5712682150947926e-05, 'epoch': 0.76}
 38%|███▊      | 390/1034 [16:50:11<26:11:50, 146.45s/it] 38%|███▊      | 391/1034 [16:52:45<26:31:55, 148.55s/it] 38%|███▊      | 392/1034 [16:55:03<25:57:14, 145.54s/it] 38%|███▊      | 393/1034 [16:57:40<26:30:31, 148.88s/it] 38%|███▊      | 394/1034 [17:00:07<26:22:57, 148.40s/it] 38%|███▊      | 395/1034 [17:02:27<25:53:49, 145.90s/it] 38%|███▊      | 396/1034 [17:05:07<26:35:42, 150.07s/it] 38%|███▊      | 397/1034 [17:07:26<25:56:42, 146.63s/it] 38%|███▊      | 398/1034 [17:09:59<26:14:25, 148.53s/it] 39%|███▊      | 399/1034 [17:12:32<26:26:06, 149.87s/it] 39%|███▊      | 400/1034 [17:14:48<25:41:48, 145.91s/it]                                                         {'loss': 0.1418, 'grad_norm': 1.9201227387991937, 'learning_rate': 1.543221720480419e-05, 'epoch': 0.77}
 39%|███▊      | 400/1034 [17:14:48<25:41:48, 145.91s/it] 39%|███▉      | 401/1034 [17:17:22<26:03:18, 148.18s/it] 39%|███▉      | 402/1034 [17:19:58<26:24:49, 150.46s/it] 39%|███▉      | 403/1034 [17:22:26<26:15:39, 149.83s/it] 39%|███▉      | 404/1034 [17:25:12<27:02:46, 154.55s/it] 39%|███▉      | 405/1034 [17:27:45<26:57:51, 154.33s/it] 39%|███▉      | 406/1034 [17:30:25<27:13:08, 156.03s/it] 39%|███▉      | 407/1034 [17:33:00<27:06:49, 155.68s/it] 39%|███▉      | 408/1034 [17:35:32<26:51:36, 154.47s/it] 40%|███▉      | 409/1034 [17:38:01<26:33:10, 152.94s/it] 40%|███▉      | 410/1034 [17:40:27<26:07:09, 150.69s/it]                                                         {'loss': 0.156, 'grad_norm': 2.2411574480260485, 'learning_rate': 1.514555400028629e-05, 'epoch': 0.79}
 40%|███▉      | 410/1034 [17:40:27<26:07:09, 150.69s/it] 40%|███▉      | 411/1034 [17:42:58<26:05:21, 150.76s/it] 40%|███▉      | 412/1034 [17:45:24<25:49:45, 149.49s/it] 40%|███▉      | 413/1034 [17:47:47<25:25:56, 147.43s/it] 40%|████      | 414/1034 [17:50:19<25:38:57, 148.93s/it] 40%|████      | 415/1034 [17:52:33<24:48:36, 144.29s/it] 40%|████      | 416/1034 [17:55:07<25:16:31, 147.24s/it] 40%|████      | 417/1034 [17:57:32<25:06:30, 146.50s/it] 40%|████      | 418/1034 [17:59:50<24:39:51, 144.14s/it] 41%|████      | 419/1034 [18:02:33<25:35:26, 149.80s/it] 41%|████      | 420/1034 [18:04:54<25:03:35, 146.93s/it]                                                         {'loss': 0.1173, 'grad_norm': 2.9068448849716413, 'learning_rate': 1.4853019625310813e-05, 'epoch': 0.81}
 41%|████      | 420/1034 [18:04:54<25:03:35, 146.93s/it] 41%|████      | 421/1034 [18:07:18<24:54:43, 146.30s/it] 41%|████      | 422/1034 [18:10:04<25:51:26, 152.10s/it] 41%|████      | 423/1034 [18:12:26<25:19:10, 149.18s/it] 41%|████      | 424/1034 [18:14:51<25:03:46, 147.91s/it] 41%|████      | 425/1034 [18:17:29<25:30:42, 150.81s/it] 41%|████      | 426/1034 [18:19:50<24:58:03, 147.83s/it] 41%|████▏     | 427/1034 [18:22:13<24:41:59, 146.49s/it] 41%|████▏     | 428/1034 [18:24:56<25:30:19, 151.52s/it] 41%|████▏     | 429/1034 [18:27:23<25:13:10, 150.07s/it] 42%|████▏     | 430/1034 [18:29:48<24:55:58, 148.61s/it]                                                         {'loss': 0.1431, 'grad_norm': 2.37223352969677, 'learning_rate': 1.455494786690634e-05, 'epoch': 0.83}
 42%|████▏     | 430/1034 [18:29:48<24:55:58, 148.61s/it] 42%|████▏     | 431/1034 [18:32:33<25:40:44, 153.31s/it] 42%|████▏     | 432/1034 [18:34:49<24:47:54, 148.30s/it] 42%|████▏     | 433/1034 [18:37:16<24:41:47, 147.93s/it] 42%|████▏     | 434/1034 [18:39:57<25:18:48, 151.88s/it] 42%|████▏     | 435/1034 [18:42:09<24:15:36, 145.80s/it] 42%|████▏     | 436/1034 [18:44:38<24:21:39, 146.65s/it] 42%|████▏     | 437/1034 [18:47:12<24:41:00, 148.84s/it] 42%|████▏     | 438/1034 [18:49:34<24:18:49, 146.86s/it] 42%|████▏     | 439/1034 [18:51:49<23:42:57, 143.49s/it] 43%|████▎     | 440/1034 [18:54:39<24:58:03, 151.32s/it]                                                         {'loss': 0.1186, 'grad_norm': 3.7552283323860203, 'learning_rate': 1.4251678830356408e-05, 'epoch': 0.85}
 43%|████▎     | 440/1034 [18:54:39<24:58:03, 151.32s/it] 43%|████▎     | 441/1034 [18:57:03<24:32:39, 149.00s/it] 43%|████▎     | 442/1034 [18:59:22<24:01:47, 146.13s/it] 43%|████▎     | 443/1034 [19:01:55<24:20:51, 148.31s/it] 43%|████▎     | 444/1034 [19:04:21<24:11:29, 147.61s/it] 43%|████▎     | 445/1034 [19:06:49<24:07:43, 147.48s/it] 43%|████▎     | 446/1034 [19:09:26<24:33:18, 150.34s/it] 43%|████▎     | 447/1034 [19:11:51<24:16:03, 148.83s/it] 43%|████▎     | 448/1034 [19:14:31<24:46:33, 152.21s/it] 43%|████▎     | 449/1034 [19:16:59<24:32:37, 151.04s/it] 44%|████▎     | 450/1034 [19:19:25<24:16:00, 149.59s/it]                                                         {'loss': 0.1244, 'grad_norm': 2.5033996453020104, 'learning_rate': 1.3943558551133186e-05, 'epoch': 0.87}
 44%|████▎     | 450/1034 [19:19:26<24:16:00, 149.59s/it] 44%|████▎     | 451/1034 [19:22:04<24:40:08, 152.33s/it] 44%|████▎     | 452/1034 [19:24:27<24:08:25, 149.32s/it] 44%|████▍     | 453/1034 [19:26:56<24:05:42, 149.30s/it] 44%|████▍     | 454/1034 [19:29:28<24:11:37, 150.17s/it] 44%|████▍     | 455/1034 [19:31:51<23:47:53, 147.97s/it] 44%|████▍     | 456/1034 [19:34:21<23:53:08, 148.77s/it] 44%|████▍     | 457/1034 [19:36:49<23:48:28, 148.54s/it] 44%|████▍     | 458/1034 [19:39:12<23:29:01, 146.77s/it] 44%|████▍     | 459/1034 [19:41:41<23:33:16, 147.47s/it] 44%|████▍     | 460/1034 [19:43:56<22:55:46, 143.81s/it]                                                         {'loss': 0.1211, 'grad_norm': 1.1637345388230889, 'learning_rate': 1.3630938600064748e-05, 'epoch': 0.89}
 44%|████▍     | 460/1034 [19:43:56<22:55:46, 143.81s/it] 45%|████▍     | 461/1034 [19:46:15<22:38:36, 142.26s/it] 45%|████▍     | 462/1034 [19:48:47<23:03:40, 145.14s/it] 45%|████▍     | 463/1034 [19:51:15<23:08:06, 145.86s/it] 45%|████▍     | 464/1034 [19:53:35<22:51:05, 144.33s/it] 45%|████▍     | 465/1034 [19:55:54<22:33:17, 142.70s/it] 45%|████▌     | 466/1034 [19:58:27<22:59:24, 145.71s/it] 45%|████▌     | 467/1034 [20:01:03<23:26:45, 148.86s/it] 45%|████▌     | 468/1034 [20:03:31<23:20:43, 148.49s/it] 45%|████▌     | 469/1034 [20:05:52<22:58:29, 146.39s/it] 45%|████▌     | 470/1034 [20:08:33<23:37:51, 150.84s/it]                                                         {'loss': 0.1315, 'grad_norm': 1.2631130791835399, 'learning_rate': 1.3314175682186358e-05, 'epoch': 0.91}
 45%|████▌     | 470/1034 [20:08:33<23:37:51, 150.84s/it] 46%|████▌     | 471/1034 [20:11:05<23:37:21, 151.05s/it] 46%|████▌     | 472/1034 [20:13:24<23:01:54, 147.53s/it] 46%|████▌     | 473/1034 [20:16:00<23:21:43, 149.92s/it] 46%|████▌     | 474/1034 [20:18:39<23:44:58, 152.68s/it] 46%|████▌     | 475/1034 [20:20:52<22:47:41, 146.80s/it] 46%|████▌     | 476/1034 [20:23:25<23:02:40, 148.68s/it] 46%|████▌     | 477/1034 [20:25:59<23:14:04, 150.17s/it] 46%|████▌     | 478/1034 [20:28:09<22:16:50, 144.26s/it] 46%|████▋     | 479/1034 [20:30:46<22:49:51, 148.09s/it] 46%|████▋     | 480/1034 [20:33:27<23:23:15, 151.98s/it]                                                         {'loss': 0.1371, 'grad_norm': 3.5771231253798272, 'learning_rate': 1.2993631229733584e-05, 'epoch': 0.93}
 46%|████▋     | 480/1034 [20:33:27<23:23:15, 151.98s/it] 47%|████▋     | 481/1034 [20:35:46<22:43:32, 147.94s/it] 47%|████▋     | 482/1034 [20:38:28<23:21:19, 152.32s/it] 47%|████▋     | 483/1034 [20:41:04<23:26:46, 153.19s/it] 47%|████▋     | 484/1034 [20:43:25<22:52:27, 149.72s/it] 47%|████▋     | 485/1034 [20:46:06<23:19:30, 152.95s/it] 47%|████▋     | 486/1034 [20:48:41<23:24:23, 153.77s/it] 47%|████▋     | 487/1034 [20:51:01<22:43:53, 149.60s/it] 47%|████▋     | 488/1034 [20:53:43<23:15:27, 153.35s/it] 47%|████▋     | 489/1034 [20:56:23<23:30:47, 155.32s/it] 47%|████▋     | 490/1034 [20:58:46<22:53:53, 151.53s/it]                                                         {'loss': 0.1018, 'grad_norm': 1.1640544126391128, 'learning_rate': 1.2669670989741519e-05, 'epoch': 0.95}
 47%|████▋     | 490/1034 [20:58:46<22:53:53, 151.53s/it] 47%|████▋     | 491/1034 [21:01:20<22:59:34, 152.44s/it] 48%|████▊     | 492/1034 [21:04:03<23:25:30, 155.59s/it] 48%|████▊     | 493/1034 [21:06:24<22:41:23, 150.99s/it] 48%|████▊     | 494/1034 [21:08:52<22:31:56, 150.22s/it] 48%|████▊     | 495/1034 [21:11:38<23:11:57, 154.95s/it] 48%|████▊     | 496/1034 [21:13:54<22:19:07, 149.35s/it] 48%|████▊     | 497/1034 [21:16:12<21:46:27, 145.97s/it] 48%|████▊     | 498/1034 [21:19:00<22:40:35, 152.31s/it] 48%|████▊     | 499/1034 [21:21:18<22:01:37, 148.22s/it] 48%|████▊     | 500/1034 [21:23:29<21:11:52, 142.91s/it]                                                         {'loss': 0.1311, 'grad_norm': 1.9176494363857286, 'learning_rate': 1.2342664606720823e-05, 'epoch': 0.97}
 48%|████▊     | 500/1034 [21:23:29<21:11:52, 142.91s/it][INFO|trainer.py:4643] 2025-12-18 22:58:09,910 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-18 22:58:09,911 >>   Num examples = 2754
[INFO|trainer.py:4648] 2025-12-18 22:58:09,911 >>   Batch size = 2

  0%|          | 0/459 [00:00<?, ?it/s][A
  0%|          | 2/459 [00:04<18:47,  2.47s/it][A
  1%|          | 3/459 [00:09<26:11,  3.45s/it][A
  1%|          | 4/459 [00:14<30:15,  3.99s/it][A
  1%|          | 5/459 [00:19<32:34,  4.31s/it][A
  1%|▏         | 6/459 [00:24<33:56,  4.50s/it][A
  2%|▏         | 7/459 [00:29<35:11,  4.67s/it][A
  2%|▏         | 8/459 [00:34<36:33,  4.86s/it][A
  2%|▏         | 9/459 [00:40<37:29,  5.00s/it][A
  2%|▏         | 10/459 [00:45<38:08,  5.10s/it][A
  2%|▏         | 11/459 [00:50<38:10,  5.11s/it][A
  3%|▎         | 12/459 [00:55<38:11,  5.13s/it][A
  3%|▎         | 13/459 [01:00<38:24,  5.17s/it][A
  3%|▎         | 14/459 [01:06<38:24,  5.18s/it][A
  3%|▎         | 15/459 [01:11<39:06,  5.28s/it][A
  3%|▎         | 16/459 [01:17<40:31,  5.49s/it][A
  4%|▎         | 17/459 [01:23<40:33,  5.51s/it][A
  4%|▍         | 18/459 [01:28<40:05,  5.46s/it][A
  4%|▍         | 19/459 [01:33<39:32,  5.39s/it][A
  4%|▍         | 20/459 [01:38<39:02,  5.34s/it][A
  5%|▍         | 21/459 [01:44<38:57,  5.34s/it][A
  5%|▍         | 22/459 [01:49<38:44,  5.32s/it][A
  5%|▌         | 23/459 [01:54<38:39,  5.32s/it][A
  5%|▌         | 24/459 [02:00<38:49,  5.35s/it][A
  5%|▌         | 25/459 [02:05<38:43,  5.35s/it][A
  6%|▌         | 26/459 [02:10<38:29,  5.33s/it][A
  6%|▌         | 27/459 [02:16<38:49,  5.39s/it][A
  6%|▌         | 28/459 [02:21<38:38,  5.38s/it][A
  6%|▋         | 29/459 [02:27<38:14,  5.34s/it][A
  7%|▋         | 30/459 [02:32<37:56,  5.31s/it][A
  7%|▋         | 31/459 [02:38<38:52,  5.45s/it][A
  7%|▋         | 32/459 [02:43<39:19,  5.53s/it][A
  7%|▋         | 33/459 [02:49<39:38,  5.58s/it][A
  7%|▋         | 34/459 [02:55<39:34,  5.59s/it][A
  8%|▊         | 35/459 [03:00<39:34,  5.60s/it][A
  8%|▊         | 36/459 [03:06<39:12,  5.56s/it][A
  8%|▊         | 37/459 [03:11<38:07,  5.42s/it][A
  8%|▊         | 38/459 [03:16<37:46,  5.38s/it][A
  8%|▊         | 39/459 [03:21<37:24,  5.34s/it][A
  9%|▊         | 40/459 [03:27<36:56,  5.29s/it][A
  9%|▉         | 41/459 [03:32<36:52,  5.29s/it][A
  9%|▉         | 42/459 [03:37<35:59,  5.18s/it][A
  9%|▉         | 43/459 [03:42<35:13,  5.08s/it][A
 10%|▉         | 44/459 [03:46<34:34,  5.00s/it][A
 10%|▉         | 45/459 [03:51<33:49,  4.90s/it][A
 10%|█         | 46/459 [03:56<33:27,  4.86s/it][A
 10%|█         | 47/459 [04:01<33:09,  4.83s/it][A
 10%|█         | 48/459 [04:05<32:51,  4.80s/it][A
 11%|█         | 49/459 [04:10<32:45,  4.79s/it][A
 11%|█         | 50/459 [04:15<32:27,  4.76s/it][A
 11%|█         | 51/459 [04:20<32:22,  4.76s/it][A
 11%|█▏        | 52/459 [04:24<32:15,  4.76s/it][A
 12%|█▏        | 53/459 [04:29<31:54,  4.72s/it][A
 12%|█▏        | 54/459 [04:34<31:48,  4.71s/it][A
 12%|█▏        | 55/459 [04:38<31:50,  4.73s/it][A
 12%|█▏        | 56/459 [04:43<31:23,  4.67s/it][A
 12%|█▏        | 57/459 [04:47<30:01,  4.48s/it][A
 13%|█▎        | 58/459 [04:51<28:56,  4.33s/it][A
 13%|█▎        | 59/459 [04:55<28:09,  4.22s/it][A
 13%|█▎        | 60/459 [04:59<27:40,  4.16s/it][A
 13%|█▎        | 61/459 [05:03<27:19,  4.12s/it][A
 14%|█▎        | 62/459 [05:07<27:01,  4.09s/it][A
 14%|█▎        | 63/459 [05:11<26:56,  4.08s/it][A
 14%|█▍        | 64/459 [05:15<26:45,  4.06s/it][A
 14%|█▍        | 65/459 [05:19<26:42,  4.07s/it][A
 14%|█▍        | 66/459 [05:23<26:30,  4.05s/it][A
 15%|█▍        | 67/459 [05:27<26:27,  4.05s/it][A
 15%|█▍        | 68/459 [05:31<26:32,  4.07s/it][A
 15%|█▌        | 69/459 [05:35<26:37,  4.10s/it][A
 15%|█▌        | 70/459 [05:40<26:37,  4.11s/it][A
 15%|█▌        | 71/459 [05:44<26:30,  4.10s/it][A
 16%|█▌        | 72/459 [05:48<26:29,  4.11s/it][A
 16%|█▌        | 73/459 [05:52<26:30,  4.12s/it][A
 16%|█▌        | 74/459 [05:56<26:29,  4.13s/it][A
 16%|█▋        | 75/459 [06:00<26:24,  4.13s/it][A
 17%|█▋        | 76/459 [06:04<26:25,  4.14s/it][A
 17%|█▋        | 77/459 [06:09<26:26,  4.15s/it][A
 17%|█▋        | 78/459 [06:13<26:19,  4.14s/it][A
 17%|█▋        | 79/459 [06:17<26:24,  4.17s/it][A
 17%|█▋        | 80/459 [06:21<26:23,  4.18s/it][A
 18%|█▊        | 81/459 [06:25<26:21,  4.18s/it][A
 18%|█▊        | 82/459 [06:29<26:12,  4.17s/it][A
 18%|█▊        | 83/459 [06:34<26:06,  4.16s/it][A
 18%|█▊        | 84/459 [06:38<26:01,  4.16s/it][A
 19%|█▊        | 85/459 [06:42<25:58,  4.17s/it][A
 19%|█▊        | 86/459 [06:46<25:56,  4.17s/it][A
 19%|█▉        | 87/459 [06:50<25:54,  4.18s/it][A
 19%|█▉        | 88/459 [06:54<25:44,  4.16s/it][A
 19%|█▉        | 89/459 [06:59<25:47,  4.18s/it][A
 20%|█▉        | 90/459 [07:03<25:50,  4.20s/it][A
 20%|█▉        | 91/459 [07:07<25:43,  4.19s/it][A
 20%|██        | 92/459 [07:11<25:37,  4.19s/it][A
 20%|██        | 93/459 [07:15<25:31,  4.18s/it][A
 20%|██        | 94/459 [07:20<25:24,  4.18s/it][A
 21%|██        | 95/459 [07:24<25:38,  4.23s/it][A
 21%|██        | 96/459 [07:28<26:00,  4.30s/it][A
 21%|██        | 97/459 [07:33<26:35,  4.41s/it][A
 21%|██▏       | 98/459 [07:38<27:26,  4.56s/it][A
 22%|██▏       | 99/459 [07:43<27:56,  4.66s/it][A
 22%|██▏       | 100/459 [07:48<28:17,  4.73s/it][A
 22%|██▏       | 101/459 [07:53<28:27,  4.77s/it][A
 22%|██▏       | 102/459 [07:58<28:41,  4.82s/it][A
 22%|██▏       | 103/459 [08:02<28:43,  4.84s/it][A
 23%|██▎       | 104/459 [08:07<28:44,  4.86s/it][A
 23%|██▎       | 105/459 [08:12<29:00,  4.92s/it][A
 23%|██▎       | 106/459 [08:17<29:05,  4.95s/it][A
 23%|██▎       | 107/459 [08:22<28:45,  4.90s/it][A
 24%|██▎       | 108/459 [08:27<28:36,  4.89s/it][A
 24%|██▎       | 109/459 [08:32<28:36,  4.90s/it][A
 24%|██▍       | 110/459 [08:37<28:39,  4.93s/it][A
 24%|██▍       | 111/459 [08:42<28:26,  4.90s/it][A
 24%|██▍       | 112/459 [08:47<28:20,  4.90s/it][A
 25%|██▍       | 113/459 [08:52<28:17,  4.91s/it][A
 25%|██▍       | 114/459 [08:57<28:19,  4.93s/it][A
 25%|██▌       | 115/459 [09:02<28:23,  4.95s/it][A
 25%|██▌       | 116/459 [09:07<28:25,  4.97s/it][A
 25%|██▌       | 117/459 [09:12<28:19,  4.97s/it][A
 26%|██▌       | 118/459 [09:17<29:33,  5.20s/it][A
 26%|██▌       | 119/459 [09:23<30:52,  5.45s/it][A
 26%|██▌       | 120/459 [09:29<31:37,  5.60s/it][A
 26%|██▋       | 121/459 [09:35<31:55,  5.67s/it][A
 27%|██▋       | 122/459 [09:41<32:29,  5.79s/it][A
 27%|██▋       | 123/459 [09:47<32:45,  5.85s/it][A
 27%|██▋       | 124/459 [09:53<32:55,  5.90s/it][A
 27%|██▋       | 125/459 [09:59<33:11,  5.96s/it][A
 27%|██▋       | 126/459 [10:05<33:18,  6.00s/it][A
 28%|██▊       | 127/459 [10:12<33:19,  6.02s/it][A
 28%|██▊       | 128/459 [10:18<33:12,  6.02s/it][A
 28%|██▊       | 129/459 [10:24<33:15,  6.05s/it][A
 28%|██▊       | 130/459 [10:30<33:00,  6.02s/it][A
 29%|██▊       | 131/459 [10:35<32:37,  5.97s/it][A
 29%|██▉       | 132/459 [10:41<32:17,  5.92s/it][A
 29%|██▉       | 133/459 [10:47<31:25,  5.78s/it][A
 29%|██▉       | 134/459 [10:52<30:10,  5.57s/it][A
 29%|██▉       | 135/459 [10:57<29:34,  5.48s/it][A
 30%|██▉       | 136/459 [11:02<28:51,  5.36s/it][A
 30%|██▉       | 137/459 [11:07<28:00,  5.22s/it][A
 30%|███       | 138/459 [11:12<27:37,  5.16s/it][A
 30%|███       | 139/459 [11:17<26:43,  5.01s/it][A
 31%|███       | 140/459 [11:21<26:05,  4.91s/it][A
 31%|███       | 141/459 [11:26<25:39,  4.84s/it][A
 31%|███       | 142/459 [11:31<25:21,  4.80s/it][A
 31%|███       | 143/459 [11:36<25:15,  4.79s/it][A
 31%|███▏      | 144/459 [11:40<25:12,  4.80s/it][A
 32%|███▏      | 145/459 [11:45<25:16,  4.83s/it][A
 32%|███▏      | 146/459 [11:50<25:18,  4.85s/it][A
 32%|███▏      | 147/459 [11:55<25:22,  4.88s/it][A
 32%|███▏      | 148/459 [12:00<25:21,  4.89s/it][A
 32%|███▏      | 149/459 [12:05<25:23,  4.91s/it][A
 33%|███▎      | 150/459 [12:10<25:12,  4.90s/it][A
 33%|███▎      | 151/459 [12:15<25:08,  4.90s/it][A
 33%|███▎      | 152/459 [12:20<24:55,  4.87s/it][A
 33%|███▎      | 153/459 [12:24<24:50,  4.87s/it][A
 34%|███▎      | 154/459 [12:29<24:42,  4.86s/it][A
 34%|███▍      | 155/459 [12:34<24:25,  4.82s/it][A
 34%|███▍      | 156/459 [12:38<23:20,  4.62s/it][A
 34%|███▍      | 157/459 [12:42<22:35,  4.49s/it][A
 34%|███▍      | 158/459 [12:47<22:05,  4.40s/it][A
 35%|███▍      | 159/459 [12:51<21:40,  4.34s/it][A
 35%|███▍      | 160/459 [12:55<21:25,  4.30s/it][A
 35%|███▌      | 161/459 [12:59<21:08,  4.26s/it][A
 35%|███▌      | 162/459 [13:03<20:59,  4.24s/it][A
 36%|███▌      | 163/459 [13:08<20:55,  4.24s/it][A
 36%|███▌      | 164/459 [13:12<20:48,  4.23s/it][A
 36%|███▌      | 165/459 [13:16<20:44,  4.23s/it][A
 36%|███▌      | 166/459 [13:20<20:36,  4.22s/it][A
 36%|███▋      | 167/459 [13:24<20:34,  4.23s/it][A
 37%|███▋      | 168/459 [13:29<20:23,  4.21s/it][A
 37%|███▋      | 169/459 [13:33<20:32,  4.25s/it][A
 37%|███▋      | 170/459 [13:37<20:42,  4.30s/it][A
 37%|███▋      | 171/459 [13:42<20:48,  4.33s/it][A
 37%|███▋      | 172/459 [13:46<21:07,  4.42s/it][A
 38%|███▊      | 173/459 [13:51<21:06,  4.43s/it][A
 38%|███▊      | 174/459 [13:55<21:10,  4.46s/it][A
 38%|███▊      | 175/459 [14:00<21:14,  4.49s/it][A
 38%|███▊      | 176/459 [14:04<21:04,  4.47s/it][A
 39%|███▊      | 177/459 [14:09<20:59,  4.47s/it][A
 39%|███▉      | 178/459 [14:13<20:55,  4.47s/it][A
 39%|███▉      | 179/459 [14:18<20:58,  4.49s/it][A
 39%|███▉      | 180/459 [14:23<21:11,  4.56s/it][A
 39%|███▉      | 181/459 [14:27<21:15,  4.59s/it][A
 40%|███▉      | 182/459 [14:32<21:09,  4.58s/it][A
 40%|███▉      | 183/459 [14:36<20:54,  4.55s/it][A
 40%|████      | 184/459 [14:41<20:32,  4.48s/it][A
 40%|████      | 185/459 [14:45<20:06,  4.40s/it][A
 41%|████      | 186/459 [14:49<19:43,  4.34s/it][A
 41%|████      | 187/459 [14:53<19:33,  4.31s/it][A
 41%|████      | 188/459 [14:58<19:27,  4.31s/it][A
 41%|████      | 189/459 [15:02<19:25,  4.32s/it][A
 41%|████▏     | 190/459 [15:06<19:14,  4.29s/it][A
 42%|████▏     | 191/459 [15:10<19:05,  4.28s/it][A
 42%|████▏     | 192/459 [15:15<19:49,  4.45s/it][A
 42%|████▏     | 193/459 [15:20<20:22,  4.60s/it][A
 42%|████▏     | 194/459 [15:25<20:47,  4.71s/it][A
 42%|████▏     | 195/459 [15:30<20:58,  4.77s/it][A
 43%|████▎     | 196/459 [15:35<21:04,  4.81s/it][A
 43%|████▎     | 197/459 [15:40<21:07,  4.84s/it][A
 43%|████▎     | 198/459 [15:45<21:08,  4.86s/it][A
 43%|████▎     | 199/459 [15:50<21:07,  4.87s/it][A
 44%|████▎     | 200/459 [15:54<20:57,  4.86s/it][A
 44%|████▍     | 201/459 [15:59<20:53,  4.86s/it][A
 44%|████▍     | 202/459 [16:04<20:56,  4.89s/it][A
 44%|████▍     | 203/459 [16:09<20:55,  4.90s/it][A
 44%|████▍     | 204/459 [16:14<20:45,  4.88s/it][A
 45%|████▍     | 205/459 [16:19<20:42,  4.89s/it][A
 45%|████▍     | 206/459 [16:24<20:38,  4.89s/it][A
 45%|████▌     | 207/459 [16:29<20:32,  4.89s/it][A
 45%|████▌     | 208/459 [16:34<20:20,  4.86s/it][A
 46%|████▌     | 209/459 [16:38<20:16,  4.86s/it][A
 46%|████▌     | 210/459 [16:43<20:11,  4.86s/it][A
 46%|████▌     | 211/459 [16:48<20:06,  4.87s/it][A
 46%|████▌     | 212/459 [16:53<20:08,  4.89s/it][A
 46%|████▋     | 213/459 [16:58<19:57,  4.87s/it][A
 47%|████▋     | 214/459 [17:03<19:59,  4.90s/it][A
 47%|████▋     | 215/459 [17:08<20:35,  5.06s/it][A
 47%|████▋     | 216/459 [17:14<21:01,  5.19s/it][A
 47%|████▋     | 217/459 [17:19<21:20,  5.29s/it][A
 47%|████▋     | 218/459 [17:25<21:33,  5.37s/it][A
 48%|████▊     | 219/459 [17:30<21:38,  5.41s/it][A
 48%|████▊     | 220/459 [17:36<21:46,  5.46s/it][A
 48%|████▊     | 221/459 [17:42<21:47,  5.49s/it][A
 48%|████▊     | 222/459 [17:47<21:44,  5.50s/it][A
 49%|████▊     | 223/459 [17:53<21:42,  5.52s/it][A
 49%|████▉     | 224/459 [17:58<21:40,  5.53s/it][A
 49%|████▉     | 225/459 [18:04<21:35,  5.53s/it][A
 49%|████▉     | 226/459 [18:09<21:21,  5.50s/it][A
 49%|████▉     | 227/459 [18:14<20:50,  5.39s/it][A
 50%|████▉     | 228/459 [18:19<20:21,  5.29s/it][A
 50%|████▉     | 229/459 [18:24<19:57,  5.20s/it][A
 50%|█████     | 230/459 [18:30<19:55,  5.22s/it][A
 50%|█████     | 231/459 [18:35<19:47,  5.21s/it][A
 51%|█████     | 232/459 [18:40<19:41,  5.20s/it][A
 51%|█████     | 233/459 [18:45<19:33,  5.19s/it][A
 51%|█████     | 234/459 [18:50<19:22,  5.17s/it][A
 51%|█████     | 235/459 [18:55<19:02,  5.10s/it][A
 51%|█████▏    | 236/459 [19:00<18:44,  5.04s/it][A
 52%|█████▏    | 237/459 [19:05<18:33,  5.02s/it][A
 52%|█████▏    | 238/459 [19:10<18:07,  4.92s/it][A
 52%|█████▏    | 239/459 [19:14<17:48,  4.86s/it][A
 52%|█████▏    | 240/459 [19:19<17:34,  4.82s/it][A
 53%|█████▎    | 241/459 [19:24<17:21,  4.78s/it][A
 53%|█████▎    | 242/459 [19:28<17:04,  4.72s/it][A
 53%|█████▎    | 243/459 [19:33<16:51,  4.68s/it][A
 53%|█████▎    | 244/459 [19:38<16:50,  4.70s/it][A
 53%|█████▎    | 245/459 [19:43<16:50,  4.72s/it][A
 54%|█████▎    | 246/459 [19:47<16:44,  4.71s/it][A
 54%|█████▍    | 247/459 [19:52<16:38,  4.71s/it][A
 54%|█████▍    | 248/459 [19:57<16:34,  4.71s/it][A
 54%|█████▍    | 249/459 [20:01<16:29,  4.71s/it][A
 54%|█████▍    | 250/459 [20:06<16:25,  4.72s/it][A
 55%|█████▍    | 251/459 [20:11<16:25,  4.74s/it][A
 55%|█████▍    | 252/459 [20:16<16:18,  4.73s/it][A
 55%|█████▌    | 253/459 [20:20<16:11,  4.71s/it][A
 55%|█████▌    | 254/459 [20:25<15:44,  4.61s/it][A
 56%|█████▌    | 255/459 [20:29<15:09,  4.46s/it][A
 56%|█████▌    | 256/459 [20:33<14:45,  4.36s/it][A
 56%|█████▌    | 257/459 [20:37<14:29,  4.31s/it][A
 56%|█████▌    | 258/459 [20:41<14:21,  4.28s/it][A
 56%|█████▋    | 259/459 [20:46<14:14,  4.27s/it][A
 57%|█████▋    | 260/459 [20:50<14:10,  4.27s/it][A
 57%|█████▋    | 261/459 [20:54<14:08,  4.29s/it][A
 57%|█████▋    | 262/459 [20:58<14:05,  4.29s/it][A
 57%|█████▋    | 263/459 [21:03<13:54,  4.26s/it][A
 58%|█████▊    | 264/459 [21:07<13:44,  4.23s/it][A
 58%|█████▊    | 265/459 [21:11<13:37,  4.21s/it][A
 58%|█████▊    | 266/459 [21:15<13:32,  4.21s/it][A
 58%|█████▊    | 267/459 [21:19<13:33,  4.24s/it][A
 58%|█████▊    | 268/459 [21:24<13:40,  4.29s/it][A
 59%|█████▊    | 269/459 [21:28<13:31,  4.27s/it][A
 59%|█████▉    | 270/459 [21:32<13:16,  4.22s/it][A
 59%|█████▉    | 271/459 [21:36<13:06,  4.18s/it][A
 59%|█████▉    | 272/459 [21:40<12:56,  4.15s/it][A
 59%|█████▉    | 273/459 [21:44<12:47,  4.13s/it][A
 60%|█████▉    | 274/459 [21:49<12:39,  4.10s/it][A
 60%|█████▉    | 275/459 [21:53<12:29,  4.07s/it][A
 60%|██████    | 276/459 [21:57<12:20,  4.05s/it][A
 60%|██████    | 277/459 [22:01<12:15,  4.04s/it][A
 61%|██████    | 278/459 [22:05<12:13,  4.05s/it][A
 61%|██████    | 279/459 [22:09<12:15,  4.08s/it][A
 61%|██████    | 280/459 [22:13<12:14,  4.10s/it][A
 61%|██████    | 281/459 [22:17<12:10,  4.11s/it][A
 61%|██████▏   | 282/459 [22:21<12:02,  4.08s/it][A
 62%|██████▏   | 283/459 [22:25<11:57,  4.07s/it][A
 62%|██████▏   | 284/459 [22:29<11:57,  4.10s/it][A
 62%|██████▏   | 285/459 [22:33<11:58,  4.13s/it][A
 62%|██████▏   | 286/459 [22:38<11:55,  4.14s/it][A
 63%|██████▎   | 287/459 [22:42<11:50,  4.13s/it][A
 63%|██████▎   | 288/459 [22:46<11:49,  4.15s/it][A
 63%|██████▎   | 289/459 [22:50<11:44,  4.14s/it][A
 63%|██████▎   | 290/459 [22:54<11:34,  4.11s/it][A
 63%|██████▎   | 291/459 [22:58<11:25,  4.08s/it][A
 64%|██████▎   | 292/459 [23:02<11:15,  4.05s/it][A
 64%|██████▍   | 293/459 [23:06<11:14,  4.07s/it][A
 64%|██████▍   | 294/459 [23:11<11:42,  4.26s/it][A
 64%|██████▍   | 295/459 [23:16<12:14,  4.48s/it][A
 64%|██████▍   | 296/459 [23:21<12:34,  4.63s/it][A
 65%|██████▍   | 297/459 [23:26<12:44,  4.72s/it][A
 65%|██████▍   | 298/459 [23:31<12:49,  4.78s/it][A
 65%|██████▌   | 299/459 [23:36<12:50,  4.81s/it][A
 65%|██████▌   | 300/459 [23:41<12:50,  4.85s/it][A
 66%|██████▌   | 301/459 [23:46<12:51,  4.88s/it][A
 66%|██████▌   | 302/459 [23:51<12:52,  4.92s/it][A
 66%|██████▌   | 303/459 [23:56<12:50,  4.94s/it][A
 66%|██████▌   | 304/459 [24:00<12:47,  4.95s/it][A
 66%|██████▋   | 305/459 [24:05<12:40,  4.94s/it][A
 67%|██████▋   | 306/459 [24:10<12:32,  4.92s/it][A
 67%|██████▋   | 307/459 [24:15<12:27,  4.92s/it][A
 67%|██████▋   | 308/459 [24:20<12:26,  4.94s/it][A
 67%|██████▋   | 309/459 [24:25<12:18,  4.92s/it][A
 68%|██████▊   | 310/459 [24:30<12:06,  4.88s/it][A
 68%|██████▊   | 311/459 [24:35<12:05,  4.90s/it][A
 68%|██████▊   | 312/459 [24:40<12:03,  4.92s/it][A
 68%|██████▊   | 313/459 [24:45<12:09,  5.00s/it][A
 68%|██████▊   | 314/459 [24:50<12:09,  5.03s/it][A
 69%|██████▊   | 315/459 [24:55<12:14,  5.10s/it][A
 69%|██████▉   | 316/459 [25:01<12:13,  5.13s/it][A
 69%|██████▉   | 317/459 [25:06<12:09,  5.14s/it][A
 69%|██████▉   | 318/459 [25:11<12:07,  5.16s/it][A
 69%|██████▉   | 319/459 [25:16<12:11,  5.22s/it][A
 70%|██████▉   | 320/459 [25:22<12:10,  5.25s/it][A
 70%|██████▉   | 321/459 [25:27<12:02,  5.23s/it][A
 70%|███████   | 322/459 [25:32<11:52,  5.20s/it][A
 70%|███████   | 323/459 [25:37<11:46,  5.20s/it][A
 71%|███████   | 324/459 [25:42<11:42,  5.20s/it][A
 71%|███████   | 325/459 [25:48<11:38,  5.22s/it][A
 71%|███████   | 326/459 [25:53<11:35,  5.23s/it][A
 71%|███████   | 327/459 [25:58<11:31,  5.24s/it][A
 71%|███████▏  | 328/459 [26:04<11:39,  5.34s/it][A
 72%|███████▏  | 329/459 [26:09<11:53,  5.49s/it][A
 72%|███████▏  | 330/459 [26:15<11:56,  5.55s/it][A
 72%|███████▏  | 331/459 [26:21<11:49,  5.54s/it][A
 72%|███████▏  | 332/459 [26:26<11:35,  5.48s/it][A
 73%|███████▎  | 333/459 [26:31<11:27,  5.46s/it][A
 73%|███████▎  | 334/459 [26:37<11:22,  5.46s/it][A
 73%|███████▎  | 335/459 [26:42<11:11,  5.41s/it][A
 73%|███████▎  | 336/459 [26:47<10:59,  5.37s/it][A
 73%|███████▎  | 337/459 [26:53<10:48,  5.32s/it][A
 74%|███████▎  | 338/459 [26:58<10:37,  5.27s/it][A
 74%|███████▍  | 339/459 [27:03<10:28,  5.24s/it][A
 74%|███████▍  | 340/459 [27:09<10:38,  5.37s/it][A
 74%|███████▍  | 341/459 [27:14<10:30,  5.34s/it][A
 75%|███████▍  | 342/459 [27:19<10:15,  5.26s/it][A
 75%|███████▍  | 343/459 [27:24<10:12,  5.28s/it][A
 75%|███████▍  | 344/459 [27:30<10:05,  5.26s/it][A
 75%|███████▌  | 345/459 [27:35<09:59,  5.26s/it][A
 75%|███████▌  | 346/459 [27:40<09:57,  5.28s/it][A
 76%|███████▌  | 347/459 [27:45<09:49,  5.26s/it][A
 76%|███████▌  | 348/459 [27:50<09:35,  5.18s/it][A
 76%|███████▌  | 349/459 [27:55<09:21,  5.11s/it][A
 76%|███████▋  | 350/459 [28:00<09:02,  4.98s/it][A
 76%|███████▋  | 351/459 [28:04<08:30,  4.72s/it][A
 77%|███████▋  | 352/459 [28:08<08:01,  4.50s/it][A
 77%|███████▋  | 353/459 [28:12<07:41,  4.35s/it][A
 77%|███████▋  | 354/459 [28:16<07:25,  4.24s/it][A
 77%|███████▋  | 355/459 [28:20<07:13,  4.17s/it][A
 78%|███████▊  | 356/459 [28:24<07:03,  4.11s/it][A
 78%|███████▊  | 357/459 [28:28<06:57,  4.10s/it][A
 78%|███████▊  | 358/459 [28:32<06:50,  4.07s/it][A
 78%|███████▊  | 359/459 [28:36<06:44,  4.04s/it][A
 78%|███████▊  | 360/459 [28:40<06:38,  4.02s/it][A
 79%|███████▊  | 361/459 [28:44<06:31,  3.99s/it][A
 79%|███████▉  | 362/459 [28:48<06:26,  3.98s/it][A
 79%|███████▉  | 363/459 [28:52<06:20,  3.97s/it][A
 79%|███████▉  | 364/459 [28:56<06:16,  3.97s/it][A
 80%|███████▉  | 365/459 [29:00<06:15,  4.00s/it][A
 80%|███████▉  | 366/459 [29:04<06:11,  3.99s/it][A
 80%|███████▉  | 367/459 [29:08<06:07,  3.99s/it][A
 80%|████████  | 368/459 [29:12<06:05,  4.02s/it][A
 80%|████████  | 369/459 [29:16<06:00,  4.01s/it][A
 81%|████████  | 370/459 [29:20<05:57,  4.01s/it][A
 81%|████████  | 371/459 [29:24<05:53,  4.02s/it][A
 81%|████████  | 372/459 [29:28<05:47,  3.99s/it][A
 81%|████████▏ | 373/459 [29:32<05:43,  4.00s/it][A
 81%|████████▏ | 374/459 [29:36<05:39,  3.99s/it][A
 82%|████████▏ | 375/459 [29:40<05:34,  3.99s/it][A
 82%|████████▏ | 376/459 [29:44<05:37,  4.06s/it][A
 82%|████████▏ | 377/459 [29:48<05:33,  4.06s/it][A
 82%|████████▏ | 378/459 [29:52<05:30,  4.08s/it][A
 83%|████████▎ | 379/459 [29:57<05:29,  4.12s/it][A
 83%|████████▎ | 380/459 [30:01<05:25,  4.12s/it][A
 83%|████████▎ | 381/459 [30:05<05:21,  4.13s/it][A
 83%|████████▎ | 382/459 [30:09<05:20,  4.16s/it][A
 83%|████████▎ | 383/459 [30:13<05:16,  4.17s/it][A
 84%|████████▎ | 384/459 [30:18<05:15,  4.21s/it][A
 84%|████████▍ | 385/459 [30:22<05:13,  4.23s/it][A
 84%|████████▍ | 386/459 [30:26<05:07,  4.21s/it][A
 84%|████████▍ | 387/459 [30:30<05:05,  4.24s/it][A
 85%|████████▍ | 388/459 [30:35<05:00,  4.24s/it][A
 85%|████████▍ | 389/459 [30:39<04:57,  4.25s/it][A
 85%|████████▍ | 390/459 [30:43<04:54,  4.27s/it][A
 85%|████████▌ | 391/459 [30:47<04:49,  4.26s/it][A
 85%|████████▌ | 392/459 [30:52<04:46,  4.27s/it][A
 86%|████████▌ | 393/459 [30:56<04:43,  4.29s/it][A
 86%|████████▌ | 394/459 [31:00<04:37,  4.27s/it][A
 86%|████████▌ | 395/459 [31:05<04:34,  4.29s/it][A
 86%|████████▋ | 396/459 [31:09<04:29,  4.29s/it][A
 86%|████████▋ | 397/459 [31:13<04:25,  4.28s/it][A
 87%|████████▋ | 398/459 [31:18<04:29,  4.43s/it][A
 87%|████████▋ | 399/459 [31:23<04:34,  4.57s/it][A
 87%|████████▋ | 400/459 [31:28<04:38,  4.72s/it][A
 87%|████████▋ | 401/459 [31:33<04:45,  4.92s/it][A
 88%|████████▊ | 402/459 [31:39<04:54,  5.16s/it][A
 88%|████████▊ | 403/459 [31:44<04:55,  5.28s/it][A
 88%|████████▊ | 404/459 [31:50<04:52,  5.32s/it][A
 88%|████████▊ | 405/459 [31:55<04:51,  5.39s/it][A
 88%|████████▊ | 406/459 [32:01<04:49,  5.45s/it][A
 89%|████████▊ | 407/459 [32:07<04:43,  5.46s/it][A
 89%|████████▉ | 408/459 [32:13<04:51,  5.72s/it][A
 89%|████████▉ | 409/459 [32:19<04:57,  5.94s/it][A
 89%|████████▉ | 410/459 [32:26<04:55,  6.04s/it][A
 90%|████████▉ | 411/459 [32:32<04:50,  6.04s/it][A
 90%|████████▉ | 412/459 [32:38<04:43,  6.03s/it][A
 90%|████████▉ | 413/459 [32:44<04:36,  6.02s/it][A
 90%|█████████ | 414/459 [32:50<04:32,  6.06s/it][A
 90%|█████████ | 415/459 [32:56<04:27,  6.08s/it][A
 91%|█████████ | 416/459 [33:02<04:23,  6.12s/it][A
 91%|█████████ | 417/459 [33:08<04:16,  6.10s/it][A
 91%|█████████ | 418/459 [33:14<04:09,  6.09s/it][A
 91%|█████████▏| 419/459 [33:20<04:03,  6.08s/it][A
 92%|█████████▏| 420/459 [33:26<03:55,  6.05s/it][A
 92%|█████████▏| 421/459 [33:32<03:52,  6.11s/it][A
 92%|█████████▏| 422/459 [33:39<03:45,  6.09s/it][A
 92%|█████████▏| 423/459 [33:45<03:40,  6.13s/it][A
 92%|█████████▏| 424/459 [33:51<03:33,  6.11s/it][A
 93%|█████████▎| 425/459 [33:57<03:26,  6.07s/it][A
 93%|█████████▎| 426/459 [34:02<03:13,  5.87s/it][A
 93%|█████████▎| 427/459 [34:08<03:03,  5.73s/it][A
 93%|█████████▎| 428/459 [34:13<02:55,  5.67s/it][A
 93%|█████████▎| 429/459 [34:18<02:47,  5.57s/it][A
 94%|█████████▎| 430/459 [34:24<02:39,  5.51s/it][A
 94%|█████████▍| 431/459 [34:29<02:33,  5.47s/it][A
 94%|█████████▍| 432/459 [34:35<02:26,  5.43s/it][A
 94%|█████████▍| 433/459 [34:40<02:20,  5.39s/it][A
 95%|█████████▍| 434/459 [34:45<02:14,  5.39s/it][A
 95%|█████████▍| 435/459 [34:51<02:09,  5.39s/it][A
 95%|█████████▍| 436/459 [34:56<02:02,  5.32s/it][A
 95%|█████████▌| 437/459 [35:01<01:53,  5.16s/it][A
 95%|█████████▌| 438/459 [35:05<01:44,  4.98s/it][A
 96%|█████████▌| 439/459 [35:10<01:37,  4.87s/it][A
 96%|█████████▌| 440/459 [35:14<01:31,  4.80s/it][A
 96%|█████████▌| 441/459 [35:19<01:24,  4.70s/it][A
 96%|█████████▋| 442/459 [35:23<01:19,  4.66s/it][A
 97%|█████████▋| 443/459 [35:28<01:14,  4.64s/it][A
 97%|█████████▋| 444/459 [35:33<01:11,  4.74s/it][A
 97%|█████████▋| 445/459 [35:38<01:06,  4.74s/it][A
 97%|█████████▋| 446/459 [35:43<01:01,  4.76s/it][A
 97%|█████████▋| 447/459 [35:47<00:55,  4.61s/it][A
 98%|█████████▊| 448/459 [35:51<00:49,  4.49s/it][A
 98%|█████████▊| 449/459 [35:55<00:43,  4.39s/it][A
 98%|█████████▊| 450/459 [35:59<00:39,  4.34s/it][A
 98%|█████████▊| 451/459 [36:04<00:34,  4.29s/it][A
 98%|█████████▊| 452/459 [36:08<00:29,  4.26s/it][A
 99%|█████████▊| 453/459 [36:12<00:25,  4.23s/it][A
 99%|█████████▉| 454/459 [36:16<00:21,  4.23s/it][A
 99%|█████████▉| 455/459 [36:20<00:16,  4.21s/it][A
 99%|█████████▉| 456/459 [36:24<00:12,  4.19s/it][A
100%|█████████▉| 457/459 [36:29<00:08,  4.19s/it][A
100%|█████████▉| 458/459 [36:33<00:04,  4.19s/it][A
100%|██████████| 459/459 [36:37<00:00,  4.20s/it][A                                                         
                                                 [A{'eval_loss': 0.1256466507911682, 'eval_runtime': 2202.9704, 'eval_samples_per_second': 1.25, 'eval_steps_per_second': 0.208, 'epoch': 0.97}
 48%|████▊     | 500/1034 [22:00:12<21:11:52, 142.91s/it]
100%|██████████| 459/459 [36:37<00:00,  4.20s/it][A
                                                 [A 48%|████▊     | 501/1034 [22:02:27<118:39:57, 801.50s/it] 49%|████▊     | 502/1034 [22:05:21<90:36:51, 613.18s/it]  49%|████▊     | 503/1034 [22:07:48<69:50:26, 473.50s/it] 49%|████▊     | 504/1034 [22:10:02<54:41:35, 371.50s/it] 49%|████▉     | 505/1034 [22:12:37<45:03:14, 306.61s/it] 49%|████▉     | 506/1034 [22:15:11<38:16:01, 260.91s/it] 49%|████▉     | 507/1034 [22:17:25<32:37:00, 222.81s/it] 49%|████▉     | 508/1034 [22:19:42<28:47:22, 197.04s/it] 49%|████▉     | 509/1034 [22:22:16<26:51:14, 184.14s/it] 49%|████▉     | 510/1034 [22:24:35<24:50:43, 170.69s/it]                                                         {'loss': 0.1373, 'grad_norm': 4.728791903210861, 'learning_rate': 1.2012985200886602e-05, 'epoch': 0.99}
 49%|████▉     | 510/1034 [22:24:35<24:50:43, 170.69s/it] 49%|████▉     | 511/1034 [22:26:42<22:53:40, 157.59s/it] 50%|████▉     | 512/1034 [22:29:10<22:24:07, 154.50s/it] 50%|████▉     | 513/1034 [22:31:38<22:05:16, 152.62s/it] 50%|████▉     | 514/1034 [22:33:46<20:58:31, 145.22s/it] 50%|████▉     | 515/1034 [22:36:07<20:46:03, 144.05s/it] 50%|████▉     | 516/1034 [22:38:46<21:21:28, 148.43s/it] 50%|█████     | 517/1034 [22:39:44<17:24:29, 121.22s/it] 50%|█████     | 518/1034 [22:42:00<18:01:10, 125.72s/it] 50%|█████     | 519/1034 [22:44:17<18:27:33, 129.04s/it] 50%|█████     | 520/1034 [22:47:00<19:54:35, 139.45s/it]                                                         {'loss': 0.1434, 'grad_norm': 1.6777799972503769, 'learning_rate': 1.1681008942421484e-05, 'epoch': 1.01}
 50%|█████     | 520/1034 [22:47:00<19:54:35, 139.45s/it] 50%|█████     | 521/1034 [22:49:26<20:07:07, 141.18s/it] 50%|█████     | 522/1034 [22:51:46<20:01:45, 140.83s/it] 51%|█████     | 523/1034 [22:54:21<20:36:19, 145.16s/it] 51%|█████     | 524/1034 [22:56:52<20:49:11, 146.96s/it] 51%|█████     | 525/1034 [22:59:18<20:45:05, 146.77s/it] 51%|█████     | 526/1034 [23:01:48<20:50:39, 147.72s/it] 51%|█████     | 527/1034 [23:04:08<20:28:22, 145.37s/it] 51%|█████     | 528/1034 [23:06:38<20:38:08, 146.81s/it] 51%|█████     | 529/1034 [23:09:06<20:37:01, 146.97s/it] 51%|█████▏    | 530/1034 [23:11:27<20:19:18, 145.16s/it]                                                         {'loss': 0.1366, 'grad_norm': 2.0930659240961402, 'learning_rate': 1.1347114622258613e-05, 'epoch': 1.03}
 51%|█████▏    | 530/1034 [23:11:27<20:19:18, 145.16s/it] 51%|█████▏    | 531/1034 [23:14:09<20:59:41, 150.26s/it] 51%|█████▏    | 532/1034 [23:17:40<23:29:31, 168.47s/it] 52%|█████▏    | 533/1034 [23:20:18<23:00:19, 165.31s/it] 52%|█████▏    | 534/1034 [23:22:30<21:34:57, 155.40s/it] 52%|█████▏    | 535/1034 [23:24:34<20:14:29, 146.03s/it] 52%|█████▏    | 536/1034 [23:26:32<19:02:02, 137.60s/it] 52%|█████▏    | 537/1034 [23:28:31<18:12:59, 131.95s/it] 52%|█████▏    | 538/1034 [23:30:29<17:36:29, 127.80s/it] 52%|█████▏    | 539/1034 [23:32:41<17:45:17, 129.13s/it] 52%|█████▏    | 540/1034 [23:39:15<28:36:55, 208.53s/it]                                                         {'loss': 0.1306, 'grad_norm': 1.594961522057658, 'learning_rate': 1.1011683219874324e-05, 'epoch': 1.04}
 52%|█████▏    | 540/1034 [23:39:15<28:36:55, 208.53s/it] 52%|█████▏    | 541/1034 [23:41:16<24:58:43, 182.40s/it] 52%|█████▏    | 542/1034 [23:43:16<22:22:27, 163.72s/it] 53%|█████▎    | 543/1034 [23:45:15<20:29:29, 150.24s/it] 53%|█████▎    | 544/1034 [23:47:11<19:01:39, 139.79s/it] 53%|█████▎    | 545/1034 [23:49:09<18:06:34, 133.32s/it] 53%|█████▎    | 546/1034 [23:51:11<17:36:45, 129.93s/it] 53%|█████▎    | 547/1034 [23:53:15<17:20:21, 128.18s/it] 53%|█████▎    | 548/1034 [23:55:16<17:01:11, 126.07s/it] 53%|█████▎    | 549/1034 [23:57:15<16:41:17, 123.87s/it] 53%|█████▎    | 550/1034 [23:59:18<16:36:53, 123.58s/it]                                                         {'loss': 0.1318, 'grad_norm': 2.3632460675896776, 'learning_rate': 1.0675097468583653e-05, 'epoch': 1.06}
 53%|█████▎    | 550/1034 [23:59:18<16:36:53, 123.58s/it] 53%|█████▎    | 551/1034 [24:01:22<16:36:43, 123.82s/it] 53%|█████▎    | 552/1034 [24:03:30<16:44:59, 125.10s/it] 53%|█████▎    | 553/1034 [24:05:38<16:48:27, 125.80s/it] 54%|█████▎    | 554/1034 [24:08:37<18:54:09, 141.77s/it] 54%|█████▎    | 555/1034 [24:11:38<20:26:09, 153.59s/it] 54%|█████▍    | 556/1034 [24:14:05<20:08:58, 151.75s/it] 54%|█████▍    | 557/1034 [24:16:58<20:55:58, 157.98s/it] 54%|█████▍    | 558/1034 [24:20:03<21:57:38, 166.09s/it] 54%|█████▍    | 559/1034 [24:22:42<21:38:37, 164.04s/it] 54%|█████▍    | 560/1034 [24:25:11<20:59:56, 159.49s/it]                                                         {'loss': 0.1118, 'grad_norm': 2.8472590106598004, 'learning_rate': 1.0337741418834683e-05, 'epoch': 1.08}
 54%|█████▍    | 560/1034 [24:25:11<20:59:56, 159.49s/it] 54%|█████▍    | 561/1034 [24:28:13<21:49:20, 166.09s/it] 54%|█████▍    | 562/1034 [24:31:00<21:50:44, 166.62s/it] 54%|█████▍    | 563/1034 [24:33:18<20:39:46, 157.93s/it] 55%|█████▍    | 564/1034 [24:35:53<20:29:16, 156.93s/it] 55%|█████▍    | 565/1034 [24:39:27<22:40:11, 174.01s/it] 55%|█████▍    | 566/1034 [24:41:54<21:34:15, 165.93s/it] 55%|█████▍    | 567/1034 [24:44:08<20:18:33, 156.56s/it] 55%|█████▍    | 568/1034 [24:46:33<19:48:15, 153.00s/it] 55%|█████▌    | 569/1034 [24:48:52<19:13:23, 148.82s/it] 55%|█████▌    | 570/1034 [24:51:44<20:03:39, 155.65s/it]                                                         {'loss': 0.1185, 'grad_norm': 1.7414171162920844, 'learning_rate': 1e-05, 'epoch': 1.1}
 55%|█████▌    | 570/1034 [24:51:44<20:03:39, 155.65s/it] 55%|█████▌    | 571/1034 [24:54:03<19:22:22, 150.63s/it] 55%|█████▌    | 572/1034 [24:56:24<18:59:35, 148.00s/it] 55%|█████▌    | 573/1034 [24:58:51<18:53:56, 147.59s/it] 56%|█████▌    | 574/1034 [25:01:13<18:39:14, 145.99s/it] 56%|█████▌    | 575/1034 [25:04:11<19:50:03, 155.56s/it] 56%|█████▌    | 576/1034 [25:06:55<20:06:44, 158.09s/it] 56%|█████▌    | 577/1034 [25:09:08<19:07:31, 150.66s/it] 56%|█████▌    | 578/1034 [25:11:16<18:11:24, 143.61s/it] 56%|█████▌    | 579/1034 [25:13:34<17:56:55, 142.01s/it] 56%|█████▌    | 580/1034 [25:16:12<18:30:51, 146.81s/it]                                                         {'loss': 0.1178, 'grad_norm': 1.4689038745891607, 'learning_rate': 9.66225858116532e-06, 'epoch': 1.12}
 56%|█████▌    | 580/1034 [25:16:12<18:30:51, 146.81s/it] 56%|█████▌    | 581/1034 [25:18:58<19:12:18, 152.62s/it] 56%|█████▋    | 582/1034 [25:21:49<19:51:28, 158.16s/it] 56%|█████▋    | 583/1034 [25:24:31<19:56:43, 159.21s/it] 56%|█████▋    | 584/1034 [25:27:23<20:22:18, 162.97s/it] 57%|█████▋    | 585/1034 [25:30:10<20:29:22, 164.28s/it] 57%|█████▋    | 586/1034 [25:32:38<19:50:45, 159.48s/it] 57%|█████▋    | 587/1034 [25:35:31<20:17:19, 163.40s/it] 57%|█████▋    | 588/1034 [25:38:19<20:25:16, 164.83s/it] 57%|█████▋    | 589/1034 [25:40:52<19:57:11, 161.42s/it] 57%|█████▋    | 590/1034 [25:43:31<19:48:53, 160.66s/it]                                                         {'loss': 0.102, 'grad_norm': 1.9431058801616221, 'learning_rate': 9.324902531416348e-06, 'epoch': 1.14}
 57%|█████▋    | 590/1034 [25:43:31<19:48:53, 160.66s/it] 57%|█████▋    | 591/1034 [25:46:13<19:47:41, 160.86s/it] 57%|█████▋    | 592/1034 [25:48:36<19:06:57, 155.70s/it] 57%|█████▋    | 593/1034 [25:50:59<18:35:36, 151.78s/it] 57%|█████▋    | 594/1034 [25:53:26<18:22:32, 150.35s/it] 58%|█████▊    | 595/1034 [25:55:53<18:13:31, 149.46s/it] 58%|█████▊    | 596/1034 [25:58:00<17:21:51, 142.72s/it] 58%|█████▊    | 597/1034 [26:00:16<17:04:58, 140.73s/it] 58%|█████▊    | 598/1034 [26:02:33<16:53:19, 139.45s/it] 58%|█████▊    | 599/1034 [26:06:23<20:08:28, 166.69s/it] 58%|█████▊    | 600/1034 [26:08:57<19:38:57, 162.99s/it]                                                         {'loss': 0.1118, 'grad_norm': 2.339793995210535, 'learning_rate': 8.98831678012568e-06, 'epoch': 1.16}
 58%|█████▊    | 600/1034 [26:08:57<19:38:57, 162.99s/it][INFO|trainer.py:4309] 2025-12-19 03:44:05,350 >> Saving model checkpoint to saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600
[INFO|configuration_utils.py:763] 2025-12-19 03:44:05,372 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-19 03:44:05,373 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2421] 2025-12-19 03:44:05,461 >> chat template saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 03:44:05,463 >> tokenizer config file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 03:44:05,463 >> Special tokens file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/special_tokens_map.json
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2025-12-19 03:44:05,692] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-12-19 03:44:07,228] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-19 03:44:07,228] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-19 03:44:07,256] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/global_step600/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-19 03:44:07,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-19 03:44:07,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-19 03:44:07,412] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-19 03:44:07,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
 58%|█████▊    | 601/1034 [26:11:45<19:45:39, 164.29s/it] 58%|█████▊    | 602/1034 [26:13:58<18:35:58, 155.00s/it] 58%|█████▊    | 603/1034 [26:16:02<17:26:35, 145.70s/it] 58%|█████▊    | 604/1034 [26:18:34<17:36:34, 147.43s/it] 59%|█████▊    | 605/1034 [26:21:17<18:07:55, 152.16s/it] 59%|█████▊    | 606/1034 [26:23:42<17:51:19, 150.18s/it] 59%|█████▊    | 607/1034 [26:25:45<16:49:21, 141.83s/it] 59%|█████▉    | 608/1034 [26:27:57<16:25:45, 138.84s/it] 59%|█████▉    | 609/1034 [26:30:20<16:34:16, 140.37s/it] 59%|█████▉    | 610/1034 [26:33:19<17:52:07, 151.72s/it]                                                         {'loss': 0.1292, 'grad_norm': 1.3393527535163317, 'learning_rate': 8.652885377741394e-06, 'epoch': 1.18}
 59%|█████▉    | 610/1034 [26:33:19<17:52:07, 151.72s/it] 59%|█████▉    | 611/1034 [26:36:06<18:22:45, 156.42s/it] 59%|█████▉    | 612/1034 [26:38:23<17:38:47, 150.54s/it] 59%|█████▉    | 613/1034 [26:40:59<17:48:25, 152.27s/it] 59%|█████▉    | 614/1034 [26:43:23<17:28:22, 149.77s/it] 59%|█████▉    | 615/1034 [26:45:48<17:15:47, 148.32s/it] 60%|█████▉    | 616/1034 [26:48:33<17:48:56, 153.44s/it] 60%|█████▉    | 617/1034 [26:50:48<17:07:15, 147.81s/it] 60%|█████▉    | 618/1034 [26:52:59<16:29:23, 142.70s/it] 60%|█████▉    | 619/1034 [26:55:04<15:49:37, 137.30s/it] 60%|█████▉    | 620/1034 [26:57:21<15:47:32, 137.33s/it]                                                         {'loss': 0.1194, 'grad_norm': 1.1292394541972264, 'learning_rate': 8.31899105757852e-06, 'epoch': 1.2}
 60%|█████▉    | 620/1034 [26:57:21<15:47:32, 137.33s/it] 60%|██████    | 621/1034 [26:59:43<15:54:52, 138.72s/it] 60%|██████    | 622/1034 [27:02:12<16:13:02, 141.70s/it] 60%|██████    | 623/1034 [27:04:36<16:15:40, 142.43s/it] 60%|██████    | 624/1034 [27:06:50<15:56:39, 140.00s/it] 60%|██████    | 625/1034 [27:08:50<15:14:07, 134.10s/it] 61%|██████    | 626/1034 [27:11:11<15:25:17, 136.07s/it] 61%|██████    | 627/1034 [27:13:29<15:27:27, 136.73s/it] 61%|██████    | 628/1034 [27:15:36<15:04:52, 133.73s/it] 61%|██████    | 629/1034 [27:17:54<15:11:46, 135.08s/it] 61%|██████    | 630/1034 [27:20:19<15:28:39, 137.92s/it]                                                         {'loss': 0.1269, 'grad_norm': 4.171111703344634, 'learning_rate': 7.987014799113398e-06, 'epoch': 1.22}
 61%|██████    | 630/1034 [27:20:19<15:28:39, 137.92s/it] 61%|██████    | 631/1034 [27:22:40<15:33:19, 138.96s/it] 61%|██████    | 632/1034 [27:25:00<15:32:41, 139.21s/it] 61%|██████    | 633/1034 [27:27:06<15:03:23, 135.17s/it] 61%|██████▏   | 634/1034 [27:29:23<15:05:12, 135.78s/it] 61%|██████▏   | 635/1034 [27:31:40<15:05:28, 136.16s/it] 62%|██████▏   | 636/1034 [27:34:11<15:33:24, 140.72s/it] 62%|██████▏   | 637/1034 [27:36:45<15:56:09, 144.51s/it] 62%|██████▏   | 638/1034 [27:39:10<15:54:24, 144.61s/it] 62%|██████▏   | 639/1034 [27:41:20<15:23:20, 140.25s/it] 62%|██████▏   | 640/1034 [27:43:39<15:19:10, 139.98s/it]                                                         {'loss': 0.1113, 'grad_norm': 2.4332660922165625, 'learning_rate': 7.657335393279179e-06, 'epoch': 1.24}
 62%|██████▏   | 640/1034 [27:43:39<15:19:10, 139.98s/it] 62%|██████▏   | 641/1034 [27:46:01<15:20:15, 140.50s/it] 62%|██████▏   | 642/1034 [27:48:27<15:29:00, 142.20s/it] 62%|██████▏   | 643/1034 [27:50:52<15:32:36, 143.11s/it] 62%|██████▏   | 644/1034 [27:52:48<14:37:42, 135.03s/it] 62%|██████▏   | 645/1034 [27:55:05<14:38:13, 135.46s/it] 62%|██████▏   | 646/1034 [27:57:35<15:04:32, 139.88s/it] 63%|██████▎   | 647/1034 [28:00:00<15:11:23, 141.30s/it] 63%|██████▎   | 648/1034 [28:02:31<15:28:49, 144.38s/it] 63%|██████▎   | 649/1034 [28:04:37<14:51:12, 138.89s/it] 63%|██████▎   | 650/1034 [28:06:53<14:42:21, 137.87s/it]                                                         {'loss': 0.134, 'grad_norm': 1.4038854830088234, 'learning_rate': 7.330329010258483e-06, 'epoch': 1.26}
 63%|██████▎   | 650/1034 [28:06:53<14:42:21, 137.87s/it] 63%|██████▎   | 651/1034 [28:09:21<15:00:32, 141.08s/it] 63%|██████▎   | 652/1034 [28:11:42<14:57:35, 140.98s/it] 63%|██████▎   | 653/1034 [28:14:08<15:05:09, 142.55s/it] 63%|██████▎   | 654/1034 [28:16:28<14:57:38, 141.73s/it] 63%|██████▎   | 655/1034 [28:18:57<15:08:31, 143.83s/it] 63%|██████▎   | 656/1034 [28:21:41<15:44:14, 149.88s/it] 64%|██████▎   | 657/1034 [28:24:00<15:22:26, 146.81s/it] 64%|██████▎   | 658/1034 [28:26:16<14:58:28, 143.37s/it] 64%|██████▎   | 659/1034 [28:28:36<14:50:01, 142.40s/it] 64%|██████▍   | 660/1034 [28:30:44<14:20:03, 137.98s/it]                                                         {'loss': 0.1124, 'grad_norm': 2.885873754647848, 'learning_rate': 7.006368770266421e-06, 'epoch': 1.28}
 64%|██████▍   | 660/1034 [28:30:44<14:20:03, 137.98s/it] 64%|██████▍   | 661/1034 [28:33:13<14:39:52, 141.53s/it] 64%|██████▍   | 662/1034 [28:35:48<15:02:31, 145.57s/it] 64%|██████▍   | 663/1034 [28:38:11<14:54:32, 144.67s/it] 64%|██████▍   | 664/1034 [28:40:39<14:57:35, 145.55s/it] 64%|██████▍   | 665/1034 [28:42:53<14:34:50, 142.25s/it] 64%|██████▍   | 666/1034 [28:44:54<13:53:44, 135.94s/it] 65%|██████▍   | 667/1034 [28:47:15<14:00:54, 137.48s/it] 65%|██████▍   | 668/1034 [28:50:09<15:04:20, 148.25s/it] 65%|██████▍   | 669/1034 [28:52:37<15:02:17, 148.32s/it] 65%|██████▍   | 670/1034 [28:54:47<14:26:40, 142.86s/it]                                                         {'loss': 0.1078, 'grad_norm': 2.7484653694625516, 'learning_rate': 6.6858243178136425e-06, 'epoch': 1.3}
 65%|██████▍   | 670/1034 [28:54:47<14:26:40, 142.86s/it] 65%|██████▍   | 671/1034 [28:57:01<14:08:08, 140.19s/it] 65%|██████▍   | 672/1034 [28:59:20<14:02:14, 139.60s/it] 65%|██████▌   | 673/1034 [29:01:59<14:35:46, 145.56s/it] 65%|██████▌   | 674/1034 [29:04:12<14:10:47, 141.80s/it] 65%|██████▌   | 675/1034 [29:06:16<13:36:23, 136.44s/it] 65%|██████▌   | 676/1034 [29:08:32<13:33:16, 136.30s/it] 65%|██████▌   | 677/1034 [29:10:32<13:02:52, 131.58s/it] 66%|██████▌   | 678/1034 [29:12:48<13:07:17, 132.69s/it] 66%|██████▌   | 679/1034 [29:15:30<13:56:58, 141.46s/it] 66%|██████▌   | 680/1034 [29:17:48<13:49:25, 140.58s/it]                                                         {'loss': 0.1258, 'grad_norm': 2.354873093818075, 'learning_rate': 6.369061399935255e-06, 'epoch': 1.32}
 66%|██████▌   | 680/1034 [29:17:48<13:49:25, 140.58s/it] 66%|██████▌   | 681/1034 [29:19:59<13:30:38, 137.79s/it] 66%|██████▌   | 682/1034 [29:22:32<13:53:37, 142.09s/it] 66%|██████▌   | 683/1034 [29:25:13<14:24:22, 147.76s/it] 66%|██████▌   | 684/1034 [29:27:16<13:40:02, 140.58s/it] 66%|██████▌   | 685/1034 [29:29:32<13:29:26, 139.16s/it] 66%|██████▋   | 686/1034 [29:32:16<14:10:12, 146.59s/it] 66%|██████▋   | 687/1034 [29:34:48<14:16:19, 148.07s/it] 67%|██████▋   | 688/1034 [29:37:14<14:11:04, 147.58s/it] 67%|██████▋   | 689/1034 [29:39:31<13:50:52, 144.50s/it] 67%|██████▋   | 690/1034 [29:41:48<13:35:08, 142.18s/it]                                                         {'loss': 0.113, 'grad_norm': 3.113344733647057, 'learning_rate': 6.056441448866817e-06, 'epoch': 1.34}
 67%|██████▋   | 690/1034 [29:41:48<13:35:08, 142.18s/it] 67%|██████▋   | 691/1034 [29:44:03<13:20:01, 139.95s/it] 67%|██████▋   | 692/1034 [29:46:46<13:56:25, 146.74s/it] 67%|██████▋   | 693/1034 [29:49:18<14:03:03, 148.34s/it] 67%|██████▋   | 694/1034 [29:51:22<13:19:15, 141.05s/it] 67%|██████▋   | 695/1034 [29:53:50<13:28:28, 143.09s/it] 67%|██████▋   | 696/1034 [29:56:18<13:34:28, 144.58s/it] 67%|██████▋   | 697/1034 [29:58:36<13:21:55, 142.78s/it] 68%|██████▊   | 698/1034 [30:01:23<13:59:19, 149.88s/it] 68%|██████▊   | 699/1034 [30:03:53<13:57:42, 150.04s/it] 68%|██████▊   | 700/1034 [30:06:29<14:04:42, 151.74s/it]                                                         {'loss': 0.1175, 'grad_norm': 2.0661563450013274, 'learning_rate': 5.748321169643596e-06, 'epoch': 1.35}
 68%|██████▊   | 700/1034 [30:06:29<14:04:42, 151.74s/it] 68%|██████▊   | 701/1034 [30:09:00<14:01:39, 151.65s/it] 68%|██████▊   | 702/1034 [30:11:29<13:53:39, 150.66s/it] 68%|██████▊   | 703/1034 [30:13:39<13:17:12, 144.51s/it] 68%|██████▊   | 704/1034 [30:16:01<13:10:43, 143.77s/it] 68%|██████▊   | 705/1034 [30:18:36<13:26:46, 147.13s/it] 68%|██████▊   | 706/1034 [30:21:07<13:31:45, 148.49s/it] 68%|██████▊   | 707/1034 [30:23:27<13:14:34, 145.79s/it] 68%|██████▊   | 708/1034 [30:25:37<12:46:44, 141.12s/it] 69%|██████▊   | 709/1034 [30:28:01<12:48:54, 141.95s/it] 69%|██████▊   | 710/1034 [30:30:30<12:58:04, 144.09s/it]                                                         {'loss': 0.1037, 'grad_norm': 1.9236620777018225, 'learning_rate': 5.44505213309366e-06, 'epoch': 1.37}
 69%|██████▊   | 710/1034 [30:30:30<12:58:04, 144.09s/it] 69%|██████▉   | 711/1034 [30:33:00<13:04:45, 145.78s/it] 69%|██████▉   | 712/1034 [30:35:35<13:17:50, 148.67s/it] 69%|██████▉   | 713/1034 [30:37:42<12:40:36, 142.17s/it] 69%|██████▉   | 714/1034 [30:39:51<12:17:33, 138.29s/it] 69%|██████▉   | 715/1034 [30:42:17<12:26:38, 140.43s/it] 69%|██████▉   | 716/1034 [30:44:41<12:29:35, 141.43s/it] 69%|██████▉   | 717/1034 [30:46:46<12:02:27, 136.74s/it] 69%|██████▉   | 718/1034 [30:49:11<12:12:08, 139.01s/it] 70%|██████▉   | 719/1034 [30:51:48<12:38:56, 144.56s/it] 70%|██████▉   | 720/1034 [30:54:00<12:16:08, 140.66s/it]                                                         {'loss': 0.1142, 'grad_norm': 3.3216460575857023, 'learning_rate': 5.146980374689192e-06, 'epoch': 1.39}
 70%|██████▉   | 720/1034 [30:54:00<12:16:08, 140.66s/it] 70%|██████▉   | 721/1034 [30:56:07<11:52:42, 136.62s/it] 70%|██████▉   | 722/1034 [30:58:34<12:06:35, 139.73s/it] 70%|██████▉   | 723/1034 [31:01:07<12:25:16, 143.78s/it] 70%|███████   | 724/1034 [31:03:52<12:55:00, 150.00s/it] 70%|███████   | 725/1034 [31:06:44<13:26:53, 156.68s/it] 70%|███████   | 726/1034 [31:09:18<13:20:06, 155.86s/it] 70%|███████   | 727/1034 [31:12:04<13:32:29, 158.79s/it] 70%|███████   | 728/1034 [31:14:22<12:58:12, 152.59s/it] 71%|███████   | 729/1034 [31:16:49<12:47:00, 150.89s/it] 71%|███████   | 730/1034 [31:19:25<12:52:21, 152.44s/it]                                                         {'loss': 0.131, 'grad_norm': 1.6396109727112569, 'learning_rate': 4.854445999713715e-06, 'epoch': 1.41}
 71%|███████   | 730/1034 [31:19:25<12:52:21, 152.44s/it] 71%|███████   | 731/1034 [31:21:37<12:18:46, 146.29s/it] 71%|███████   | 732/1034 [31:23:33<11:30:58, 137.28s/it] 71%|███████   | 733/1034 [31:25:51<11:29:45, 137.49s/it] 71%|███████   | 734/1034 [31:28:23<11:50:02, 142.01s/it] 71%|███████   | 735/1034 [31:30:50<11:54:54, 143.46s/it] 71%|███████   | 736/1034 [31:33:10<11:46:52, 142.32s/it] 71%|███████▏  | 737/1034 [31:35:22<11:28:47, 139.15s/it] 71%|███████▏  | 738/1034 [31:37:30<11:11:02, 136.02s/it] 71%|███████▏  | 739/1034 [31:40:04<11:34:34, 141.27s/it] 72%|███████▏  | 740/1034 [31:42:41<11:55:26, 146.01s/it]                                                         {'loss': 0.1164, 'grad_norm': 2.6607705047035157, 'learning_rate': 4.567782795195816e-06, 'epoch': 1.43}
 72%|███████▏  | 740/1034 [31:42:41<11:55:26, 146.01s/it] 72%|███████▏  | 741/1034 [31:44:55<11:35:22, 142.40s/it] 72%|███████▏  | 742/1034 [31:47:11<11:24:24, 140.63s/it] 72%|███████▏  | 743/1034 [31:49:23<11:08:23, 137.81s/it] 72%|███████▏  | 744/1034 [31:51:25<10:44:08, 133.27s/it] 72%|███████▏  | 745/1034 [31:53:42<10:46:56, 134.31s/it] 72%|███████▏  | 746/1034 [31:56:39<11:45:49, 147.05s/it] 72%|███████▏  | 747/1034 [31:58:56<11:29:26, 144.13s/it] 72%|███████▏  | 748/1034 [32:01:04<11:03:47, 139.26s/it] 72%|███████▏  | 749/1034 [32:03:17<10:52:41, 137.41s/it] 73%|███████▎  | 750/1034 [32:05:36<10:52:53, 137.93s/it]                                                         {'loss': 0.139, 'grad_norm': 2.2417120899865948, 'learning_rate': 4.287317849052075e-06, 'epoch': 1.45}
 73%|███████▎  | 750/1034 [32:05:36<10:52:53, 137.93s/it] 73%|███████▎  | 751/1034 [32:08:11<11:14:41, 143.05s/it] 73%|███████▎  | 752/1034 [32:11:14<12:08:51, 155.08s/it] 73%|███████▎  | 753/1034 [32:13:53<12:10:55, 156.07s/it] 73%|███████▎  | 754/1034 [32:16:06<11:36:13, 149.19s/it] 73%|███████▎  | 755/1034 [32:18:14<11:04:51, 142.98s/it] 73%|███████▎  | 756/1034 [32:20:18<10:35:43, 137.21s/it] 73%|███████▎  | 757/1034 [32:22:28<10:22:37, 134.86s/it] 73%|███████▎  | 758/1034 [32:24:36<10:11:43, 132.98s/it] 73%|███████▎  | 759/1034 [32:26:40<9:57:16, 130.31s/it]  74%|███████▎  | 760/1034 [32:28:36<9:34:26, 125.79s/it]                                                        {'loss': 0.0914, 'grad_norm': 1.5368872253522545, 'learning_rate': 4.013371176873849e-06, 'epoch': 1.47}
 74%|███████▎  | 760/1034 [32:28:36<9:34:26, 125.79s/it] 74%|███████▎  | 761/1034 [32:30:42<9:32:36, 125.85s/it] 74%|███████▎  | 762/1034 [32:33:28<10:25:36, 138.00s/it] 74%|███████▍  | 763/1034 [32:36:38<11:33:50, 153.62s/it] 74%|███████▍  | 764/1034 [32:39:12<11:32:33, 153.90s/it] 74%|███████▍  | 765/1034 [32:41:25<11:01:22, 147.52s/it] 74%|███████▍  | 766/1034 [32:43:39<10:40:58, 143.50s/it] 74%|███████▍  | 767/1034 [32:45:44<10:14:00, 137.98s/it] 74%|███████▍  | 768/1034 [32:48:00<10:08:59, 137.37s/it] 74%|███████▍  | 769/1034 [32:50:24<10:14:49, 139.20s/it] 74%|███████▍  | 770/1034 [32:53:00<10:34:30, 144.21s/it]                                                         {'loss': 0.124, 'grad_norm': 1.4887044036459856, 'learning_rate': 3.7462553567836324e-06, 'epoch': 1.49}
 74%|███████▍  | 770/1034 [32:53:00<10:34:30, 144.21s/it] 75%|███████▍  | 771/1034 [32:55:39<10:52:14, 148.80s/it] 75%|███████▍  | 772/1034 [32:57:51<10:27:56, 143.80s/it] 75%|███████▍  | 773/1034 [33:00:10<10:18:41, 142.23s/it] 75%|███████▍  | 774/1034 [33:02:16<9:55:46, 137.49s/it]  75%|███████▍  | 775/1034 [33:04:18<9:33:40, 132.90s/it] 75%|███████▌  | 776/1034 [33:06:31<9:30:52, 132.76s/it] 75%|███████▌  | 777/1034 [33:09:36<10:35:21, 148.33s/it] 75%|███████▌  | 778/1034 [33:12:28<11:03:51, 155.59s/it] 75%|███████▌  | 779/1034 [33:14:30<10:18:10, 145.45s/it] 75%|███████▌  | 780/1034 [33:16:29<9:41:38, 137.39s/it]                                                         {'loss': 0.1166, 'grad_norm': 1.9746669875212879, 'learning_rate': 3.48627517277778e-06, 'epoch': 1.51}
 75%|███████▌  | 780/1034 [33:16:29<9:41:38, 137.39s/it] 76%|███████▌  | 781/1034 [33:18:36<9:27:11, 134.51s/it] 76%|███████▌  | 782/1034 [33:21:00<9:36:33, 137.27s/it] 76%|███████▌  | 783/1034 [33:23:30<9:50:40, 141.20s/it] 76%|███████▌  | 784/1034 [33:26:00<9:58:43, 143.69s/it] 76%|███████▌  | 785/1034 [33:28:23<9:55:41, 143.54s/it] 76%|███████▌  | 786/1034 [33:30:27<9:29:10, 137.70s/it] 76%|███████▌  | 787/1034 [33:32:34<9:13:22, 134.42s/it] 76%|███████▌  | 788/1034 [33:34:54<9:18:28, 136.21s/it] 76%|███████▋  | 789/1034 [33:37:19<9:26:32, 138.74s/it] 76%|███████▋  | 790/1034 [33:39:37<9:23:27, 138.56s/it]                                                        {'loss': 0.1224, 'grad_norm': 1.7839329313644472, 'learning_rate': 3.233727266962425e-06, 'epoch': 1.53}
 76%|███████▋  | 790/1034 [33:39:37<9:23:27, 138.56s/it] 76%|███████▋  | 791/1034 [33:41:45<9:07:39, 135.23s/it] 77%|███████▋  | 792/1034 [33:43:56<9:00:40, 134.05s/it] 77%|███████▋  | 793/1034 [33:46:17<9:06:52, 136.15s/it] 77%|███████▋  | 794/1034 [33:48:52<9:27:57, 141.99s/it] 77%|███████▋  | 795/1034 [33:51:18<9:29:50, 143.06s/it] 77%|███████▋  | 796/1034 [33:53:36<9:21:50, 141.64s/it] 77%|███████▋  | 797/1034 [33:55:53<9:13:02, 140.01s/it] 77%|███████▋  | 798/1034 [33:58:24<9:24:06, 143.42s/it] 77%|███████▋  | 799/1034 [34:01:00<9:36:30, 147.19s/it] 77%|███████▋  | 800/1034 [34:03:22<9:27:45, 145.58s/it]                                                        {'loss': 0.1106, 'grad_norm': 1.6106776855050629, 'learning_rate': 2.9888998010794745e-06, 'epoch': 1.55}
 77%|███████▋  | 800/1034 [34:03:22<9:27:45, 145.58s/it] 77%|███████▋  | 801/1034 [34:05:41<9:17:58, 143.69s/it] 78%|███████▊  | 802/1034 [34:07:53<9:01:47, 140.12s/it] 78%|███████▊  | 803/1034 [34:09:53<8:36:24, 134.13s/it] 78%|███████▊  | 804/1034 [34:11:51<8:15:52, 129.36s/it] 78%|███████▊  | 805/1034 [34:13:47<7:57:54, 125.22s/it] 78%|███████▊  | 806/1034 [34:15:42<7:44:12, 122.16s/it] 78%|███████▊  | 807/1034 [34:18:08<8:08:55, 129.23s/it] 78%|███████▊  | 808/1034 [34:20:40<8:33:25, 136.31s/it] 78%|███████▊  | 809/1034 [34:23:17<8:54:14, 142.47s/it] 78%|███████▊  | 810/1034 [34:26:02<9:17:25, 149.31s/it]                                                        {'loss': 0.1426, 'grad_norm': 2.71151295006156, 'learning_rate': 2.7520721277088023e-06, 'epoch': 1.57}
 78%|███████▊  | 810/1034 [34:26:02<9:17:25, 149.31s/it] 78%|███████▊  | 811/1034 [34:28:44<9:28:07, 152.86s/it] 79%|███████▊  | 812/1034 [34:30:49<8:54:52, 144.56s/it] 79%|███████▊  | 813/1034 [34:32:58<8:35:09, 139.86s/it] 79%|███████▊  | 814/1034 [34:35:09<8:23:09, 137.22s/it] 79%|███████▉  | 815/1034 [34:37:31<8:25:51, 138.59s/it] 79%|███████▉  | 816/1034 [34:40:34<9:12:12, 151.98s/it] 79%|███████▉  | 817/1034 [34:42:47<8:49:45, 146.48s/it] 79%|███████▉  | 818/1034 [34:45:02<8:34:10, 142.83s/it] 79%|███████▉  | 819/1034 [34:47:08<8:14:07, 137.90s/it] 79%|███████▉  | 820/1034 [34:49:10<7:55:09, 133.22s/it]                                                        {'loss': 0.1084, 'grad_norm': 2.8330311385567413, 'learning_rate': 2.523514471521913e-06, 'epoch': 1.59}
 79%|███████▉  | 820/1034 [34:49:10<7:55:09, 133.22s/it] 79%|███████▉  | 821/1034 [34:51:37<8:07:00, 137.19s/it] 79%|███████▉  | 822/1034 [34:54:35<8:48:29, 149.57s/it] 80%|███████▉  | 823/1034 [34:57:01<8:42:09, 148.48s/it] 80%|███████▉  | 824/1034 [34:59:15<8:24:27, 144.13s/it] 80%|███████▉  | 825/1034 [35:01:20<8:01:32, 138.24s/it] 80%|███████▉  | 826/1034 [35:03:28<7:49:20, 135.39s/it] 80%|███████▉  | 827/1034 [35:06:12<8:15:49, 143.72s/it] 80%|████████  | 828/1034 [35:08:48<8:26:22, 147.49s/it] 80%|████████  | 829/1034 [35:11:05<8:13:19, 144.39s/it] 80%|████████  | 830/1034 [35:13:05<7:46:09, 137.11s/it]                                                        {'loss': 0.1019, 'grad_norm': 1.6133400558644984, 'learning_rate': 2.303487620950677e-06, 'epoch': 1.61}
 80%|████████  | 830/1034 [35:13:05<7:46:09, 137.11s/it] 80%|████████  | 831/1034 [35:15:03<7:24:23, 131.35s/it] 80%|████████  | 832/1034 [35:17:30<7:37:36, 135.93s/it] 81%|████████  | 833/1034 [35:19:55<7:44:44, 138.73s/it] 81%|████████  | 834/1034 [35:22:00<7:28:30, 134.55s/it] 81%|████████  | 835/1034 [35:24:28<7:39:59, 138.69s/it] 81%|████████  | 836/1034 [35:26:42<7:32:54, 137.24s/it] 81%|████████  | 837/1034 [35:28:56<7:27:00, 136.15s/it] 81%|████████  | 838/1034 [35:31:21<7:33:30, 138.83s/it] 81%|████████  | 839/1034 [35:33:35<7:26:51, 137.49s/it] 81%|████████  | 840/1034 [35:35:36<7:08:22, 132.49s/it]                                                        {'loss': 0.0789, 'grad_norm': 0.9865849149549613, 'learning_rate': 2.092242630623016e-06, 'epoch': 1.63}
 81%|████████  | 840/1034 [35:35:36<7:08:22, 132.49s/it] 81%|████████▏ | 841/1034 [35:38:03<7:20:25, 136.92s/it] 81%|████████▏ | 842/1034 [35:40:34<7:31:55, 141.23s/it] 82%|████████▏ | 843/1034 [35:42:54<7:27:56, 140.71s/it] 82%|████████▏ | 844/1034 [35:45:11<7:22:03, 139.60s/it] 82%|████████▏ | 845/1034 [35:47:16<7:05:41, 135.14s/it] 82%|████████▏ | 846/1034 [35:49:23<6:55:41, 132.67s/it] 82%|████████▏ | 847/1034 [35:52:02<7:18:14, 140.61s/it] 82%|████████▏ | 848/1034 [35:54:18<7:11:39, 139.24s/it] 82%|████████▏ | 849/1034 [35:56:29<7:02:24, 137.00s/it] 82%|████████▏ | 850/1034 [35:58:41<6:55:17, 135.42s/it]                                                        {'loss': 0.1059, 'grad_norm': 1.6338471918869344, 'learning_rate': 1.8900205349049904e-06, 'epoch': 1.64}
 82%|████████▏ | 850/1034 [35:58:41<6:55:17, 135.42s/it] 82%|████████▏ | 851/1034 [36:01:00<6:56:02, 136.41s/it] 82%|████████▏ | 852/1034 [36:03:24<7:00:17, 138.56s/it] 82%|████████▏ | 853/1034 [36:05:44<6:59:32, 139.07s/it] 83%|████████▎ | 854/1034 [36:07:52<6:47:44, 135.91s/it] 83%|████████▎ | 855/1034 [36:10:04<6:41:58, 134.74s/it] 83%|████████▎ | 856/1034 [36:12:38<6:56:39, 140.45s/it] 83%|████████▎ | 857/1034 [36:15:01<6:56:51, 141.31s/it] 83%|████████▎ | 858/1034 [36:17:26<6:57:04, 142.18s/it] 83%|████████▎ | 859/1034 [36:19:36<6:43:55, 138.49s/it] 83%|████████▎ | 860/1034 [36:21:38<6:27:41, 133.68s/it]                                                        {'loss': 0.1173, 'grad_norm': 1.0033565856047928, 'learning_rate': 1.6970520728762374e-06, 'epoch': 1.66}
 83%|████████▎ | 860/1034 [36:21:38<6:27:41, 133.68s/it] 83%|████████▎ | 861/1034 [36:24:04<6:35:47, 137.27s/it] 83%|████████▎ | 862/1034 [36:26:17<6:30:05, 136.08s/it] 83%|████████▎ | 863/1034 [36:28:28<6:23:35, 134.59s/it] 84%|████████▎ | 864/1034 [36:30:59<6:34:54, 139.38s/it] 84%|████████▎ | 865/1034 [36:33:10<6:26:06, 137.08s/it] 84%|████████▍ | 866/1034 [36:35:16<6:14:31, 133.76s/it] 84%|████████▍ | 867/1034 [36:37:39<6:20:08, 136.58s/it] 84%|████████▍ | 868/1034 [36:40:17<6:35:35, 142.98s/it] 84%|████████▍ | 869/1034 [36:42:38<6:31:37, 142.41s/it] 84%|████████▍ | 870/1034 [36:44:59<6:28:05, 141.98s/it]                                                        {'loss': 0.1085, 'grad_norm': 2.259178674342013, 'learning_rate': 1.5135574250524898e-06, 'epoch': 1.68}
 84%|████████▍ | 870/1034 [36:44:59<6:28:05, 141.98s/it] 84%|████████▍ | 871/1034 [36:47:14<6:19:34, 139.72s/it] 84%|████████▍ | 872/1034 [36:49:27<6:11:49, 137.71s/it] 84%|████████▍ | 873/1034 [36:51:47<6:11:06, 138.30s/it] 85%|████████▍ | 874/1034 [36:54:03<6:07:00, 137.63s/it] 85%|████████▍ | 875/1034 [36:56:07<5:54:18, 133.70s/it] 85%|████████▍ | 876/1034 [36:58:30<5:59:08, 136.38s/it] 85%|████████▍ | 877/1034 [37:00:57<6:05:36, 139.72s/it] 85%|████████▍ | 878/1034 [37:03:22<6:07:11, 141.23s/it] 85%|████████▌ | 879/1034 [37:05:34<5:57:34, 138.42s/it] 85%|████████▌ | 880/1034 [37:07:41<5:46:29, 135.00s/it]                                                        {'loss': 0.1157, 'grad_norm': 1.2538416029779882, 'learning_rate': 1.339745962155613e-06, 'epoch': 1.7}
 85%|████████▌ | 880/1034 [37:07:41<5:46:29, 135.00s/it] 85%|████████▌ | 881/1034 [37:09:55<5:43:25, 134.68s/it] 85%|████████▌ | 882/1034 [37:12:16<5:46:14, 136.68s/it] 85%|████████▌ | 883/1034 [37:14:37<5:46:47, 137.80s/it] 85%|████████▌ | 884/1034 [37:16:47<5:38:57, 135.58s/it] 86%|████████▌ | 885/1034 [37:18:49<5:26:48, 131.60s/it] 86%|████████▌ | 886/1034 [37:20:57<5:21:18, 130.26s/it] 86%|████████▌ | 887/1034 [37:23:20<5:28:32, 134.10s/it] 86%|████████▌ | 888/1034 [37:25:35<5:27:21, 134.53s/it] 86%|████████▌ | 889/1034 [37:27:43<5:19:57, 132.40s/it] 86%|████████▌ | 890/1034 [37:29:50<5:14:07, 130.88s/it]                                                        {'loss': 0.0933, 'grad_norm': 1.7421871266225994, 'learning_rate': 1.1758160062178093e-06, 'epoch': 1.72}
 86%|████████▌ | 890/1034 [37:29:50<5:14:07, 130.88s/it] 86%|████████▌ | 891/1034 [37:31:49<5:03:30, 127.34s/it] 86%|████████▋ | 892/1034 [37:34:05<5:07:16, 129.84s/it] 86%|████████▋ | 893/1034 [37:36:25<5:12:45, 133.09s/it] 86%|████████▋ | 894/1034 [37:38:39<5:10:42, 133.16s/it] 87%|████████▋ | 895/1034 [37:40:45<5:03:39, 131.08s/it] 87%|████████▋ | 896/1034 [37:42:51<4:57:55, 129.53s/it] 87%|████████▋ | 897/1034 [37:45:24<5:12:09, 136.71s/it] 87%|████████▋ | 898/1034 [37:48:03<5:24:56, 143.36s/it] 87%|████████▋ | 899/1034 [37:50:20<5:18:03, 141.36s/it] 87%|████████▋ | 900/1034 [37:52:36<5:12:18, 139.84s/it]                                                        {'loss': 0.0982, 'grad_norm': 1.9481636939082958, 'learning_rate': 1.0219546042925842e-06, 'epoch': 1.74}
 87%|████████▋ | 900/1034 [37:52:36<5:12:18, 139.84s/it][INFO|trainer.py:4309] 2025-12-19 15:27:36,743 >> Saving model checkpoint to saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900
[INFO|configuration_utils.py:763] 2025-12-19 15:27:36,765 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-19 15:27:36,766 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2421] 2025-12-19 15:27:36,843 >> chat template saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 15:27:36,845 >> tokenizer config file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 15:27:36,845 >> Special tokens file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/special_tokens_map.json
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2025-12-19 15:27:37,063] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
[2025-12-19 15:27:38,145] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-19 15:27:38,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-19 15:27:38,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/global_step900/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-19 15:27:38,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-19 15:27:38,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-19 15:27:38,312] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-19 15:27:38,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
 87%|████████▋ | 901/1034 [37:55:06<5:16:27, 142.76s/it] 87%|████████▋ | 902/1034 [37:57:19<5:07:34, 139.81s/it] 87%|████████▋ | 903/1034 [37:59:42<5:07:26, 140.81s/it] 87%|████████▋ | 904/1034 [38:02:03<5:05:39, 141.07s/it] 88%|████████▊ | 905/1034 [38:04:25<5:03:49, 141.32s/it] 88%|████████▊ | 906/1034 [38:06:30<4:50:36, 136.22s/it] 88%|████████▊ | 907/1034 [38:08:35<4:41:26, 132.97s/it] 88%|████████▊ | 908/1034 [38:10:56<4:44:04, 135.27s/it] 88%|████████▊ | 909/1034 [38:13:16<4:45:06, 136.85s/it] 88%|████████▊ | 910/1034 [38:15:29<4:40:34, 135.76s/it]                                                        {'loss': 0.1206, 'grad_norm': 2.575289040147813, 'learning_rate': 8.783373150306663e-07, 'epoch': 1.76}
 88%|████████▊ | 910/1034 [38:15:29<4:40:34, 135.76s/it] 88%|████████▊ | 911/1034 [38:17:32<4:30:25, 131.91s/it] 88%|████████▊ | 912/1034 [38:19:29<4:18:45, 127.26s/it] 88%|████████▊ | 913/1034 [38:21:43<4:20:49, 129.34s/it] 88%|████████▊ | 914/1034 [38:24:12<4:30:13, 135.11s/it] 88%|████████▊ | 915/1034 [38:26:27<4:27:58, 135.11s/it] 89%|████████▊ | 916/1034 [38:28:31<4:19:12, 131.80s/it] 89%|████████▊ | 917/1034 [38:30:30<4:09:55, 128.16s/it] 89%|████████▉ | 918/1034 [38:32:28<4:01:31, 124.93s/it] 89%|████████▉ | 919/1034 [38:35:09<4:20:29, 135.91s/it] 89%|████████▉ | 920/1034 [38:37:30<4:20:52, 137.30s/it]                                                        {'loss': 0.1141, 'grad_norm': 2.4765974097211787, 'learning_rate': 7.451280083644052e-07, 'epoch': 1.78}
 89%|████████▉ | 920/1034 [38:37:30<4:20:52, 137.30s/it] 89%|████████▉ | 921/1034 [38:39:28<4:07:55, 131.64s/it] 89%|████████▉ | 922/1034 [38:41:29<3:59:37, 128.37s/it] 89%|████████▉ | 923/1034 [38:43:29<3:53:04, 125.98s/it] 89%|████████▉ | 924/1034 [38:46:04<4:06:39, 134.54s/it] 89%|████████▉ | 925/1034 [38:48:33<4:12:28, 138.97s/it] 90%|████████▉ | 926/1034 [38:50:39<4:02:48, 134.89s/it] 90%|████████▉ | 927/1034 [38:52:40<3:53:20, 130.84s/it] 90%|████████▉ | 928/1034 [38:54:43<3:47:00, 128.50s/it] 90%|████████▉ | 929/1034 [38:57:17<3:58:25, 136.25s/it] 90%|████████▉ | 930/1034 [38:59:42<4:00:36, 138.81s/it]                                                        {'loss': 0.0983, 'grad_norm': 1.2721818683108552, 'learning_rate': 6.22478678529197e-07, 'epoch': 1.8}
 90%|████████▉ | 930/1034 [38:59:42<4:00:36, 138.81s/it] 90%|█████████ | 931/1034 [39:01:44<3:49:45, 133.84s/it] 90%|█████████ | 932/1034 [39:03:49<3:42:41, 131.00s/it] 90%|█████████ | 933/1034 [39:06:00<3:40:34, 131.03s/it] 90%|█████████ | 934/1034 [39:08:14<3:40:00, 132.00s/it] 90%|█████████ | 935/1034 [39:10:40<3:44:43, 136.20s/it] 91%|█████████ | 936/1034 [39:12:52<3:40:21, 134.91s/it] 91%|█████████ | 937/1034 [39:14:56<3:32:59, 131.75s/it] 91%|█████████ | 938/1034 [39:16:57<3:25:20, 128.34s/it] 91%|█████████ | 939/1034 [39:19:11<3:25:44, 129.94s/it] 91%|█████████ | 940/1034 [39:21:34<3:29:49, 133.93s/it]                                                        {'loss': 0.1089, 'grad_norm': 1.6435932321583953, 'learning_rate': 5.105292706353093e-07, 'epoch': 1.82}
 91%|█████████ | 940/1034 [39:21:34<3:29:49, 133.93s/it] 91%|█████████ | 941/1034 [39:23:51<3:29:08, 134.94s/it] 91%|█████████ | 942/1034 [39:26:04<3:25:59, 134.35s/it] 91%|█████████ | 943/1034 [39:28:07<3:18:31, 130.89s/it] 91%|█████████▏| 944/1034 [39:30:18<3:16:40, 131.12s/it] 91%|█████████▏| 945/1034 [39:32:30<3:14:28, 131.11s/it] 91%|█████████▏| 946/1034 [39:34:43<3:13:26, 131.89s/it] 92%|█████████▏| 947/1034 [39:36:57<3:12:11, 132.55s/it] 92%|█████████▏| 948/1034 [39:39:18<3:13:32, 135.02s/it] 92%|█████████▏| 949/1034 [39:41:25<3:07:53, 132.62s/it] 92%|█████████▏| 950/1034 [39:43:33<3:03:49, 131.30s/it]                                                        {'loss': 0.0958, 'grad_norm': 1.651144536284834, 'learning_rate': 4.094075209879789e-07, 'epoch': 1.84}
 92%|█████████▏| 950/1034 [39:43:33<3:03:49, 131.30s/it] 92%|█████████▏| 951/1034 [39:45:52<3:04:30, 133.38s/it] 92%|█████████▏| 952/1034 [39:48:08<3:03:19, 134.14s/it] 92%|█████████▏| 953/1034 [39:50:29<3:04:07, 136.39s/it] 92%|█████████▏| 954/1034 [39:52:35<2:57:38, 133.23s/it] 92%|█████████▏| 955/1034 [39:54:45<2:54:17, 132.38s/it] 92%|█████████▏| 956/1034 [39:57:04<2:54:26, 134.19s/it] 93%|█████████▎| 957/1034 [39:59:17<2:51:44, 133.83s/it] 93%|█████████▎| 958/1034 [40:01:33<2:50:22, 134.50s/it] 93%|█████████▎| 959/1034 [40:03:36<2:43:59, 131.20s/it] 93%|█████████▎| 960/1034 [40:05:54<2:44:12, 133.15s/it]                                                        {'loss': 0.1095, 'grad_norm': 1.3599652622516776, 'learning_rate': 3.1922881133795827e-07, 'epoch': 1.86}
 93%|█████████▎| 960/1034 [40:05:54<2:44:12, 133.15s/it] 93%|█████████▎| 961/1034 [40:08:15<2:44:50, 135.49s/it] 93%|█████████▎| 962/1034 [40:10:30<2:42:26, 135.36s/it] 93%|█████████▎| 963/1034 [40:12:38<2:37:22, 132.99s/it] 93%|█████████▎| 964/1034 [40:14:49<2:34:39, 132.57s/it] 93%|█████████▎| 965/1034 [40:17:14<2:36:38, 136.21s/it] 93%|█████████▎| 966/1034 [40:19:41<2:37:55, 139.34s/it] 94%|█████████▎| 967/1034 [40:21:54<2:33:43, 137.67s/it] 94%|█████████▎| 968/1034 [40:24:07<2:29:57, 136.33s/it] 94%|█████████▎| 969/1034 [40:26:20<2:26:19, 135.07s/it] 94%|█████████▍| 970/1034 [40:28:39<2:25:28, 136.39s/it]                                                        {'loss': 0.1088, 'grad_norm': 1.8862264279888048, 'learning_rate': 2.4009603722884745e-07, 'epoch': 1.88}
 94%|█████████▍| 970/1034 [40:28:39<2:25:28, 136.39s/it] 94%|█████████▍| 971/1034 [40:30:46<2:20:15, 133.57s/it] 94%|█████████▍| 972/1034 [40:32:52<2:15:41, 131.31s/it] 94%|█████████▍| 973/1034 [40:35:02<2:12:58, 130.80s/it] 94%|█████████▍| 974/1034 [40:37:13<2:10:52, 130.88s/it] 94%|█████████▍| 975/1034 [40:39:23<2:08:31, 130.70s/it] 94%|█████████▍| 976/1034 [40:41:26<2:04:12, 128.49s/it] 94%|█████████▍| 977/1034 [40:43:33<2:01:31, 127.91s/it] 95%|█████████▍| 978/1034 [40:45:41<1:59:33, 128.09s/it] 95%|█████████▍| 979/1034 [40:48:02<2:00:42, 131.69s/it] 95%|█████████▍| 980/1034 [40:50:20<2:00:23, 133.76s/it]                                                        {'loss': 0.1221, 'grad_norm': 1.2114588326668176, 'learning_rate': 1.7209949059142084e-07, 'epoch': 1.9}
 95%|█████████▍| 980/1034 [40:50:20<2:00:23, 133.76s/it] 95%|█████████▍| 981/1034 [40:52:28<1:56:42, 132.12s/it] 95%|█████████▍| 982/1034 [40:54:44<1:55:24, 133.17s/it] 95%|█████████▌| 983/1034 [40:57:07<1:55:44, 136.17s/it] 95%|█████████▌| 984/1034 [40:59:22<1:53:02, 135.65s/it] 95%|█████████▌| 985/1034 [41:01:39<1:51:13, 136.19s/it] 95%|█████████▌| 986/1034 [41:03:54<1:48:37, 135.78s/it] 95%|█████████▌| 987/1034 [41:06:23<1:49:28, 139.76s/it] 96%|█████████▌| 988/1034 [41:08:43<1:47:06, 139.70s/it] 96%|█████████▌| 989/1034 [41:10:45<1:40:57, 134.61s/it] 96%|█████████▌| 990/1034 [41:12:55<1:37:33, 133.04s/it]                                                        {'loss': 0.0971, 'grad_norm': 1.4223092276912435, 'learning_rate': 1.1531675671888621e-07, 'epoch': 1.92}
 96%|█████████▌| 990/1034 [41:12:55<1:37:33, 133.04s/it] 96%|█████████▌| 991/1034 [41:15:31<1:40:17, 139.94s/it] 96%|█████████▌| 992/1034 [41:18:05<1:41:02, 144.35s/it] 96%|█████████▌| 993/1034 [41:20:21<1:36:55, 141.83s/it] 96%|█████████▌| 994/1034 [41:22:25<1:30:58, 136.47s/it] 96%|█████████▌| 995/1034 [41:24:38<1:27:58, 135.35s/it] 96%|█████████▋| 996/1034 [41:27:22<1:31:14, 144.07s/it] 96%|█████████▋| 997/1034 [41:29:48<1:29:05, 144.47s/it] 97%|█████████▋| 998/1034 [41:32:04<1:25:14, 142.07s/it] 97%|█████████▋| 999/1034 [41:34:17<1:21:09, 139.12s/it] 97%|█████████▋| 1000/1034 [41:36:39<1:19:25, 140.17s/it]                                                         {'loss': 0.1281, 'grad_norm': 1.2608133757934237, 'learning_rate': 6.981262574066395e-08, 'epoch': 1.94}
 97%|█████████▋| 1000/1034 [41:36:39<1:19:25, 140.17s/it][INFO|trainer.py:4643] 2025-12-19 19:11:20,343 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-19 19:11:20,343 >>   Num examples = 2754
[INFO|trainer.py:4648] 2025-12-19 19:11:20,343 >>   Batch size = 2

  0%|          | 0/459 [00:00<?, ?it/s][A
  0%|          | 2/459 [00:05<21:29,  2.82s/it][A
  1%|          | 3/459 [00:11<29:59,  3.95s/it][A
  1%|          | 4/459 [00:16<34:16,  4.52s/it][A
  1%|          | 5/459 [00:22<36:36,  4.84s/it][A
  1%|▏         | 6/459 [00:27<37:56,  5.03s/it][A
  2%|▏         | 7/459 [00:32<38:45,  5.14s/it][A
  2%|▏         | 8/459 [00:38<39:38,  5.27s/it][A
  2%|▏         | 9/459 [00:44<40:14,  5.36s/it][A
  2%|▏         | 10/459 [00:49<40:25,  5.40s/it][A
  2%|▏         | 11/459 [00:54<39:58,  5.35s/it][A
  3%|▎         | 12/459 [01:00<40:01,  5.37s/it][A
  3%|▎         | 13/459 [01:05<39:58,  5.38s/it][A
  3%|▎         | 14/459 [01:11<40:15,  5.43s/it][A
  3%|▎         | 15/459 [01:16<39:52,  5.39s/it][A
  3%|▎         | 16/459 [01:21<39:39,  5.37s/it][A
  4%|▎         | 17/459 [01:27<39:28,  5.36s/it][A
  4%|▍         | 18/459 [01:32<38:53,  5.29s/it][A
  4%|▍         | 19/459 [01:37<39:17,  5.36s/it][A
  4%|▍         | 20/459 [01:43<40:30,  5.54s/it][A
  5%|▍         | 21/459 [01:49<41:30,  5.69s/it][A
  5%|▍         | 22/459 [01:55<41:32,  5.70s/it][A
  5%|▌         | 23/459 [02:00<41:10,  5.67s/it][A
  5%|▌         | 24/459 [02:06<40:00,  5.52s/it][A
  5%|▌         | 25/459 [02:11<38:49,  5.37s/it][A
  6%|▌         | 26/459 [02:16<37:58,  5.26s/it][A
  6%|▌         | 27/459 [02:21<37:49,  5.25s/it][A
  6%|▌         | 28/459 [02:25<36:07,  5.03s/it][A
  6%|▋         | 29/459 [02:30<34:20,  4.79s/it][A
  7%|▋         | 30/459 [02:34<33:14,  4.65s/it][A
  7%|▋         | 31/459 [02:38<31:53,  4.47s/it][A
  7%|▋         | 32/459 [02:42<30:52,  4.34s/it][A
  7%|▋         | 33/459 [02:46<30:35,  4.31s/it][A
  7%|▋         | 34/459 [02:51<30:21,  4.28s/it][A
  8%|▊         | 35/459 [02:55<31:11,  4.41s/it][A
  8%|▊         | 36/459 [03:00<32:13,  4.57s/it][A
  8%|▊         | 37/459 [03:05<31:45,  4.51s/it][A
  8%|▊         | 38/459 [03:09<31:36,  4.50s/it][A
  8%|▊         | 39/459 [03:13<31:14,  4.46s/it][A
  9%|▊         | 40/459 [03:18<30:47,  4.41s/it][A
  9%|▉         | 41/459 [03:22<31:02,  4.46s/it][A
  9%|▉         | 42/459 [03:27<31:03,  4.47s/it][A
  9%|▉         | 43/459 [03:31<31:13,  4.50s/it][A
 10%|▉         | 44/459 [03:36<31:41,  4.58s/it][A
 10%|▉         | 45/459 [03:41<32:21,  4.69s/it][A
 10%|█         | 46/459 [03:46<32:26,  4.71s/it][A
 10%|█         | 47/459 [03:51<32:18,  4.70s/it][A
 10%|█         | 48/459 [03:55<32:10,  4.70s/it][A
 11%|█         | 49/459 [04:00<31:56,  4.67s/it][A
 11%|█         | 50/459 [04:04<31:49,  4.67s/it][A
 11%|█         | 51/459 [04:09<31:25,  4.62s/it][A
 11%|█▏        | 52/459 [04:14<31:17,  4.61s/it][A
 12%|█▏        | 53/459 [04:18<31:11,  4.61s/it][A
 12%|█▏        | 54/459 [04:23<32:05,  4.75s/it][A
 12%|█▏        | 55/459 [04:28<32:24,  4.81s/it][A
 12%|█▏        | 56/459 [04:33<32:02,  4.77s/it][A
 12%|█▏        | 57/459 [04:38<31:50,  4.75s/it][A
 13%|█▎        | 58/459 [04:42<31:59,  4.79s/it][A
 13%|█▎        | 59/459 [04:47<31:53,  4.78s/it][A
 13%|█▎        | 60/459 [04:52<32:22,  4.87s/it][A
 13%|█▎        | 61/459 [04:57<32:15,  4.86s/it][A
 14%|█▎        | 62/459 [05:01<30:59,  4.68s/it][A
 14%|█▎        | 63/459 [05:06<30:02,  4.55s/it][A
 14%|█▍        | 64/459 [05:10<29:08,  4.43s/it][A
 14%|█▍        | 65/459 [05:14<28:21,  4.32s/it][A
 14%|█▍        | 66/459 [05:18<27:56,  4.27s/it][A
 15%|█▍        | 67/459 [05:22<28:07,  4.30s/it][A
 15%|█▍        | 68/459 [05:27<28:04,  4.31s/it][A
 15%|█▌        | 69/459 [05:31<28:03,  4.32s/it][A
 15%|█▌        | 70/459 [05:35<27:56,  4.31s/it][A
 15%|█▌        | 71/459 [05:40<28:17,  4.38s/it][A
 16%|█▌        | 72/459 [05:44<28:14,  4.38s/it][A
 16%|█▌        | 73/459 [05:48<27:28,  4.27s/it][A
 16%|█▌        | 74/459 [05:52<27:01,  4.21s/it][A
 16%|█▋        | 75/459 [05:56<26:39,  4.17s/it][A
 17%|█▋        | 76/459 [06:00<26:19,  4.12s/it][A
 17%|█▋        | 77/459 [06:04<26:04,  4.09s/it][A
 17%|█▋        | 78/459 [06:09<26:00,  4.10s/it][A
 17%|█▋        | 79/459 [06:13<26:12,  4.14s/it][A
 17%|█▋        | 80/459 [06:17<25:53,  4.10s/it][A
 18%|█▊        | 81/459 [06:21<25:42,  4.08s/it][A
 18%|█▊        | 82/459 [06:25<25:27,  4.05s/it][A
 18%|█▊        | 83/459 [06:29<25:15,  4.03s/it][A
 18%|█▊        | 84/459 [06:33<24:54,  3.99s/it][A
 19%|█▊        | 85/459 [06:37<24:54,  3.99s/it][A
 19%|█▊        | 86/459 [06:41<24:50,  4.00s/it][A
 19%|█▉        | 87/459 [06:45<24:45,  3.99s/it][A
 19%|█▉        | 88/459 [06:49<24:48,  4.01s/it][A
 19%|█▉        | 89/459 [06:53<25:08,  4.08s/it][A
 20%|█▉        | 90/459 [06:57<25:08,  4.09s/it][A
 20%|█▉        | 91/459 [07:01<25:34,  4.17s/it][A
 20%|██        | 92/459 [07:06<25:54,  4.23s/it][A
 20%|██        | 93/459 [07:10<26:22,  4.32s/it][A
 20%|██        | 94/459 [07:15<26:44,  4.40s/it][A
 21%|██        | 95/459 [07:19<26:34,  4.38s/it][A
 21%|██        | 96/459 [07:24<26:45,  4.42s/it][A
 21%|██        | 97/459 [07:28<26:44,  4.43s/it][A
 21%|██▏       | 98/459 [07:33<26:22,  4.38s/it][A
 22%|██▏       | 99/459 [07:37<25:59,  4.33s/it][A
 22%|██▏       | 100/459 [07:41<25:42,  4.30s/it][A
 22%|██▏       | 101/459 [07:45<25:25,  4.26s/it][A
 22%|██▏       | 102/459 [07:49<25:12,  4.24s/it][A
 22%|██▏       | 103/459 [07:53<24:45,  4.17s/it][A
 23%|██▎       | 104/459 [07:57<24:34,  4.15s/it][A
 23%|██▎       | 105/459 [08:02<24:22,  4.13s/it][A
 23%|██▎       | 106/459 [08:06<24:06,  4.10s/it][A
 23%|██▎       | 107/459 [08:10<23:52,  4.07s/it][A
 24%|██▎       | 108/459 [08:14<23:41,  4.05s/it][A
 24%|██▎       | 109/459 [08:18<23:40,  4.06s/it][A
 24%|██▍       | 110/459 [08:22<23:51,  4.10s/it][A
 24%|██▍       | 111/459 [08:26<24:12,  4.17s/it][A
 24%|██▍       | 112/459 [08:31<24:37,  4.26s/it][A
 25%|██▍       | 113/459 [08:35<24:42,  4.29s/it][A
 25%|██▍       | 114/459 [08:39<24:33,  4.27s/it][A
 25%|██▌       | 115/459 [08:43<24:15,  4.23s/it][A
 25%|██▌       | 116/459 [08:48<24:38,  4.31s/it][A
 25%|██▌       | 117/459 [08:52<24:27,  4.29s/it][A
 26%|██▌       | 118/459 [08:56<24:14,  4.27s/it][A
 26%|██▌       | 119/459 [09:01<24:05,  4.25s/it][A
 26%|██▌       | 120/459 [09:05<24:00,  4.25s/it][A
 26%|██▋       | 121/459 [09:09<23:55,  4.25s/it][A
 27%|██▋       | 122/459 [09:13<23:41,  4.22s/it][A
 27%|██▋       | 123/459 [09:17<23:22,  4.17s/it][A
 27%|██▋       | 124/459 [09:21<23:13,  4.16s/it][A
 27%|██▋       | 125/459 [09:25<22:56,  4.12s/it][A
 27%|██▋       | 126/459 [09:29<22:32,  4.06s/it][A
 28%|██▊       | 127/459 [09:33<22:21,  4.04s/it][A
 28%|██▊       | 128/459 [09:37<22:14,  4.03s/it][A
 28%|██▊       | 129/459 [09:41<22:11,  4.04s/it][A
 28%|██▊       | 130/459 [09:46<22:40,  4.13s/it][A
 29%|██▊       | 131/459 [09:50<22:42,  4.15s/it][A
 29%|██▉       | 132/459 [09:54<22:47,  4.18s/it][A
 29%|██▉       | 133/459 [09:58<22:38,  4.17s/it][A
 29%|██▉       | 134/459 [10:03<22:56,  4.24s/it][A
 29%|██▉       | 135/459 [10:07<23:01,  4.26s/it][A
 30%|██▉       | 136/459 [10:11<23:08,  4.30s/it][A
 30%|██▉       | 137/459 [10:16<23:20,  4.35s/it][A
 30%|███       | 138/459 [10:20<23:38,  4.42s/it][A
 30%|███       | 139/459 [10:25<23:51,  4.47s/it][A
 31%|███       | 140/459 [10:30<23:54,  4.50s/it][A
 31%|███       | 141/459 [10:34<23:58,  4.52s/it][A
 31%|███       | 142/459 [10:39<24:04,  4.56s/it][A
 31%|███       | 143/459 [10:43<23:52,  4.53s/it][A
 31%|███▏      | 144/459 [10:48<23:54,  4.55s/it][A
 32%|███▏      | 145/459 [10:52<23:45,  4.54s/it][A
 32%|███▏      | 146/459 [10:57<23:38,  4.53s/it][A
 32%|███▏      | 147/459 [11:01<23:17,  4.48s/it][A
 32%|███▏      | 148/459 [11:06<23:38,  4.56s/it][A
 32%|███▏      | 149/459 [11:11<24:00,  4.65s/it][A
 33%|███▎      | 150/459 [11:16<24:18,  4.72s/it][A
 33%|███▎      | 151/459 [11:21<24:47,  4.83s/it][A
 33%|███▎      | 152/459 [11:26<24:37,  4.81s/it][A
 33%|███▎      | 153/459 [11:30<24:23,  4.78s/it][A
 34%|███▎      | 154/459 [11:35<24:22,  4.79s/it][A
 34%|███▍      | 155/459 [11:40<24:30,  4.84s/it][A
 34%|███▍      | 156/459 [11:45<24:16,  4.81s/it][A
 34%|███▍      | 157/459 [11:50<24:21,  4.84s/it][A
 34%|███▍      | 158/459 [11:55<24:12,  4.83s/it][A
 35%|███▍      | 159/459 [11:59<24:08,  4.83s/it][A
 35%|███▍      | 160/459 [12:04<23:55,  4.80s/it][A
 35%|███▌      | 161/459 [12:09<23:57,  4.82s/it][A
 35%|███▌      | 162/459 [12:14<24:00,  4.85s/it][A
 36%|███▌      | 163/459 [12:19<24:03,  4.88s/it][A
 36%|███▌      | 164/459 [12:24<24:12,  4.92s/it][A
 36%|███▌      | 165/459 [12:29<23:55,  4.88s/it][A
 36%|███▌      | 166/459 [12:34<23:57,  4.91s/it][A
 36%|███▋      | 167/459 [12:39<24:15,  4.99s/it][A
 37%|███▋      | 168/459 [12:44<24:42,  5.09s/it][A
 37%|███▋      | 169/459 [12:49<24:31,  5.08s/it][A
 37%|███▋      | 170/459 [12:54<24:40,  5.12s/it][A
 37%|███▋      | 171/459 [13:00<25:04,  5.23s/it][A
 37%|███▋      | 172/459 [13:06<25:47,  5.39s/it][A
 38%|███▊      | 173/459 [13:11<25:47,  5.41s/it][A
 38%|███▊      | 174/459 [13:16<25:17,  5.32s/it][A
 38%|███▊      | 175/459 [13:21<24:33,  5.19s/it][A
 38%|███▊      | 176/459 [13:26<24:10,  5.12s/it][A
 39%|███▊      | 177/459 [13:31<23:58,  5.10s/it][A
 39%|███▉      | 178/459 [13:36<23:37,  5.05s/it][A
 39%|███▉      | 179/459 [13:41<23:03,  4.94s/it][A
 39%|███▉      | 180/459 [13:45<22:25,  4.82s/it][A
 39%|███▉      | 181/459 [13:50<21:54,  4.73s/it][A
 40%|███▉      | 182/459 [13:54<21:38,  4.69s/it][A
 40%|███▉      | 183/459 [13:59<21:25,  4.66s/it][A
 40%|████      | 184/459 [14:04<21:21,  4.66s/it][A
 40%|████      | 185/459 [14:08<21:26,  4.69s/it][A
 41%|████      | 186/459 [14:13<21:19,  4.69s/it][A
 41%|████      | 187/459 [14:18<21:16,  4.69s/it][A
 41%|████      | 188/459 [14:22<20:58,  4.65s/it][A
 41%|████      | 189/459 [14:27<20:57,  4.66s/it][A
 41%|████▏     | 190/459 [14:32<20:59,  4.68s/it][A
 42%|████▏     | 191/459 [14:37<21:11,  4.75s/it][A
 42%|████▏     | 192/459 [14:41<21:10,  4.76s/it][A
 42%|████▏     | 193/459 [14:46<21:09,  4.77s/it][A
 42%|████▏     | 194/459 [14:51<21:16,  4.82s/it][A
 42%|████▏     | 195/459 [14:56<21:24,  4.87s/it][A
 43%|████▎     | 196/459 [15:01<21:22,  4.88s/it][A
 43%|████▎     | 197/459 [15:06<21:16,  4.87s/it][A
 43%|████▎     | 198/459 [15:11<21:04,  4.84s/it][A
 43%|████▎     | 199/459 [15:16<21:03,  4.86s/it][A
 44%|████▎     | 200/459 [15:21<21:12,  4.91s/it][A
 44%|████▍     | 201/459 [15:26<21:18,  4.95s/it][A
 44%|████▍     | 202/459 [15:31<21:10,  4.95s/it][A
 44%|████▍     | 203/459 [15:36<21:08,  4.96s/it][A
 44%|████▍     | 204/459 [15:41<21:04,  4.96s/it][A
 45%|████▍     | 205/459 [15:46<21:11,  5.01s/it][A
 45%|████▍     | 206/459 [15:51<21:18,  5.05s/it][A
 45%|████▌     | 207/459 [15:56<21:03,  5.01s/it][A
 45%|████▌     | 208/459 [16:01<20:53,  4.99s/it][A
 46%|████▌     | 209/459 [16:06<20:52,  5.01s/it][A
 46%|████▌     | 210/459 [16:11<20:39,  4.98s/it][A
 46%|████▌     | 211/459 [16:16<20:41,  5.00s/it][A
 46%|████▌     | 212/459 [16:21<20:37,  5.01s/it][A
 46%|████▋     | 213/459 [16:25<20:12,  4.93s/it][A
 47%|████▋     | 214/459 [16:30<20:05,  4.92s/it][A
 47%|████▋     | 215/459 [16:35<20:05,  4.94s/it][A
 47%|████▋     | 216/459 [16:40<20:04,  4.96s/it][A
 47%|████▋     | 217/459 [16:46<20:26,  5.07s/it][A
 47%|████▋     | 218/459 [16:51<20:20,  5.06s/it][A
 48%|████▊     | 219/459 [16:56<20:16,  5.07s/it][A
 48%|████▊     | 220/459 [17:01<20:09,  5.06s/it][A
 48%|████▊     | 221/459 [17:05<19:21,  4.88s/it][A
 48%|████▊     | 222/459 [17:10<18:28,  4.68s/it][A
 49%|████▊     | 223/459 [17:14<17:51,  4.54s/it][A
 49%|████▉     | 224/459 [17:18<17:27,  4.46s/it][A
 49%|████▉     | 225/459 [17:22<17:06,  4.39s/it][A
 49%|████▉     | 226/459 [17:26<16:43,  4.31s/it][A
 49%|████▉     | 227/459 [17:31<16:36,  4.30s/it][A
 50%|████▉     | 228/459 [17:35<16:25,  4.27s/it][A
 50%|████▉     | 229/459 [17:39<16:16,  4.25s/it][A
 50%|█████     | 230/459 [17:43<16:05,  4.22s/it][A
 50%|█████     | 231/459 [17:47<15:56,  4.19s/it][A
 51%|█████     | 232/459 [17:51<15:45,  4.16s/it][A
 51%|█████     | 233/459 [17:55<15:31,  4.12s/it][A
 51%|█████     | 234/459 [18:00<15:28,  4.13s/it][A
 51%|█████     | 235/459 [18:04<15:20,  4.11s/it][A
 51%|█████▏    | 236/459 [18:08<15:08,  4.07s/it][A
 52%|█████▏    | 237/459 [18:12<14:58,  4.05s/it][A
 52%|█████▏    | 238/459 [18:16<14:54,  4.05s/it][A
 52%|█████▏    | 239/459 [18:20<14:49,  4.04s/it][A
 52%|█████▏    | 240/459 [18:24<14:45,  4.04s/it][A
 53%|█████▎    | 241/459 [18:28<14:39,  4.03s/it][A
 53%|█████▎    | 242/459 [18:32<14:30,  4.01s/it][A
 53%|█████▎    | 243/459 [18:36<14:26,  4.01s/it][A
 53%|█████▎    | 244/459 [18:40<14:23,  4.01s/it][A
 53%|█████▎    | 245/459 [18:44<14:17,  4.01s/it][A
 54%|█████▎    | 246/459 [18:48<14:18,  4.03s/it][A
 54%|█████▍    | 247/459 [18:52<14:18,  4.05s/it][A
 54%|█████▍    | 248/459 [18:56<14:23,  4.09s/it][A
 54%|█████▍    | 249/459 [19:00<14:21,  4.10s/it][A
 54%|█████▍    | 250/459 [19:04<14:24,  4.14s/it][A
 55%|█████▍    | 251/459 [19:09<14:23,  4.15s/it][A
 55%|█████▍    | 252/459 [19:13<14:29,  4.20s/it][A
 55%|█████▌    | 253/459 [19:17<14:26,  4.21s/it][A
 55%|█████▌    | 254/459 [19:21<14:20,  4.20s/it][A
 56%|█████▌    | 255/459 [19:26<14:23,  4.23s/it][A
 56%|█████▌    | 256/459 [19:30<14:22,  4.25s/it][A
 56%|█████▌    | 257/459 [19:34<14:16,  4.24s/it][A
 56%|█████▌    | 258/459 [19:39<14:20,  4.28s/it][A
 56%|█████▋    | 259/459 [19:43<14:14,  4.27s/it][A
 57%|█████▋    | 260/459 [19:47<14:11,  4.28s/it][A
 57%|█████▋    | 261/459 [19:52<14:16,  4.33s/it][A
 57%|█████▋    | 262/459 [19:56<14:12,  4.33s/it][A
 57%|█████▋    | 263/459 [20:00<14:13,  4.35s/it][A
 58%|█████▊    | 264/459 [20:05<14:13,  4.38s/it][A
 58%|█████▊    | 265/459 [20:09<14:04,  4.35s/it][A
 58%|█████▊    | 266/459 [20:13<14:03,  4.37s/it][A
 58%|█████▊    | 267/459 [20:18<13:56,  4.36s/it][A
 58%|█████▊    | 268/459 [20:22<13:53,  4.36s/it][A
 59%|█████▊    | 269/459 [20:27<14:03,  4.44s/it][A
 59%|█████▉    | 270/459 [20:31<13:54,  4.41s/it][A
 59%|█████▉    | 271/459 [20:35<13:36,  4.34s/it][A
 59%|█████▉    | 272/459 [20:40<13:32,  4.34s/it][A
 59%|█████▉    | 273/459 [20:44<13:25,  4.33s/it][A
 60%|█████▉    | 274/459 [20:48<13:15,  4.30s/it][A
 60%|█████▉    | 275/459 [20:52<13:12,  4.31s/it][A
 60%|██████    | 276/459 [20:57<13:10,  4.32s/it][A
 60%|██████    | 277/459 [21:01<12:59,  4.28s/it][A
 61%|██████    | 278/459 [21:05<12:53,  4.27s/it][A
 61%|██████    | 279/459 [21:09<12:46,  4.26s/it][A
 61%|██████    | 280/459 [21:14<12:45,  4.28s/it][A
 61%|██████    | 281/459 [21:18<12:44,  4.30s/it][A
 61%|██████▏   | 282/459 [21:23<12:49,  4.35s/it][A
 62%|██████▏   | 283/459 [21:27<12:59,  4.43s/it][A
 62%|██████▏   | 284/459 [21:32<13:07,  4.50s/it][A
 62%|██████▏   | 285/459 [21:36<13:04,  4.51s/it][A
 62%|██████▏   | 286/459 [21:41<12:59,  4.51s/it][A
 63%|██████▎   | 287/459 [21:45<12:54,  4.50s/it][A
 63%|██████▎   | 288/459 [21:50<12:52,  4.52s/it][A
 63%|██████▎   | 289/459 [21:54<12:49,  4.53s/it][A
 63%|██████▎   | 290/459 [21:59<12:52,  4.57s/it][A
 63%|██████▎   | 291/459 [22:04<12:52,  4.60s/it][A
 64%|██████▎   | 292/459 [22:09<13:06,  4.71s/it][A
 64%|██████▍   | 293/459 [22:14<13:15,  4.79s/it][A
 64%|██████▍   | 294/459 [22:19<13:13,  4.81s/it][A
 64%|██████▍   | 295/459 [22:24<13:12,  4.84s/it][A
 64%|██████▍   | 296/459 [22:28<13:01,  4.79s/it][A
 65%|██████▍   | 297/459 [22:33<12:44,  4.72s/it][A
 65%|██████▍   | 298/459 [22:37<12:28,  4.65s/it][A
 65%|██████▌   | 299/459 [22:42<12:16,  4.60s/it][A
 65%|██████▌   | 300/459 [22:46<12:05,  4.56s/it][A
 66%|██████▌   | 301/459 [22:51<11:59,  4.56s/it][A
 66%|██████▌   | 302/459 [22:55<11:55,  4.56s/it][A
 66%|██████▌   | 303/459 [23:00<11:53,  4.57s/it][A
 66%|██████▌   | 304/459 [23:05<11:59,  4.64s/it][A
 66%|██████▋   | 305/459 [23:09<11:52,  4.63s/it][A
 67%|██████▋   | 306/459 [23:14<11:52,  4.65s/it][A
 67%|██████▋   | 307/459 [23:19<11:54,  4.70s/it][A
 67%|██████▋   | 308/459 [23:24<11:55,  4.74s/it][A
 67%|██████▋   | 309/459 [23:29<11:57,  4.78s/it][A
 68%|██████▊   | 310/459 [23:33<11:53,  4.79s/it][A
 68%|██████▊   | 311/459 [23:38<11:47,  4.78s/it][A
 68%|██████▊   | 312/459 [23:43<11:50,  4.83s/it][A
 68%|██████▊   | 313/459 [23:48<11:55,  4.90s/it][A
 68%|██████▊   | 314/459 [23:53<11:38,  4.82s/it][A
 69%|██████▊   | 315/459 [23:58<11:34,  4.83s/it][A
 69%|██████▉   | 316/459 [24:02<11:30,  4.83s/it][A
 69%|██████▉   | 317/459 [24:07<11:14,  4.75s/it][A
 69%|██████▉   | 318/459 [24:12<11:13,  4.77s/it][A
 69%|██████▉   | 319/459 [24:17<11:09,  4.78s/it][A
 70%|██████▉   | 320/459 [24:21<11:05,  4.79s/it][A
 70%|██████▉   | 321/459 [24:26<11:06,  4.83s/it][A
 70%|███████   | 322/459 [24:31<10:55,  4.79s/it][A
 70%|███████   | 323/459 [24:36<10:48,  4.77s/it][A
 71%|███████   | 324/459 [24:41<10:42,  4.76s/it][A
 71%|███████   | 325/459 [24:45<10:31,  4.72s/it][A
 71%|███████   | 326/459 [24:50<10:25,  4.70s/it][A
 71%|███████   | 327/459 [24:54<10:19,  4.69s/it][A
 71%|███████▏  | 328/459 [24:59<10:15,  4.70s/it][A
 72%|███████▏  | 329/459 [25:04<10:13,  4.72s/it][A
 72%|███████▏  | 330/459 [25:09<10:08,  4.72s/it][A
 72%|███████▏  | 331/459 [25:13<10:02,  4.70s/it][A
 72%|███████▏  | 332/459 [25:18<09:59,  4.72s/it][A
 73%|███████▎  | 333/459 [25:23<09:55,  4.73s/it][A
 73%|███████▎  | 334/459 [25:28<09:53,  4.75s/it][A
 73%|███████▎  | 335/459 [25:33<09:57,  4.81s/it][A
 73%|███████▎  | 336/459 [25:37<09:52,  4.82s/it][A
 73%|███████▎  | 337/459 [25:43<09:57,  4.90s/it][A
 74%|███████▎  | 338/459 [25:48<09:57,  4.94s/it][A
 74%|███████▍  | 339/459 [25:52<09:47,  4.90s/it][A
 74%|███████▍  | 340/459 [25:57<09:36,  4.85s/it][A
 74%|███████▍  | 341/459 [26:02<09:25,  4.79s/it][A
 75%|███████▍  | 342/459 [26:07<09:20,  4.79s/it][A
 75%|███████▍  | 343/459 [26:12<09:21,  4.84s/it][A
 75%|███████▍  | 344/459 [26:16<09:16,  4.84s/it][A
 75%|███████▌  | 345/459 [26:21<09:04,  4.78s/it][A
 75%|███████▌  | 346/459 [26:26<08:57,  4.75s/it][A
 76%|███████▌  | 347/459 [26:30<08:42,  4.67s/it][A
 76%|███████▌  | 348/459 [26:35<08:30,  4.60s/it][A
 76%|███████▌  | 349/459 [26:39<08:19,  4.54s/it][A
 76%|███████▋  | 350/459 [26:43<08:13,  4.52s/it][A
 76%|███████▋  | 351/459 [26:48<08:09,  4.53s/it][A
 77%|███████▋  | 352/459 [26:52<07:59,  4.48s/it][A
 77%|███████▋  | 353/459 [26:57<07:51,  4.45s/it][A
 77%|███████▋  | 354/459 [27:01<07:44,  4.42s/it][A
 77%|███████▋  | 355/459 [27:06<07:38,  4.41s/it][A
 78%|███████▊  | 356/459 [27:10<07:33,  4.40s/it][A
 78%|███████▊  | 357/459 [27:14<07:28,  4.40s/it][A
 78%|███████▊  | 358/459 [27:19<07:23,  4.39s/it][A
 78%|███████▊  | 359/459 [27:23<07:21,  4.41s/it][A
 78%|███████▊  | 360/459 [27:27<07:06,  4.31s/it][A
 79%|███████▊  | 361/459 [27:31<06:51,  4.20s/it][A
 79%|███████▉  | 362/459 [27:35<06:40,  4.13s/it][A
 79%|███████▉  | 363/459 [27:39<06:35,  4.12s/it][A
 79%|███████▉  | 364/459 [27:43<06:30,  4.11s/it][A
 80%|███████▉  | 365/459 [27:47<06:26,  4.11s/it][A
 80%|███████▉  | 366/459 [27:51<06:21,  4.10s/it][A
 80%|███████▉  | 367/459 [27:56<06:15,  4.08s/it][A
 80%|████████  | 368/459 [28:00<06:10,  4.07s/it][A
 80%|████████  | 369/459 [28:04<06:11,  4.12s/it][A
 81%|████████  | 370/459 [28:08<06:07,  4.13s/it][A
 81%|████████  | 371/459 [28:12<06:08,  4.19s/it][A
 81%|████████  | 372/459 [28:17<06:05,  4.20s/it][A
 81%|████████▏ | 373/459 [28:21<06:00,  4.20s/it][A
 81%|████████▏ | 374/459 [28:25<05:59,  4.23s/it][A
 82%|████████▏ | 375/459 [28:29<05:56,  4.25s/it][A
 82%|████████▏ | 376/459 [28:34<05:52,  4.25s/it][A
 82%|████████▏ | 377/459 [28:38<05:52,  4.29s/it][A
 82%|████████▏ | 378/459 [28:42<05:50,  4.32s/it][A
 83%|████████▎ | 379/459 [28:47<05:48,  4.35s/it][A
 83%|████████▎ | 380/459 [28:51<05:43,  4.35s/it][A
 83%|████████▎ | 381/459 [28:56<05:41,  4.38s/it][A
 83%|████████▎ | 382/459 [29:00<05:41,  4.44s/it][A
 83%|████████▎ | 383/459 [29:05<05:38,  4.45s/it][A
 84%|████████▎ | 384/459 [29:09<05:35,  4.47s/it][A
 84%|████████▍ | 385/459 [29:14<05:33,  4.50s/it][A
 84%|████████▍ | 386/459 [29:18<05:33,  4.56s/it][A
 84%|████████▍ | 387/459 [29:23<05:31,  4.61s/it][A
 85%|████████▍ | 388/459 [29:28<05:29,  4.64s/it][A
 85%|████████▍ | 389/459 [29:32<05:23,  4.62s/it][A
 85%|████████▍ | 390/459 [29:37<05:16,  4.59s/it][A
 85%|████████▌ | 391/459 [29:41<05:11,  4.58s/it][A
 85%|████████▌ | 392/459 [29:46<05:07,  4.59s/it][A
 86%|████████▌ | 393/459 [29:51<05:02,  4.58s/it][A
 86%|████████▌ | 394/459 [29:55<04:56,  4.56s/it][A
 86%|████████▌ | 395/459 [30:00<04:51,  4.55s/it][A
 86%|████████▋ | 396/459 [30:04<04:46,  4.54s/it][A
 86%|████████▋ | 397/459 [30:09<04:40,  4.53s/it][A
 87%|████████▋ | 398/459 [30:13<04:35,  4.52s/it][A
 87%|████████▋ | 399/459 [30:18<04:31,  4.53s/it][A
 87%|████████▋ | 400/459 [30:22<04:27,  4.53s/it][A
 87%|████████▋ | 401/459 [30:27<04:23,  4.54s/it][A
 88%|████████▊ | 402/459 [30:31<04:19,  4.56s/it][A
 88%|████████▊ | 403/459 [30:36<04:13,  4.52s/it][A
 88%|████████▊ | 404/459 [30:40<04:08,  4.52s/it][A
 88%|████████▊ | 405/459 [30:45<04:03,  4.52s/it][A
 88%|████████▊ | 406/459 [30:49<03:58,  4.50s/it][A
 89%|████████▊ | 407/459 [30:54<03:52,  4.47s/it][A
 89%|████████▉ | 408/459 [30:58<03:48,  4.48s/it][A
 89%|████████▉ | 409/459 [31:03<03:44,  4.49s/it][A
 89%|████████▉ | 410/459 [31:07<03:39,  4.48s/it][A
 90%|████████▉ | 411/459 [31:12<03:35,  4.48s/it][A
 90%|████████▉ | 412/459 [31:16<03:33,  4.55s/it][A
 90%|████████▉ | 413/459 [31:21<03:30,  4.59s/it][A
 90%|█████████ | 414/459 [31:26<03:30,  4.68s/it][A
 90%|█████████ | 415/459 [31:31<03:30,  4.78s/it][A
 91%|█████████ | 416/459 [31:36<03:28,  4.84s/it][A
 91%|█████████ | 417/459 [31:41<03:25,  4.89s/it][A
 91%|█████████ | 418/459 [31:46<03:22,  4.95s/it][A
 91%|█████████▏| 419/459 [31:51<03:19,  4.98s/it][A
 92%|█████████▏| 420/459 [31:56<03:14,  4.99s/it][A
 92%|█████████▏| 421/459 [32:01<03:08,  4.95s/it][A
 92%|█████████▏| 422/459 [32:05<02:55,  4.75s/it][A
 92%|█████████▏| 423/459 [32:09<02:44,  4.58s/it][A
 92%|█████████▏| 424/459 [32:14<02:36,  4.47s/it][A
 93%|█████████▎| 425/459 [32:18<02:28,  4.38s/it][A
 93%|█████████▎| 426/459 [32:22<02:21,  4.30s/it][A
 93%|█████████▎| 427/459 [32:26<02:17,  4.30s/it][A
 93%|█████████▎| 428/459 [32:30<02:12,  4.27s/it][A
 93%|█████████▎| 429/459 [32:35<02:06,  4.23s/it][A
 94%|█████████▎| 430/459 [32:39<02:04,  4.28s/it][A
 94%|█████████▍| 431/459 [32:43<02:01,  4.33s/it][A
 94%|█████████▍| 432/459 [32:48<01:57,  4.36s/it][A
 94%|█████████▍| 433/459 [32:52<01:53,  4.37s/it][A
 95%|█████████▍| 434/459 [32:57<01:48,  4.35s/it][A
 95%|█████████▍| 435/459 [33:01<01:43,  4.33s/it][A
 95%|█████████▍| 436/459 [33:05<01:39,  4.34s/it][A
 95%|█████████▌| 437/459 [33:10<01:36,  4.37s/it][A
 95%|█████████▌| 438/459 [33:14<01:31,  4.34s/it][A
 96%|█████████▌| 439/459 [33:18<01:27,  4.36s/it][A
 96%|█████████▌| 440/459 [33:23<01:22,  4.36s/it][A
 96%|█████████▌| 441/459 [33:27<01:17,  4.33s/it][A
 96%|█████████▋| 442/459 [33:31<01:13,  4.33s/it][A
 97%|█████████▋| 443/459 [33:36<01:10,  4.40s/it][A
 97%|█████████▋| 444/459 [33:41<01:07,  4.47s/it][A
 97%|█████████▋| 445/459 [33:45<01:03,  4.51s/it][A
 97%|█████████▋| 446/459 [33:50<00:59,  4.57s/it][A
 97%|█████████▋| 447/459 [33:54<00:55,  4.59s/it][A
 98%|█████████▊| 448/459 [33:59<00:50,  4.61s/it][A
 98%|█████████▊| 449/459 [34:04<00:46,  4.63s/it][A
 98%|█████████▊| 450/459 [34:08<00:41,  4.64s/it][A
 98%|█████████▊| 451/459 [34:13<00:37,  4.65s/it][A
 98%|█████████▊| 452/459 [34:18<00:32,  4.67s/it][A
 99%|█████████▊| 453/459 [34:22<00:27,  4.66s/it][A
 99%|█████████▉| 454/459 [34:27<00:23,  4.65s/it][A
 99%|█████████▉| 455/459 [34:32<00:18,  4.66s/it][A
 99%|█████████▉| 456/459 [34:36<00:13,  4.66s/it][A
100%|█████████▉| 457/459 [34:41<00:09,  4.63s/it][A
100%|█████████▉| 458/459 [34:46<00:04,  4.65s/it][A
100%|██████████| 459/459 [34:50<00:00,  4.65s/it][A                                                         
                                                 [A{'eval_loss': 0.1160208061337471, 'eval_runtime': 2097.1232, 'eval_samples_per_second': 1.313, 'eval_steps_per_second': 0.219, 'epoch': 1.94}
 97%|█████████▋| 1000/1034 [42:11:36<1:19:25, 140.17s/it]
100%|██████████| 459/459 [34:50<00:00,  4.65s/it][A
                                                 [A 97%|█████████▋| 1001/1034 [42:13:55<7:02:56, 769.00s/it] 97%|█████████▋| 1002/1034 [42:16:08<5:08:19, 578.11s/it] 97%|█████████▋| 1003/1034 [42:18:22<3:49:50, 444.87s/it] 97%|█████████▋| 1004/1034 [42:20:31<2:55:04, 350.17s/it] 97%|█████████▋| 1005/1034 [42:22:53<2:19:03, 287.71s/it] 97%|█████████▋| 1006/1034 [42:25:12<1:53:25, 243.07s/it] 97%|█████████▋| 1007/1034 [42:27:33<1:35:33, 212.36s/it] 97%|█████████▋| 1008/1034 [42:30:06<1:24:16, 194.50s/it] 98%|█████████▊| 1009/1034 [42:32:36<1:15:27, 181.11s/it] 98%|█████████▊| 1010/1034 [42:34:53<1:07:14, 168.09s/it]                                                         {'loss': 0.1118, 'grad_norm': 1.2843092392230633, 'learning_rate': 3.56390186956701e-08, 'epoch': 1.95}
 98%|█████████▊| 1010/1034 [42:34:53<1:07:14, 168.09s/it] 98%|█████████▊| 1011/1034 [42:37:21<1:02:08, 162.12s/it] 98%|█████████▊| 1012/1034 [42:39:43<57:08, 155.84s/it]   98%|█████████▊| 1013/1034 [42:41:51<51:41, 147.71s/it] 98%|█████████▊| 1014/1034 [42:44:03<47:35, 142.77s/it] 98%|█████████▊| 1015/1034 [42:46:19<44:37, 140.94s/it] 98%|█████████▊| 1016/1034 [42:48:40<42:14, 140.78s/it] 98%|█████████▊| 1017/1034 [42:50:50<38:57, 137.52s/it] 98%|█████████▊| 1018/1034 [42:53:01<36:12, 135.78s/it] 99%|█████████▊| 1019/1034 [42:55:18<34:01, 136.09s/it] 99%|█████████▊| 1020/1034 [42:57:56<33:16, 142.60s/it]                                                       {'loss': 0.1079, 'grad_norm': 1.3769920285923394, 'learning_rate': 1.2834928289472415e-08, 'epoch': 1.97}
 99%|█████████▊| 1020/1034 [42:57:56<33:16, 142.60s/it] 99%|█████████▊| 1021/1034 [43:00:20<30:58, 142.96s/it] 99%|█████████▉| 1022/1034 [43:02:31<27:53, 139.48s/it] 99%|█████████▉| 1023/1034 [43:04:46<25:18, 138.03s/it] 99%|█████████▉| 1024/1034 [43:07:17<23:41, 142.14s/it] 99%|█████████▉| 1025/1034 [43:10:17<23:01, 153.47s/it] 99%|█████████▉| 1026/1034 [43:12:55<20:37, 154.64s/it] 99%|█████████▉| 1027/1034 [43:15:04<17:08, 147.00s/it] 99%|█████████▉| 1028/1034 [43:17:20<14:22, 143.83s/it]100%|█████████▉| 1029/1034 [43:19:55<12:15, 147.09s/it]100%|█████████▉| 1030/1034 [43:22:28<09:55, 148.95s/it]                                                       {'loss': 0.1071, 'grad_norm': 2.316649658929882, 'learning_rate': 1.4263744029019422e-09, 'epoch': 1.99}
100%|█████████▉| 1030/1034 [43:22:28<09:55, 148.95s/it]100%|█████████▉| 1031/1034 [43:24:42<07:12, 144.31s/it]100%|█████████▉| 1032/1034 [43:26:48<04:37, 138.95s/it]100%|█████████▉| 1033/1034 [43:28:58<02:16, 136.25s/it]100%|██████████| 1034/1034 [43:29:51<00:00, 111.19s/it][INFO|trainer.py:4309] 2025-12-19 21:04:51,714 >> Saving model checkpoint to saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034
[INFO|configuration_utils.py:763] 2025-12-19 21:04:51,733 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-19 21:04:51,733 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2421] 2025-12-19 21:04:51,817 >> chat template saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 21:04:51,818 >> tokenizer config file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 21:04:51,818 >> Special tokens file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/special_tokens_map.json
/home/user150/.conda/envs/mcp_env/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2025-12-19 21:04:52,029] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1034 is about to be saved!
[2025-12-19 21:04:52,058] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/global_step1034/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-19 21:04:52,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/global_step1034/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-19 21:04:52,083] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/global_step1034/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-19 21:04:52,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/global_step1034/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-19 21:04:52,284] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/global_step1034/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-19 21:04:52,284] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/checkpoint-1034/global_step1034/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-19 21:04:52,367] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1034 is ready now!
[INFO|trainer.py:2810] 2025-12-19 21:04:52,381 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                       {'train_runtime': 156611.7076, 'train_samples_per_second': 0.317, 'train_steps_per_second': 0.007, 'train_loss': 0.9332886489839572, 'epoch': 2.0}
100%|██████████| 1034/1034 [43:30:11<00:00, 111.19s/it]100%|██████████| 1034/1034 [43:30:11<00:00, 151.46s/it]
[INFO|trainer.py:4309] 2025-12-19 21:05:12,084 >> Saving model checkpoint to saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3
[INFO|configuration_utils.py:763] 2025-12-19 21:05:12,099 >> loading configuration file /home/user150/models/Qwen3-14B/config.json
[INFO|configuration_utils.py:839] 2025-12-19 21:05:12,100 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17408,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 40,
  "model_type": "qwen3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2421] 2025-12-19 21:05:12,177 >> chat template saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 21:05:12,177 >> tokenizer config file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 21:05:12,177 >> Special tokens file saved in saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/special_tokens_map.json
***** train metrics *****
  epoch                    =                2.0
  total_flos               =           778359GF
  train_loss               =             0.9333
  train_runtime            = 1 day, 19:30:11.70
  train_samples_per_second =              0.317
  train_steps_per_second   =              0.007
Figure saved at: saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/training_loss.png
Figure saved at: saves/qwen3-14b/lora8_z3_test/financial_sentiment_ds3/training_eval_loss.png
[WARNING|2025-12-19 21:05:13] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4643] 2025-12-19 21:05:13,389 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-19 21:05:13,389 >>   Num examples = 2754
[INFO|trainer.py:4648] 2025-12-19 21:05:13,389 >>   Batch size = 2
  0%|          | 0/459 [00:00<?, ?it/s]  0%|          | 2/459 [00:04<18:53,  2.48s/it]  1%|          | 3/459 [00:09<26:21,  3.47s/it]  1%|          | 4/459 [00:14<30:21,  4.00s/it]  1%|          | 5/459 [00:19<32:49,  4.34s/it]  1%|▏         | 6/459 [00:24<34:10,  4.53s/it]  2%|▏         | 7/459 [00:29<34:51,  4.63s/it]  2%|▏         | 8/459 [00:34<35:24,  4.71s/it]  2%|▏         | 9/459 [00:39<35:56,  4.79s/it]  2%|▏         | 10/459 [00:44<36:06,  4.82s/it]  2%|▏         | 11/459 [00:48<35:59,  4.82s/it]  3%|▎         | 12/459 [00:53<36:10,  4.86s/it]  3%|▎         | 13/459 [00:58<36:29,  4.91s/it]  3%|▎         | 14/459 [01:03<36:33,  4.93s/it]  3%|▎         | 15/459 [01:08<36:15,  4.90s/it]  3%|▎         | 16/459 [01:13<35:51,  4.86s/it]  4%|▎         | 17/459 [01:18<35:41,  4.84s/it]  4%|▍         | 18/459 [01:23<35:30,  4.83s/it]  4%|▍         | 19/459 [01:27<35:20,  4.82s/it]  4%|▍         | 20/459 [01:32<35:14,  4.82s/it]  5%|▍         | 21/459 [01:37<35:26,  4.86s/it]  5%|▍         | 22/459 [01:42<35:35,  4.89s/it]  5%|▌         | 23/459 [01:47<35:58,  4.95s/it]  5%|▌         | 24/459 [01:52<36:19,  5.01s/it]  5%|▌         | 25/459 [01:58<36:32,  5.05s/it]  6%|▌         | 26/459 [02:03<36:29,  5.06s/it]  6%|▌         | 27/459 [02:07<35:51,  4.98s/it]  6%|▌         | 28/459 [02:12<35:13,  4.90s/it]  6%|▋         | 29/459 [02:17<34:59,  4.88s/it]  7%|▋         | 30/459 [02:22<35:12,  4.92s/it]  7%|▋         | 31/459 [02:27<35:20,  4.96s/it]  7%|▋         | 32/459 [02:32<36:11,  5.09s/it]  7%|▋         | 33/459 [02:38<36:21,  5.12s/it]  7%|▋         | 34/459 [02:43<35:52,  5.07s/it]  8%|▊         | 35/459 [02:48<35:59,  5.09s/it]  8%|▊         | 36/459 [02:53<35:20,  5.01s/it]  8%|▊         | 37/459 [02:58<35:11,  5.00s/it]  8%|▊         | 38/459 [03:02<35:01,  4.99s/it]  8%|▊         | 39/459 [03:07<34:41,  4.96s/it]  9%|▊         | 40/459 [03:12<34:28,  4.94s/it]  9%|▉         | 41/459 [03:17<34:36,  4.97s/it]  9%|▉         | 42/459 [03:22<34:39,  4.99s/it]  9%|▉         | 43/459 [03:27<34:16,  4.94s/it] 10%|▉         | 44/459 [03:32<34:03,  4.93s/it] 10%|▉         | 45/459 [03:37<34:22,  4.98s/it] 10%|█         | 46/459 [03:42<33:28,  4.86s/it] 10%|█         | 47/459 [03:46<32:59,  4.80s/it] 10%|█         | 48/459 [03:51<32:46,  4.79s/it] 11%|█         | 49/459 [03:56<32:09,  4.71s/it] 11%|█         | 50/459 [04:00<31:46,  4.66s/it] 11%|█         | 51/459 [04:05<31:26,  4.62s/it] 11%|█▏        | 52/459 [04:09<30:19,  4.47s/it] 12%|█▏        | 53/459 [04:13<29:32,  4.37s/it] 12%|█▏        | 54/459 [04:17<28:53,  4.28s/it] 12%|█▏        | 55/459 [04:21<28:27,  4.23s/it] 12%|█▏        | 56/459 [04:25<28:10,  4.19s/it] 12%|█▏        | 57/459 [04:29<27:58,  4.18s/it] 13%|█▎        | 58/459 [04:34<27:51,  4.17s/it] 13%|█▎        | 59/459 [04:38<27:42,  4.16s/it] 13%|█▎        | 60/459 [04:42<27:30,  4.14s/it] 13%|█▎        | 61/459 [04:46<27:34,  4.16s/it] 14%|█▎        | 62/459 [04:50<27:24,  4.14s/it] 14%|█▎        | 63/459 [04:54<27:19,  4.14s/it] 14%|█▍        | 64/459 [04:58<27:06,  4.12s/it] 14%|█▍        | 65/459 [05:02<26:52,  4.09s/it] 14%|█▍        | 66/459 [05:06<26:47,  4.09s/it] 15%|█▍        | 67/459 [05:10<26:31,  4.06s/it] 15%|█▍        | 68/459 [05:14<26:23,  4.05s/it] 15%|█▌        | 69/459 [05:18<26:14,  4.04s/it] 15%|█▌        | 70/459 [05:23<26:26,  4.08s/it] 15%|█▌        | 71/459 [05:27<26:24,  4.08s/it] 16%|█▌        | 72/459 [05:31<26:11,  4.06s/it] 16%|█▌        | 73/459 [05:35<26:05,  4.06s/it] 16%|█▌        | 74/459 [05:39<25:58,  4.05s/it] 16%|█▋        | 75/459 [05:43<25:51,  4.04s/it] 17%|█▋        | 76/459 [05:47<25:48,  4.04s/it] 17%|█▋        | 77/459 [05:51<25:39,  4.03s/it] 17%|█▋        | 78/459 [05:55<25:31,  4.02s/it] 17%|█▋        | 79/459 [05:59<25:56,  4.10s/it] 17%|█▋        | 80/459 [06:04<26:24,  4.18s/it] 18%|█▊        | 81/459 [06:08<26:52,  4.27s/it] 18%|█▊        | 82/459 [06:12<26:59,  4.29s/it] 18%|█▊        | 83/459 [06:17<27:18,  4.36s/it] 18%|█▊        | 84/459 [06:21<27:40,  4.43s/it] 19%|█▊        | 85/459 [06:26<27:40,  4.44s/it] 19%|█▊        | 86/459 [06:30<27:45,  4.47s/it] 19%|█▉        | 87/459 [06:35<27:22,  4.41s/it] 19%|█▉        | 88/459 [06:39<27:03,  4.38s/it] 19%|█▉        | 89/459 [06:43<26:38,  4.32s/it] 20%|█▉        | 90/459 [06:47<26:22,  4.29s/it] 20%|█▉        | 91/459 [06:52<26:40,  4.35s/it] 20%|██        | 92/459 [06:56<26:23,  4.32s/it] 20%|██        | 93/459 [07:00<26:10,  4.29s/it] 20%|██        | 94/459 [07:05<26:07,  4.29s/it] 21%|██        | 95/459 [07:09<26:01,  4.29s/it] 21%|██        | 96/459 [07:13<25:44,  4.26s/it] 21%|██        | 97/459 [07:17<25:27,  4.22s/it] 21%|██▏       | 98/459 [07:22<25:38,  4.26s/it] 22%|██▏       | 99/459 [07:26<26:00,  4.33s/it] 22%|██▏       | 100/459 [07:31<25:57,  4.34s/it] 22%|██▏       | 101/459 [07:35<25:34,  4.29s/it] 22%|██▏       | 102/459 [07:39<25:24,  4.27s/it] 22%|██▏       | 103/459 [07:43<25:15,  4.26s/it] 23%|██▎       | 104/459 [07:47<25:06,  4.24s/it] 23%|██▎       | 105/459 [07:52<25:08,  4.26s/it] 23%|██▎       | 106/459 [07:56<25:12,  4.29s/it] 23%|██▎       | 107/459 [08:00<25:06,  4.28s/it] 24%|██▎       | 108/459 [08:05<25:01,  4.28s/it] 24%|██▎       | 109/459 [08:09<25:02,  4.29s/it] 24%|██▍       | 110/459 [08:13<24:50,  4.27s/it] 24%|██▍       | 111/459 [08:17<24:37,  4.24s/it] 24%|██▍       | 112/459 [08:21<24:21,  4.21s/it] 25%|██▍       | 113/459 [08:25<24:05,  4.18s/it] 25%|██▍       | 114/459 [08:30<24:04,  4.19s/it] 25%|██▌       | 115/459 [08:34<24:00,  4.19s/it] 25%|██▌       | 116/459 [08:38<23:54,  4.18s/it] 25%|██▌       | 117/459 [08:42<23:50,  4.18s/it] 26%|██▌       | 118/459 [08:46<23:45,  4.18s/it] 26%|██▌       | 119/459 [08:51<23:36,  4.17s/it] 26%|██▌       | 120/459 [08:55<23:40,  4.19s/it] 26%|██▋       | 121/459 [08:59<23:39,  4.20s/it] 27%|██▋       | 122/459 [09:03<23:37,  4.21s/it] 27%|██▋       | 123/459 [09:08<24:04,  4.30s/it] 27%|██▋       | 124/459 [09:12<24:23,  4.37s/it] 27%|██▋       | 125/459 [09:17<24:42,  4.44s/it] 27%|██▋       | 126/459 [09:21<24:49,  4.47s/it] 28%|██▊       | 127/459 [09:26<24:59,  4.52s/it] 28%|██▊       | 128/459 [09:31<24:52,  4.51s/it] 28%|██▊       | 129/459 [09:35<24:39,  4.48s/it] 28%|██▊       | 130/459 [09:39<24:30,  4.47s/it] 29%|██▊       | 131/459 [09:44<24:22,  4.46s/it] 29%|██▉       | 132/459 [09:48<24:16,  4.46s/it] 29%|██▉       | 133/459 [09:53<24:09,  4.45s/it] 29%|██▉       | 134/459 [09:57<24:10,  4.46s/it] 29%|██▉       | 135/459 [10:02<24:15,  4.49s/it] 30%|██▉       | 136/459 [10:06<24:23,  4.53s/it] 30%|██▉       | 137/459 [10:11<24:30,  4.57s/it] 30%|███       | 138/459 [10:16<24:38,  4.61s/it] 30%|███       | 139/459 [10:21<24:51,  4.66s/it] 31%|███       | 140/459 [10:25<25:11,  4.74s/it] 31%|███       | 141/459 [10:30<25:10,  4.75s/it] 31%|███       | 142/459 [10:35<24:50,  4.70s/it] 31%|███       | 143/459 [10:40<24:50,  4.72s/it] 31%|███▏      | 144/459 [10:44<24:41,  4.70s/it] 32%|███▏      | 145/459 [10:49<24:40,  4.72s/it] 32%|███▏      | 146/459 [10:54<24:41,  4.73s/it] 32%|███▏      | 147/459 [10:59<24:43,  4.76s/it] 32%|███▏      | 148/459 [11:03<24:53,  4.80s/it] 32%|███▏      | 149/459 [11:08<24:56,  4.83s/it] 33%|███▎      | 150/459 [11:13<24:56,  4.84s/it] 33%|███▎      | 151/459 [11:18<24:48,  4.83s/it] 33%|███▎      | 152/459 [11:23<24:48,  4.85s/it] 33%|███▎      | 153/459 [11:28<24:39,  4.83s/it] 34%|███▎      | 154/459 [11:33<24:27,  4.81s/it] 34%|███▍      | 155/459 [11:37<24:32,  4.85s/it] 34%|███▍      | 156/459 [11:42<24:43,  4.90s/it] 34%|███▍      | 157/459 [11:47<24:47,  4.93s/it] 34%|███▍      | 158/459 [11:52<24:44,  4.93s/it] 35%|███▍      | 159/459 [11:57<24:43,  4.95s/it] 35%|███▍      | 160/459 [12:02<24:24,  4.90s/it] 35%|███▌      | 161/459 [12:07<24:23,  4.91s/it] 35%|███▌      | 162/459 [12:12<23:58,  4.84s/it] 36%|███▌      | 163/459 [12:17<24:15,  4.92s/it] 36%|███▌      | 164/459 [12:22<24:15,  4.94s/it] 36%|███▌      | 165/459 [12:27<23:52,  4.87s/it] 36%|███▌      | 166/459 [12:31<23:37,  4.84s/it] 36%|███▋      | 167/459 [12:36<23:34,  4.84s/it] 37%|███▋      | 168/459 [12:41<23:27,  4.84s/it] 37%|███▋      | 169/459 [12:46<23:07,  4.79s/it] 37%|███▋      | 170/459 [12:50<22:55,  4.76s/it] 37%|███▋      | 171/459 [12:55<22:55,  4.78s/it] 37%|███▋      | 172/459 [13:00<23:15,  4.86s/it] 38%|███▊      | 173/459 [13:05<23:16,  4.88s/it] 38%|███▊      | 174/459 [13:10<23:13,  4.89s/it] 38%|███▊      | 175/459 [13:15<22:56,  4.85s/it] 38%|███▊      | 176/459 [13:19<22:05,  4.68s/it] 39%|███▊      | 177/459 [13:23<21:26,  4.56s/it] 39%|███▉      | 178/459 [13:28<21:03,  4.50s/it] 39%|███▉      | 179/459 [13:32<20:43,  4.44s/it] 39%|███▉      | 180/459 [13:37<20:41,  4.45s/it] 39%|███▉      | 181/459 [13:41<20:59,  4.53s/it] 40%|███▉      | 182/459 [13:46<21:18,  4.62s/it] 40%|███▉      | 183/459 [13:51<21:26,  4.66s/it] 40%|████      | 184/459 [13:56<21:28,  4.69s/it] 40%|████      | 185/459 [14:00<21:23,  4.68s/it] 41%|████      | 186/459 [14:05<21:25,  4.71s/it] 41%|████      | 187/459 [14:10<21:12,  4.68s/it] 41%|████      | 188/459 [14:14<21:05,  4.67s/it] 41%|████      | 189/459 [14:19<20:57,  4.66s/it] 41%|████▏     | 190/459 [14:24<20:52,  4.66s/it] 42%|████▏     | 191/459 [14:28<21:05,  4.72s/it] 42%|████▏     | 192/459 [14:33<20:57,  4.71s/it] 42%|████▏     | 193/459 [14:38<20:53,  4.71s/it] 42%|████▏     | 194/459 [14:43<20:51,  4.72s/it] 42%|████▏     | 195/459 [14:47<20:16,  4.61s/it] 43%|████▎     | 196/459 [14:51<19:43,  4.50s/it] 43%|████▎     | 197/459 [14:55<19:22,  4.44s/it] 43%|████▎     | 198/459 [15:00<19:17,  4.44s/it] 43%|████▎     | 199/459 [15:04<19:00,  4.39s/it] 44%|████▎     | 200/459 [15:09<19:03,  4.42s/it] 44%|████▍     | 201/459 [15:13<19:03,  4.43s/it] 44%|████▍     | 202/459 [15:18<18:58,  4.43s/it] 44%|████▍     | 203/459 [15:22<18:46,  4.40s/it] 44%|████▍     | 204/459 [15:26<18:37,  4.38s/it] 45%|████▍     | 205/459 [15:31<18:32,  4.38s/it] 45%|████▍     | 206/459 [15:35<18:08,  4.30s/it] 45%|████▌     | 207/459 [15:39<17:51,  4.25s/it] 45%|████▌     | 208/459 [15:43<17:40,  4.22s/it] 46%|████▌     | 209/459 [15:47<17:31,  4.21s/it] 46%|████▌     | 210/459 [15:51<17:22,  4.19s/it] 46%|████▌     | 211/459 [15:55<17:03,  4.13s/it] 46%|████▌     | 212/459 [15:59<16:52,  4.10s/it] 46%|████▋     | 213/459 [16:03<16:42,  4.08s/it] 47%|████▋     | 214/459 [16:08<16:44,  4.10s/it] 47%|████▋     | 215/459 [16:12<16:39,  4.10s/it] 47%|████▋     | 216/459 [16:16<16:48,  4.15s/it] 47%|████▋     | 217/459 [16:20<16:48,  4.17s/it] 47%|████▋     | 218/459 [16:24<16:54,  4.21s/it] 48%|████▊     | 219/459 [16:29<16:49,  4.21s/it] 48%|████▊     | 220/459 [16:33<16:44,  4.20s/it] 48%|████▊     | 221/459 [16:37<16:42,  4.21s/it] 48%|████▊     | 222/459 [16:41<16:40,  4.22s/it] 49%|████▊     | 223/459 [16:45<16:34,  4.21s/it] 49%|████▉     | 224/459 [16:50<16:30,  4.22s/it] 49%|████▉     | 225/459 [16:54<16:18,  4.18s/it] 49%|████▉     | 226/459 [16:58<16:15,  4.19s/it] 49%|████▉     | 227/459 [17:02<16:08,  4.17s/it] 50%|████▉     | 228/459 [17:06<16:02,  4.17s/it] 50%|████▉     | 229/459 [17:10<15:53,  4.15s/it] 50%|█████     | 230/459 [17:14<15:46,  4.13s/it] 50%|█████     | 231/459 [17:18<15:34,  4.10s/it] 51%|█████     | 232/459 [17:23<15:24,  4.07s/it] 51%|█████     | 233/459 [17:27<15:17,  4.06s/it] 51%|█████     | 234/459 [17:31<15:09,  4.04s/it] 51%|█████     | 235/459 [17:35<15:08,  4.06s/it] 51%|█████▏    | 236/459 [17:39<15:04,  4.06s/it] 52%|█████▏    | 237/459 [17:43<15:11,  4.11s/it] 52%|█████▏    | 238/459 [17:47<15:21,  4.17s/it] 52%|█████▏    | 239/459 [17:51<15:23,  4.20s/it] 52%|█████▏    | 240/459 [17:56<15:23,  4.22s/it] 53%|█████▎    | 241/459 [18:00<15:19,  4.22s/it] 53%|█████▎    | 242/459 [18:04<15:15,  4.22s/it] 53%|█████▎    | 243/459 [18:08<15:09,  4.21s/it] 53%|█████▎    | 244/459 [18:13<15:05,  4.21s/it] 53%|█████▎    | 245/459 [18:17<15:01,  4.21s/it] 54%|█████▎    | 246/459 [18:21<14:59,  4.22s/it] 54%|█████▍    | 247/459 [18:25<15:02,  4.26s/it] 54%|█████▍    | 248/459 [18:30<14:56,  4.25s/it] 54%|█████▍    | 249/459 [18:34<14:53,  4.26s/it] 54%|█████▍    | 250/459 [18:38<14:45,  4.24s/it] 55%|█████▍    | 251/459 [18:42<14:36,  4.22s/it] 55%|█████▍    | 252/459 [18:47<14:35,  4.23s/it] 55%|█████▌    | 253/459 [18:51<14:28,  4.22s/it] 55%|█████▌    | 254/459 [18:55<14:26,  4.23s/it] 56%|█████▌    | 255/459 [18:59<14:21,  4.22s/it] 56%|█████▌    | 256/459 [19:04<14:24,  4.26s/it] 56%|█████▌    | 257/459 [19:08<14:24,  4.28s/it] 56%|█████▌    | 258/459 [19:12<14:29,  4.33s/it] 56%|█████▋    | 259/459 [19:17<14:27,  4.34s/it] 57%|█████▋    | 260/459 [19:21<14:22,  4.33s/it] 57%|█████▋    | 261/459 [19:25<14:14,  4.32s/it] 57%|█████▋    | 262/459 [19:30<14:09,  4.31s/it] 57%|█████▋    | 263/459 [19:34<14:04,  4.31s/it] 58%|█████▊    | 264/459 [19:38<14:00,  4.31s/it] 58%|█████▊    | 265/459 [19:43<13:59,  4.33s/it] 58%|█████▊    | 266/459 [19:47<13:58,  4.34s/it] 58%|█████▊    | 267/459 [19:51<14:02,  4.39s/it] 58%|█████▊    | 268/459 [19:56<14:11,  4.46s/it] 59%|█████▊    | 269/459 [20:01<14:20,  4.53s/it] 59%|█████▉    | 270/459 [20:05<14:16,  4.53s/it] 59%|█████▉    | 271/459 [20:10<14:09,  4.52s/it] 59%|█████▉    | 272/459 [20:14<14:11,  4.56s/it] 59%|█████▉    | 273/459 [20:19<14:08,  4.56s/it] 60%|█████▉    | 274/459 [20:23<14:02,  4.55s/it] 60%|█████▉    | 275/459 [20:28<14:02,  4.58s/it] 60%|██████    | 276/459 [20:33<14:01,  4.60s/it] 60%|██████    | 277/459 [20:37<13:55,  4.59s/it] 61%|██████    | 278/459 [20:42<13:49,  4.58s/it] 61%|██████    | 279/459 [20:46<13:42,  4.57s/it] 61%|██████    | 280/459 [20:51<13:56,  4.68s/it] 61%|██████    | 281/459 [20:56<14:11,  4.78s/it] 61%|██████▏   | 282/459 [21:01<14:17,  4.84s/it] 62%|██████▏   | 283/459 [21:06<14:17,  4.87s/it] 62%|██████▏   | 284/459 [21:11<14:20,  4.92s/it] 62%|██████▏   | 285/459 [21:16<14:25,  4.97s/it] 62%|██████▏   | 286/459 [21:22<14:24,  5.00s/it] 63%|██████▎   | 287/459 [21:27<14:21,  5.01s/it] 63%|██████▎   | 288/459 [21:32<14:21,  5.04s/it] 63%|██████▎   | 289/459 [21:37<14:21,  5.07s/it] 63%|██████▎   | 290/459 [21:42<14:17,  5.07s/it] 63%|██████▎   | 291/459 [21:47<14:12,  5.08s/it] 64%|██████▎   | 292/459 [21:52<14:03,  5.05s/it] 64%|██████▍   | 293/459 [21:57<13:51,  5.01s/it] 64%|██████▍   | 294/459 [22:02<13:47,  5.01s/it] 64%|██████▍   | 295/459 [22:07<13:35,  4.98s/it] 64%|██████▍   | 296/459 [22:12<13:30,  4.97s/it] 65%|██████▍   | 297/459 [22:17<13:21,  4.95s/it] 65%|██████▍   | 298/459 [22:22<13:27,  5.01s/it] 65%|██████▌   | 299/459 [22:27<13:27,  5.05s/it] 65%|██████▌   | 300/459 [22:32<13:29,  5.09s/it] 66%|██████▌   | 301/459 [22:37<13:27,  5.11s/it] 66%|██████▌   | 302/459 [22:42<13:24,  5.13s/it] 66%|██████▌   | 303/459 [22:48<13:30,  5.20s/it] 66%|██████▌   | 304/459 [22:53<13:23,  5.18s/it] 66%|██████▋   | 305/459 [22:58<13:27,  5.24s/it] 67%|██████▋   | 306/459 [23:04<13:21,  5.24s/it] 67%|██████▋   | 307/459 [23:09<13:20,  5.27s/it] 67%|██████▋   | 308/459 [23:14<13:06,  5.21s/it] 67%|██████▋   | 309/459 [23:19<13:01,  5.21s/it] 68%|██████▊   | 310/459 [23:24<12:55,  5.20s/it] 68%|██████▊   | 311/459 [23:29<12:45,  5.17s/it] 68%|██████▊   | 312/459 [23:34<12:20,  5.04s/it] 68%|██████▊   | 313/459 [23:39<11:50,  4.86s/it] 68%|██████▊   | 314/459 [23:43<11:28,  4.75s/it] 69%|██████▊   | 315/459 [23:48<11:09,  4.65s/it] 69%|██████▉   | 316/459 [23:52<10:55,  4.59s/it] 69%|██████▉   | 317/459 [23:56<10:46,  4.55s/it] 69%|██████▉   | 318/459 [24:01<10:39,  4.53s/it] 69%|██████▉   | 319/459 [24:05<10:31,  4.51s/it] 70%|██████▉   | 320/459 [24:10<10:22,  4.48s/it] 70%|██████▉   | 321/459 [24:14<10:18,  4.48s/it] 70%|███████   | 322/459 [24:19<10:15,  4.50s/it] 70%|███████   | 323/459 [24:23<10:15,  4.53s/it] 71%|███████   | 324/459 [24:28<10:11,  4.53s/it] 71%|███████   | 325/459 [24:32<10:06,  4.53s/it] 71%|███████   | 326/459 [24:37<10:03,  4.54s/it] 71%|███████   | 327/459 [24:42<09:59,  4.54s/it] 71%|███████▏  | 328/459 [24:46<09:49,  4.50s/it] 72%|███████▏  | 329/459 [24:50<09:42,  4.48s/it] 72%|███████▏  | 330/459 [24:55<09:34,  4.45s/it] 72%|███████▏  | 331/459 [24:59<09:26,  4.43s/it] 72%|███████▏  | 332/459 [25:04<09:22,  4.43s/it] 73%|███████▎  | 333/459 [25:08<09:19,  4.44s/it] 73%|███████▎  | 334/459 [25:12<09:14,  4.44s/it] 73%|███████▎  | 335/459 [25:17<09:13,  4.47s/it] 73%|███████▎  | 336/459 [25:22<09:11,  4.49s/it] 73%|███████▎  | 337/459 [25:26<09:07,  4.49s/it] 74%|███████▎  | 338/459 [25:30<08:56,  4.43s/it] 74%|███████▍  | 339/459 [25:34<08:40,  4.34s/it] 74%|███████▍  | 340/459 [25:38<08:24,  4.24s/it] 74%|███████▍  | 341/459 [25:43<08:14,  4.19s/it] 75%|███████▍  | 342/459 [25:47<08:04,  4.15s/it] 75%|███████▍  | 343/459 [25:51<07:56,  4.11s/it] 75%|███████▍  | 344/459 [25:55<07:49,  4.08s/it] 75%|███████▌  | 345/459 [25:59<07:42,  4.06s/it] 75%|███████▌  | 346/459 [26:03<07:36,  4.04s/it] 76%|███████▌  | 347/459 [26:07<07:30,  4.03s/it] 76%|███████▌  | 348/459 [26:11<07:34,  4.10s/it] 76%|███████▌  | 349/459 [26:15<07:27,  4.07s/it] 76%|███████▋  | 350/459 [26:19<07:22,  4.06s/it] 76%|███████▋  | 351/459 [26:23<07:15,  4.03s/it] 77%|███████▋  | 352/459 [26:27<07:08,  4.01s/it] 77%|███████▋  | 353/459 [26:31<07:01,  3.98s/it] 77%|███████▋  | 354/459 [26:35<06:58,  3.99s/it] 77%|███████▋  | 355/459 [26:39<06:58,  4.02s/it] 78%|███████▊  | 356/459 [26:43<06:55,  4.03s/it] 78%|███████▊  | 357/459 [26:47<06:54,  4.06s/it] 78%|███████▊  | 358/459 [26:51<06:52,  4.08s/it] 78%|███████▊  | 359/459 [26:55<06:49,  4.10s/it] 78%|███████▊  | 360/459 [26:59<06:44,  4.09s/it] 79%|███████▊  | 361/459 [27:03<06:39,  4.08s/it] 79%|███████▉  | 362/459 [27:08<06:37,  4.09s/it] 79%|███████▉  | 363/459 [27:12<06:37,  4.14s/it] 79%|███████▉  | 364/459 [27:16<06:36,  4.17s/it] 80%|███████▉  | 365/459 [27:20<06:34,  4.20s/it] 80%|███████▉  | 366/459 [27:25<06:32,  4.22s/it] 80%|███████▉  | 367/459 [27:29<06:31,  4.26s/it] 80%|████████  | 368/459 [27:33<06:23,  4.21s/it] 80%|████████  | 369/459 [27:37<06:18,  4.21s/it] 81%|████████  | 370/459 [27:41<06:12,  4.19s/it] 81%|████████  | 371/459 [27:45<06:05,  4.15s/it] 81%|████████  | 372/459 [27:50<06:01,  4.15s/it] 81%|████████▏ | 373/459 [27:54<05:57,  4.16s/it] 81%|████████▏ | 374/459 [27:58<05:52,  4.14s/it] 82%|████████▏ | 375/459 [28:02<05:46,  4.12s/it] 82%|████████▏ | 376/459 [28:06<05:42,  4.13s/it] 82%|████████▏ | 377/459 [28:10<05:36,  4.10s/it] 82%|████████▏ | 378/459 [28:14<05:35,  4.15s/it] 83%|████████▎ | 379/459 [28:19<05:32,  4.16s/it] 83%|████████▎ | 380/459 [28:23<05:30,  4.18s/it] 83%|████████▎ | 381/459 [28:27<05:26,  4.19s/it] 83%|████████▎ | 382/459 [28:31<05:22,  4.19s/it] 83%|████████▎ | 383/459 [28:35<05:17,  4.17s/it] 84%|████████▎ | 384/459 [28:40<05:14,  4.19s/it] 84%|████████▍ | 385/459 [28:44<05:08,  4.17s/it] 84%|████████▍ | 386/459 [28:48<05:07,  4.22s/it] 84%|████████▍ | 387/459 [28:52<05:05,  4.24s/it] 85%|████████▍ | 388/459 [28:57<05:02,  4.26s/it] 85%|████████▍ | 389/459 [29:01<04:58,  4.27s/it] 85%|████████▍ | 390/459 [29:05<04:55,  4.28s/it] 85%|████████▌ | 391/459 [29:10<04:52,  4.30s/it] 85%|████████▌ | 392/459 [29:14<04:51,  4.35s/it] 86%|████████▌ | 393/459 [29:19<04:50,  4.40s/it] 86%|████████▌ | 394/459 [29:23<04:43,  4.36s/it] 86%|████████▌ | 395/459 [29:27<04:35,  4.30s/it] 86%|████████▋ | 396/459 [29:32<04:38,  4.42s/it] 86%|████████▋ | 397/459 [29:36<04:35,  4.45s/it] 87%|████████▋ | 398/459 [29:41<04:30,  4.43s/it] 87%|████████▋ | 399/459 [29:45<04:28,  4.47s/it] 87%|████████▋ | 400/459 [29:50<04:25,  4.49s/it] 87%|████████▋ | 401/459 [29:54<04:19,  4.47s/it] 88%|████████▊ | 402/459 [29:59<04:14,  4.46s/it] 88%|████████▊ | 403/459 [30:03<04:09,  4.46s/it] 88%|████████▊ | 404/459 [30:07<04:04,  4.45s/it] 88%|████████▊ | 405/459 [30:12<03:59,  4.43s/it] 88%|████████▊ | 406/459 [30:16<03:55,  4.44s/it] 89%|████████▊ | 407/459 [30:21<03:52,  4.47s/it] 89%|████████▉ | 408/459 [30:26<03:52,  4.56s/it] 89%|████████▉ | 409/459 [30:31<03:52,  4.65s/it] 89%|████████▉ | 410/459 [30:35<03:47,  4.64s/it] 90%|████████▉ | 411/459 [30:40<03:44,  4.68s/it] 90%|████████▉ | 412/459 [30:45<03:42,  4.73s/it] 90%|████████▉ | 413/459 [30:49<03:34,  4.67s/it] 90%|█████████ | 414/459 [30:54<03:30,  4.68s/it] 90%|█████████ | 415/459 [30:59<03:29,  4.77s/it] 91%|█████████ | 416/459 [31:04<03:27,  4.83s/it] 91%|█████████ | 417/459 [31:09<03:25,  4.89s/it] 91%|█████████ | 418/459 [31:14<03:22,  4.94s/it] 91%|█████████▏| 419/459 [31:19<03:18,  4.96s/it] 92%|█████████▏| 420/459 [31:24<03:12,  4.92s/it] 92%|█████████▏| 421/459 [31:29<03:07,  4.92s/it] 92%|█████████▏| 422/459 [31:34<03:02,  4.92s/it] 92%|█████████▏| 423/459 [31:38<02:55,  4.88s/it] 92%|█████████▏| 424/459 [31:43<02:51,  4.89s/it] 93%|█████████▎| 425/459 [31:48<02:45,  4.87s/it] 93%|█████████▎| 426/459 [31:53<02:41,  4.90s/it] 93%|█████████▎| 427/459 [31:58<02:34,  4.84s/it] 93%|█████████▎| 428/459 [32:02<02:26,  4.73s/it] 93%|█████████▎| 429/459 [32:07<02:19,  4.65s/it] 94%|█████████▎| 430/459 [32:12<02:15,  4.69s/it] 94%|█████████▍| 431/459 [32:17<02:14,  4.80s/it] 94%|█████████▍| 432/459 [32:22<02:10,  4.84s/it] 94%|█████████▍| 433/459 [32:27<02:06,  4.87s/it] 95%|█████████▍| 434/459 [32:31<02:00,  4.82s/it] 95%|█████████▍| 435/459 [32:36<01:54,  4.79s/it] 95%|█████████▍| 436/459 [32:41<01:50,  4.80s/it] 95%|█████████▌| 437/459 [32:46<01:45,  4.80s/it] 95%|█████████▌| 438/459 [32:50<01:40,  4.80s/it] 96%|█████████▌| 439/459 [32:55<01:36,  4.80s/it] 96%|█████████▌| 440/459 [33:00<01:31,  4.83s/it] 96%|█████████▌| 441/459 [33:05<01:26,  4.80s/it] 96%|█████████▋| 442/459 [33:10<01:21,  4.80s/it] 97%|█████████▋| 443/459 [33:15<01:18,  4.89s/it] 97%|█████████▋| 444/459 [33:20<01:14,  4.95s/it] 97%|█████████▋| 445/459 [33:25<01:09,  4.97s/it] 97%|█████████▋| 446/459 [33:30<01:04,  4.94s/it] 97%|█████████▋| 447/459 [33:35<00:59,  4.94s/it] 98%|█████████▊| 448/459 [33:39<00:53,  4.90s/it] 98%|█████████▊| 449/459 [33:44<00:49,  4.91s/it] 98%|█████████▊| 450/459 [33:49<00:44,  4.93s/it] 98%|█████████▊| 451/459 [33:54<00:39,  4.91s/it] 98%|█████████▊| 452/459 [33:59<00:34,  4.91s/it] 99%|█████████▊| 453/459 [34:04<00:29,  4.92s/it] 99%|█████████▉| 454/459 [34:09<00:24,  4.92s/it] 99%|█████████▉| 455/459 [34:14<00:19,  4.95s/it] 99%|█████████▉| 456/459 [34:19<00:14,  4.85s/it]100%|█████████▉| 457/459 [34:23<00:09,  4.76s/it]100%|█████████▉| 458/459 [34:28<00:04,  4.70s/it]100%|██████████| 459/459 [34:33<00:00,  4.82s/it]100%|██████████| 459/459 [34:33<00:00,  4.52s/it]
***** eval metrics *****
  epoch                   =        2.0
  eval_loss               =     0.1161
  eval_runtime            = 0:34:39.24
  eval_samples_per_second =      1.325
  eval_steps_per_second   =      0.221
[INFO|modelcard.py:456] 2025-12-19 21:39:52,629 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[W1219 21:39:55.056091802 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1219 21:39:55.502886008 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1219 21:39:56.010244890 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1219 21:40:37.060859468 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1219 21:40:38.189986118 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
