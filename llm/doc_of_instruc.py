# Qwen3-32B é‡‘èæƒ…æ„Ÿåˆ†æå¾®è°ƒæŒ‡å—

> ä½¿ç”¨ LLaMA-Factory æ¡†æ¶å¾®è°ƒ Qwen3-32B æ¨¡å‹è¿›è¡Œé‡‘èæ–‡æœ¬æƒ…æ„Ÿåˆ†æ

---

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ•°æ®é›†ä»‹ç»](#2-æ•°æ®é›†ä»‹ç»)
3. [ç¯å¢ƒå‡†å¤‡](#3-ç¯å¢ƒå‡†å¤‡)
4. [æ•°æ®å¤„ç†](#4-æ•°æ®å¤„ç†)
5. [è®­ç»ƒé…ç½®](#5-è®­ç»ƒé…ç½®)
6. [å¼€å§‹è®­ç»ƒ](#6-å¼€å§‹è®­ç»ƒ)
7. [æ¨¡å‹å¯¼å‡ºä¸åˆå¹¶](#7-æ¨¡å‹å¯¼å‡ºä¸åˆå¹¶)
8. [æ¨¡å‹æ¨ç†](#8-æ¨¡å‹æ¨ç†)
9. [å¸¸è§é—®é¢˜](#9-å¸¸è§é—®é¢˜)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 ä»»åŠ¡æè¿°

å°†é‡‘èæ–‡æœ¬ï¼ˆæ–°é—»ã€æ¨æ–‡ç­‰ï¼‰åˆ†ç±»ä¸ºä¸‰ç§æƒ…æ„Ÿï¼š
- **positive** (æ­£é¢): çœ‹æ¶¨ã€åˆ©å¥½ã€å¢é•¿
- **neutral** (ä¸­æ€§): å®¢è§‚é™ˆè¿°ã€æ— æ˜æ˜¾å€¾å‘
- **negative** (è´Ÿé¢): çœ‹è·Œã€åˆ©ç©ºã€ä¸‹è·Œ

### 1.2 æ–‡ä»¶ç»“æ„

```
model_train/train_code/llm/
â”œâ”€â”€ prepare_financial_sentiment_data.py    # æ•°æ®å¤„ç†è„šæœ¬
â”œâ”€â”€ train_qwen3_financial_sentiment.sh     # ä¸€é”®è®­ç»ƒè„šæœ¬
â”œâ”€â”€ financial_sentiment_inference.py       # æ¨ç†è„šæœ¬
â””â”€â”€ README_é‡‘èæƒ…æ„Ÿåˆ†æå¾®è°ƒæŒ‡å—.md          # æœ¬æ–‡æ¡£

LLaMA-Factory/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_info.json                  # æ•°æ®é›†é…ç½®ï¼ˆéœ€æ›´æ–°ï¼‰
â”‚   â”œâ”€â”€ financial_sentiment_train.json     # è®­ç»ƒé›†ï¼ˆç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ financial_sentiment_eval.json      # éªŒè¯é›†ï¼ˆç”Ÿæˆï¼‰
â”‚   â””â”€â”€ financial_sentiment_all.json       # å®Œæ•´æ•°æ®é›†ï¼ˆç”Ÿæˆï¼‰
â””â”€â”€ examples/train_lora/
    â”œâ”€â”€ qwen3_32b_financial_sentiment_lora_sft.yaml      # åŸºç¡€é…ç½®
    â””â”€â”€ qwen3_32b_financial_sentiment_lora_sft_ds3.yaml  # DeepSpeedé…ç½®
```

---

## 2. æ•°æ®é›†ä»‹ç»

### 2.1 æ•°æ®æ¥æº

| æ•°æ®é›† | æ¥æº | æ ·æœ¬æ•° | åŸå§‹æ ‡ç­¾ | è¯´æ˜ |
|--------|------|--------|----------|------|
| **FPB** | [Financial PhraseBank](https://huggingface.co/datasets/takala/financial_phrasebank) | ~2,264 | positive/neutral/negative | é‡‘èæ–°é—»çŸ­å¥ï¼Œä¸“å®¶æ ‡æ³¨ |
| **TFNS** | [Twitter Financial News](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | ~11,930 | Bullish/Bearish/Neutral | é‡‘èæ¨æ–‡ |
| **NWGI** | [News with GPT Instructions](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | ~16,200 | 7ç±»ç»†ç²’åº¦æ ‡ç­¾ | GPTæ ‡æ³¨é‡‘èæ–°é—» |

### 2.2 æ ‡ç­¾ç»Ÿä¸€æ˜ å°„

```
FPB:   positive â†’ positive,  neutral â†’ neutral,  negative â†’ negative
TFNS:  Bullish â†’ positive,   Neutral â†’ neutral,  Bearish â†’ negative
NWGI:  *positive* â†’ positive, neutral â†’ neutral, *negative* â†’ negative
```

### 2.3 æ•°æ®æ ¼å¼ (LLaMA-Factory Alpacaæ ¼å¼)

```json
{
  "instruction": "Analyze the sentiment of the following financial text...\n\nText: Apple reported record quarterly revenue...",
  "input": "",
  "output": "positive",
  "system": "You are an expert financial analyst..."
}
```

---

## 3. ç¯å¢ƒå‡†å¤‡

### 3.1 å®‰è£… LLaMA-Factory

```bash
cd /home/user150/LLaMA-Factory

# å®‰è£…åŸºç¡€ä¾èµ–
pip install -e ".[torch,metrics]"

# å®‰è£… Flash Attention 2 (å¯é€‰ï¼ŒåŠ é€Ÿè®­ç»ƒ)
pip install flash-attn --no-build-isolation
```

### 3.2 å®‰è£…æ•°æ®å¤„ç†ä¾èµ–

```bash
pip install datasets
```

### 3.3 éªŒè¯å®‰è£…

```bash
llamafactory-cli version
```

### 3.4 ç¡¬ä»¶è¦æ±‚

| é…ç½® | GPU | æ˜¾å­˜è¦æ±‚ | æ¨èé…ç½® |
|------|-----|----------|----------|
| åŸºç¡€ | 4Ã—A100 80G | ~60GB/GPU | å•æœºå¤šå¡ |
| æ¨è | 8Ã—A100 80G | ~35GB/GPU | DeepSpeed ZeRO-3 |
| æœ€ä½ | 4Ã—A100 40G | éœ€è¦ offload | ZeRO-3 + CPU offload |

---

## 4. æ•°æ®å¤„ç†

### 4.1 è¿è¡Œæ•°æ®å¤„ç†è„šæœ¬

```bash
python /home/user150/model_train/train_code/llm/prepare_financial_sentiment_data.py \
    --output_dir /home/user150/LLaMA-Factory/data \
    --fpb_subset sentences_allagree \
    --train_ratio 0.9 \
    --seed 42
```

### 4.2 å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--output_dir` | `/home/user150/LLaMA-Factory/data` | è¾“å‡ºç›®å½• |
| `--fpb_subset` | `sentences_allagree` | FPBå­é›†é€‰æ‹© |
| `--train_ratio` | `0.9` | è®­ç»ƒé›†æ¯”ä¾‹ |
| `--seed` | `42` | éšæœºç§å­ |

### 4.3 FPB å­é›†é€‰æ‹©

| å­é›†å | æ ·æœ¬æ•° | è¯´æ˜ |
|--------|--------|------|
| `sentences_allagree` | ~2,264 | æ‰€æœ‰æ ‡æ³¨è€…ä¸€è‡´ï¼ˆè´¨é‡æœ€é«˜ï¼‰ |
| `sentences_75agree` | ~3,453 | 75%ä»¥ä¸Šä¸€è‡´ |
| `sentences_66agree` | ~4,217 | 66%ä»¥ä¸Šä¸€è‡´ |
| `sentences_50agree` | ~4,846 | 50%ä»¥ä¸Šä¸€è‡´ï¼ˆæ•°é‡æœ€å¤šï¼‰ |

### 4.4 æ›´æ–° dataset_info.json

æ•°æ®å¤„ç†å®Œæˆåï¼Œéœ€è¦å°†ä»¥ä¸‹é…ç½®æ·»åŠ åˆ° `LLaMA-Factory/data/dataset_info.json`ï¼š

```json
{
  "financial_sentiment_train": {
    "file_name": "financial_sentiment_train.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output",
      "system": "system"
    }
  },
  "financial_sentiment_eval": {
    "file_name": "financial_sentiment_eval.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output",
      "system": "system"
    }
  },
  "financial_sentiment_all": {
    "file_name": "financial_sentiment_all.json",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output",
      "system": "system"
    }
  }
}
```

**å¿«æ·å‘½ä»¤ï¼ˆè‡ªåŠ¨åˆå¹¶é…ç½®ï¼‰ï¼š**

```bash
python << 'EOF'
import json

# è¯»å–åŸå§‹é…ç½®
with open("/home/user150/LLaMA-Factory/data/dataset_info.json", "r") as f:
    config = json.load(f)

# æ·»åŠ æ–°æ•°æ®é›†
config.update({
    "financial_sentiment_train": {
        "file_name": "financial_sentiment_train.json",
        "columns": {"prompt": "instruction", "query": "input", "response": "output", "system": "system"}
    },
    "financial_sentiment_eval": {
        "file_name": "financial_sentiment_eval.json",
        "columns": {"prompt": "instruction", "query": "input", "response": "output", "system": "system"}
    },
    "financial_sentiment_all": {
        "file_name": "financial_sentiment_all.json",
        "columns": {"prompt": "instruction", "query": "input", "response": "output", "system": "system"}
    }
})

# ä¿å­˜
with open("/home/user150/LLaMA-Factory/data/dataset_info.json", "w") as f:
    json.dump(config, f, indent=2, ensure_ascii=False)

print("âœ… dataset_info.json æ›´æ–°æˆåŠŸ!")
EOF
```

---

## 5. è®­ç»ƒé…ç½®

### 5.1 åŸºç¡€é…ç½® (qwen3_32b_financial_sentiment_lora_sft.yaml)

```yaml
### model
model_name_or_path: /home/user150/models/Qwen3-32B
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target: all

### dataset
dataset: financial_sentiment_train
template: qwen3
cutoff_len: 1024
max_samples: 50000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/qwen3-32b/lora/financial_sentiment
logging_steps: 10
save_steps: 500
save_total_limit: 3
plot_loss: true
overwrite_output_dir: true
report_to: tensorboard

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2.0e-5
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
gradient_checkpointing: true
flash_attn: fa2

### eval
eval_dataset: financial_sentiment_eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
```

### 5.2 å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| **lora_rank** | 64 | LoRAç§©ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œæ˜¾å­˜å ç”¨è¶Šå¤§ |
| **lora_alpha** | 128 | ç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸º 2Ã—rank |
| **lora_dropout** | 0.05 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **lora_target** | all | å¯¹æ‰€æœ‰çº¿æ€§å±‚åº”ç”¨LoRA |
| **learning_rate** | 2e-5 | å¤§æ¨¡å‹å»ºè®®ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ |
| **per_device_train_batch_size** | 1 | 32Bæ¨¡å‹æ˜¾å­˜æœ‰é™ |
| **gradient_accumulation_steps** | 16 | ç­‰æ•ˆbatch_size = 1Ã—16Ã—GPUæ•° |
| **num_train_epochs** | 3 | åˆ†ç±»ä»»åŠ¡2-3è½®é€šå¸¸è¶³å¤Ÿ |
| **cutoff_len** | 1024 | æƒ…æ„Ÿåˆ†ææ–‡æœ¬è¾ƒçŸ­ |
| **gradient_checkpointing** | true | èŠ‚çœæ˜¾å­˜ |
| **flash_attn** | fa2 | Flash Attention 2 åŠ é€Ÿ |

### 5.3 DeepSpeed ZeRO-3 é…ç½®

é€‚ç”¨äºå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒï¼Œåœ¨åŸºç¡€é…ç½®ä¸Šæ·»åŠ ï¼š

```yaml
deepspeed: examples/deepspeed/ds_z3_config.json
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
```

### 5.4 æ˜¾å­˜ä¸è¶³æ—¶çš„è°ƒæ•´

å¦‚æœé‡åˆ° OOM (Out of Memory)ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºè°ƒæ•´ï¼š

1. å‡å° `per_device_train_batch_size` â†’ 1
2. å¢å¤§ `gradient_accumulation_steps` ä¿æŒç­‰æ•ˆbatch_size
3. å‡å° `lora_rank` â†’ 32 æˆ– 16
4. å‡å° `cutoff_len` â†’ 512
5. ä½¿ç”¨ DeepSpeed ZeRO-3 + CPU offload

---

## 6. å¼€å§‹è®­ç»ƒ

### 6.1 æ–¹å¼ä¸€ï¼šä½¿ç”¨ä¸€é”®è„šæœ¬

```bash
# è®¾ç½®GPU
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# è¿è¡Œè„šæœ¬ï¼ˆåŒ…å«æ•°æ®å¤„ç†+è®­ç»ƒï¼‰
bash /home/user150/model_train/train_code/llm/train_qwen3_financial_sentiment.sh
```

### 6.2 æ–¹å¼äºŒï¼šåˆ†æ­¥æ‰§è¡Œ

```bash
cd /home/user150/LLaMA-Factory
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# Step 1: å¤„ç†æ•°æ®
python /home/user150/model_train/train_code/llm/prepare_financial_sentiment_data.py \
    --output_dir /home/user150/LLaMA-Factory/data

# Step 2: æ›´æ–° dataset_info.json (å‚è€ƒ 4.4 èŠ‚)

# Step 3: å¼€å§‹è®­ç»ƒ
# å•æœºè®­ç»ƒ
llamafactory-cli train examples/train_lora/qwen3_32b_financial_sentiment_lora_sft.yaml

# æˆ– DeepSpeed å¤šå¡è®­ç»ƒ
llamafactory-cli train examples/train_lora/qwen3_32b_financial_sentiment_lora_sft_ds3.yaml
```

### 6.3 ä½¿ç”¨ WebUI è®­ç»ƒï¼ˆå¯é€‰ï¼‰

```bash
llamafactory-cli webui
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­é…ç½®å‚æ•°å¹¶å¯åŠ¨è®­ç»ƒã€‚

### 6.4 ç›‘æ§è®­ç»ƒ

```bash
# æŸ¥çœ‹ TensorBoard
tensorboard --logdir saves/qwen3-32b/lora/financial_sentiment

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f saves/qwen3-32b/lora/financial_sentiment/trainer_log.jsonl
```

### 6.5 é¢„è®¡è®­ç»ƒæ—¶é—´

| é…ç½® | æ•°æ®é‡ | é¢„è®¡æ—¶é—´ |
|------|--------|----------|
| 8Ã—A100 80G | ~30,000æ¡ Ã— 3 epochs | 2-4 å°æ—¶ |
| 4Ã—A100 80G | ~30,000æ¡ Ã— 3 epochs | 4-8 å°æ—¶ |

---

## 7. æ¨¡å‹å¯¼å‡ºä¸åˆå¹¶

### 7.1 åˆå¹¶ LoRA æƒé‡

è®­ç»ƒå®Œæˆåï¼Œå°† LoRA æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼š

```bash
llamafactory-cli export \
    --model_name_or_path /home/user150/models/Qwen3-32B \
    --adapter_name_or_path saves/qwen3-32b/lora/financial_sentiment \
    --template qwen3 \
    --finetuning_type lora \
    --export_dir saves/qwen3-32b/merged/financial_sentiment \
    --export_size 4 \
    --export_device auto
```

### 7.2 å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ |
|------|------|
| `--adapter_name_or_path` | LoRA æƒé‡è·¯å¾„ |
| `--export_dir` | åˆå¹¶åæ¨¡å‹ä¿å­˜è·¯å¾„ |
| `--export_size` | åˆ†ç‰‡æ•°é‡ |
| `--export_device` | ä½¿ç”¨çš„è®¾å¤‡ |

### 7.3 åªä½¿ç”¨ LoRA æƒé‡ï¼ˆä¸åˆå¹¶ï¼‰

å¦‚æœä¸æƒ³åˆå¹¶ï¼Œä¹Ÿå¯ä»¥ç›´æ¥åŠ è½½ LoRA æƒé‡ï¼š

```python
from llamafactory.chat import ChatModel

model = ChatModel({
    "model_name_or_path": "/home/user150/models/Qwen3-32B",
    "adapter_name_or_path": "saves/qwen3-32b/lora/financial_sentiment",
    "template": "qwen3",
    "finetuning_type": "lora",
})
```

---

## 8. æ¨¡å‹æ¨ç†

### 8.1 ä½¿ç”¨ vLLM æ¨ç†ï¼ˆæ¨èï¼‰

```bash
python /home/user150/model_train/train_code/llm/financial_sentiment_inference.py
```

### 8.2 ä½¿ç”¨ LLaMA-Factory CLI æ¨ç†

```bash
llamafactory-cli chat \
    --model_name_or_path saves/qwen3-32b/merged/financial_sentiment \
    --template qwen3
```

### 8.3 Python API ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(
    model="saves/qwen3-32b/merged/financial_sentiment",
    tensor_parallel_size=4,
    trust_remote_code=True,
)

# æ„é€ æç¤ºè¯
system = "You are an expert financial analyst..."
text = "Apple reported record quarterly revenue of $123.9 billion."
prompt = f"<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\nAnalyze the sentiment: {text}<|im_end|>\n<|im_start|>assistant\n"

# æ¨ç†
outputs = llm.generate([prompt], SamplingParams(temperature=0.1, max_tokens=10))
print(outputs[0].outputs[0].text)  # positive
```

### 8.4 æ‰¹é‡æ¨ç†

```python
from financial_sentiment_inference import load_financial_sentiment_model, analyze_sentiment

# åŠ è½½æ¨¡å‹
llm = load_financial_sentiment_model("saves/qwen3-32b/merged/financial_sentiment")

# æ‰¹é‡åˆ†æ
texts = [
    "Tesla shares plunged 12% after disappointing delivery numbers.",
    "The Federal Reserve announced it will maintain current interest rates.",
    "Amazon's cloud computing division AWS continues to show strong growth.",
]

results = analyze_sentiment(llm, texts)
for r in results:
    print(f"{r['sentiment']}: {r['text'][:50]}...")
```

---

## 9. å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶ OOM æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
1. å‡å° `per_device_train_batch_size` åˆ° 1
2. å¢å¤§ `gradient_accumulation_steps`
3. å¯ç”¨ `gradient_checkpointing: true`
4. å‡å° `lora_rank` åˆ° 32 æˆ– 16
5. ä½¿ç”¨ DeepSpeed ZeRO-3 + offload

### Q2: æ•°æ®é›†ä¸‹è½½å¤±è´¥ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# è®¾ç½® Hugging Face é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½åæ”¾åˆ°æœ¬åœ°
```

### Q3: Flash Attention å®‰è£…å¤±è´¥ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ç¡®ä¿ CUDA ç‰ˆæœ¬åŒ¹é…
pip install flash-attn --no-build-isolation

# å¦‚æœä»å¤±è´¥ï¼Œå¯åœ¨é…ç½®ä¸­ç¦ç”¨
# flash_attn: disabled
```

### Q4: å¦‚ä½•æ–­ç‚¹ç»­è®­ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®
resume_from_checkpoint: saves/qwen3-32b/lora/financial_sentiment/checkpoint-1000
```

### Q5: å¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
lr_scheduler_type: cosine  # å¯é€‰: linear, cosine, constant
warmup_ratio: 0.1          # é¢„çƒ­æ¯”ä¾‹
# æˆ–ä½¿ç”¨ warmup_steps: 100
```

### Q6: å¦‚ä½•æ·»åŠ æ›´å¤šæ•°æ®é›†ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
1. å°†æ•°æ®è½¬æ¢ä¸º alpaca æ ¼å¼
2. åœ¨ `dataset_info.json` ä¸­æ·»åŠ é…ç½®
3. åœ¨è®­ç»ƒé…ç½®çš„ `dataset` å­—æ®µä¸­ç”¨é€—å·åˆ†éš”å¤šä¸ªæ•°æ®é›†å

```yaml
dataset: financial_sentiment_train,your_new_dataset
```

---

## ğŸ“ å‚è€ƒèµ„æº

- [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory)
- [LLaMA-Factory æ–‡æ¡£](https://llamafactory.readthedocs.io/)
- [Qwen3 æ¨¡å‹](https://huggingface.co/Qwen)
- [Financial PhraseBank](https://huggingface.co/datasets/takala/financial_phrasebank)
- [Twitter Financial News Sentiment](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment)
- [News with GPT Instructions](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions)

---

*æ–‡æ¡£åˆ›å»ºæ—¥æœŸ: 2025å¹´12æœˆ15æ—¥*
