"""
æ‰‹åŠ¨ä¸‹è½½é‡‘èæƒ…æ„Ÿæ•°æ®é›†
æ”¯æŒä½¿ç”¨é•œåƒç«™
"""
import os
import requests
from tqdm import tqdm

HF_MIRROR = "https://hf-mirror.com"

def download_file(url, save_path):
    """ä¸‹è½½æ–‡ä»¶"""
    print(f"Downloading: {url}")
    response = requests.get(url, stream=True, timeout=30)
    response.raise_for_status()
    
    total_size = int(response.headers.get('content-length', 0))
    with open(save_path, 'wb') as f:
        with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                pbar.update(len(chunk))
    print(f"âœ… Saved to: {save_path}")

def main():
    output_dir = "/home/user150/LLaMA-Factory/data/raw_data"
    os.makedirs(output_dir, exist_ok=True)
    
    # æ›´æ–°åçš„æ­£ç¡®URL
    datasets = {
        # Financial PhraseBank - sentences_allagree å­é›†
        "fpb": f"{HF_MIRROR}/datasets/takala/financial_phrasebank/resolve/main/sentences_allagree/train-00000-of-00001.parquet",
        
        # NWGI - News with GPT Instructions (å°è¯•ä¸åŒçš„æ–‡ä»¶è·¯å¾„)
        "nwgi_train": f"{HF_MIRROR}/datasets/oliverwang15/news_with_gpt_instructions/resolve/main/data/train-00000-of-00001.parquet",
    }
    
    for name, url in datasets.items():
        ext = url.split('.')[-1]
        save_path = os.path.join(output_dir, f"{name}.{ext}")
        
        # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if os.path.exists(save_path):
            print(f"â­ï¸  {name} already exists, skipping...")
            continue
            
        try:
            download_file(url, save_path)
        except Exception as e:
            print(f"âŒ Failed to download {name}: {e}")
            
    print("\n" + "="*50)
    print("Download Summary:")
    print("="*50)
    for f in os.listdir(output_dir):
        fpath = os.path.join(output_dir, f)
        size = os.path.getsize(fpath) / 1024
        print(f"  ğŸ“„ {f} ({size:.1f} KB)")

if __name__ == "__main__":
    main()
